Background Task Processing (Pipeline Execution via RQ)

To handle potentially long-running bioinformatics pipelines without blocking the web server or timing out HTTP requests, this application uses a background task queue system based on Redis Queue (RQ).

Motivation

Previously, the pipeline script was executed directly within the FastAPI application's process using asyncio. This approach had limitations:

Scalability: Only one pipeline could run at a time per web server process.

Reliability: If the web server process crashed or restarted, the running pipeline would be terminated.

Timeouts: Long pipelines could exceed web server or load balancer timeouts.

Using RQ addresses these issues by decoupling the task submission from the task execution.

How it Works

Enqueue Job: When a user submits a pipeline run via the /run_pipeline endpoint in the web UI:

The FastAPI application (backend/app/app.py) validates the input files.

Instead of running the script directly, it enqueues a job onto a specific queue (pipeline_tasks) stored in a Redis database.

The job details include the task function to run (backend.app.tasks.run_pipeline_task) and the necessary arguments (paths to input files, script path).

FastAPI immediately returns a 202 Accepted response to the frontend, along with the unique job_id for the queued task.

Worker Execution:

One or more separate RQ worker processes constantly monitor the pipeline_tasks queue in Redis.

When a worker finds a job, it dequeues it.

The worker imports and executes the specified task function (run_pipeline_task from backend/app/tasks.py) with the provided arguments.

The run_pipeline_task function is responsible for:

Running the actual pipeline.sh script using subprocess.run.

Logging relevant information (job start, script output, errors) to the worker's console output.

Handling script success or failure (including timeouts).

Returning results (like the path to the output directory) or error information upon completion. RQ stores this result in Redis associated with the job_id.

Status Polling:

The frontend (frontend/static/run_pipeline.js), after receiving the job_id, starts polling the /job_status/{job_id} endpoint on the FastAPI server periodically (e.g., every 5 seconds).

The /job_status endpoint fetches the job's current status (queued, started, finished, failed, etc.), result, and error information directly from Redis using the job_id.

The frontend updates the UI accordingly, informing the user about the pipeline's progress and eventual outcome.

Polling stops once the job reaches a final state (finished or failed).

Key Components & Files

Redis: External dependency acting as the message broker. Must be running and accessible.

RQ Worker: A separate Python process launched via the rq worker command. You need to run at least one worker for jobs to be processed.

backend/app/tasks.py: Defines the run_pipeline_task function that contains the actual pipeline execution logic (calling pipeline.sh).

backend/app/app.py:

Connects to Redis and initializes the RQ Queue.

Modified /run_pipeline (POST) endpoint to enqueue jobs.

New /job_status/{job_id} (GET) endpoint for status polling.

frontend/static/run_pipeline.js: Modified to submit the job request and then poll the status endpoint using the returned job_id.

conda_env.yml: Includes rq and redis (the Python client library) dependencies.

backend/__init__.py & backend/app/__init__.py: Empty files necessary for Python's package import system to allow the worker to find backend.app.tasks.

Running Locally for Development

Start Redis: Ensure a Redis instance is running (e.g., using Docker: docker run -d -p 6379:6379 --name bio-redis redis:alpine).

Start RQ Worker(s): In a dedicated terminal:

# Navigate to project root
cd /path/to/bioinformatics-webapp
# Activate conda environment
mamba activate bio-webapp
# Set PYTHONPATH (crucial for worker imports)
export PYTHONPATH=$(pwd):$PYTHONPATH
# Start the worker listening on the 'pipeline_tasks' queue
rq worker pipeline_tasks --url redis://localhost:6379


Keep this terminal open.

Start FastAPI App: In another terminal:

# Navigate to project root
cd /path/to/bioinformatics-webapp
# Activate conda environment
mamba activate bio-webapp
# Run the app
python main.py
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Keep this terminal open.

Access Web UI: Open https://localhost:8000 in your browser and use the "Run Pipeline" page. Observe the UI status updates and the logs in the RQ worker terminal.

You can place this within your README.md under a new section or link to it from the README if you put it in a separate docs/ file. Remember to adjust paths or commands if your specific setup differs slightly.
