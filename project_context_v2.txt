--- START FILE: ./requirements_worker.txt (Size: 110 bytes) ---
# ./requirements_worker.txt
rq
redis
psutil
# Add any other specific Python libraries ONLY needed by tasks.py

--- END FILE: ./requirements_worker.txt ---

--- START FILE: ./.contextignore (Size: 1272 bytes) ---
# Directories to ignore
.git/
__pycache__/
node_modules/
venv/
.venv/
build/
dist/
target/
.mypy_cache/
.pytest_cache/
.ruff_cache/
.next/
.vercel/

# Data, logs, and results directories
bioinformatics/data/
bioinformatics/logs/
bioinformatics/results/
tls/

# Frontend build and cache
frontend_app/.next/
frontend_app/node_modules/
frontend_app/out/
frontend_app/.vercel/

# Backend cache and data
backend/app/__pycache__/
backend/app/**/__pycache__/

# Development files
*.pyc
*.pyo
*.pyd
*.so
*.dll
*.dylib
*.log
*.env
*.env.*
.env.local
.env.*.local

# Package files
package-lock.json
yarn.lock
poetry.lock

# Database files
*.db
*.sqlite
*.sqlite3

# Docker volumes and data
docker/filebrowser.db
docker/settings.json

# Large documentation files
*.pdf
*.md

# Previous context files
project_context*.txt

# IDE files
.idea/
.vscode/
*.swp
*.swo

# Nextflow specific
.nextflow/
.nextflow.log*
work/
nxf-tmp.*
timeline*html
report*html
dag*html
trace*txt
.command.*
.exitcode
*.command.*
*.exitcode
**/work/
**/.nextflow/
**/nf_work/
nextflow_work_vol/

# Nextflow-specific patterns
.nextflow/
.nextflow.log*
work/
nxf-tmp.*
timeline*html
report*html
dag*html
trace*txt
.command.*
.exitcode
*.command.*
*.exitcode
**/work/
**/.nextflow/
**/nf_work/
nextflow_work_vol/ 
--- END FILE: ./.contextignore ---

--- START FILE: ./.dockerignore (Size: 273 bytes) ---
# .dockerignore

# Backend specific
**/__pycache__
*.pyc
backend/venv
backend/.venv

# Frontend specific
frontend_app/node_modules
frontend_app/.next
frontend_app/out
frontend_app/.DS_Store
frontend_app/*.log

# General
.git
.vscode
*.env*
tls/
*.pem
docker/filebrowser.db

--- END FILE: ./.dockerignore ---

--- START FILE: ./CHANGELOG.MD (Size: 34 bytes) ---
# Week 1 - 20250321
- main page !1
--- END FILE: ./CHANGELOG.MD ---

--- START FILE: ./start_dev.sh (Size: 8567 bytes) ---
#!/bin/bash

# Exit immediately if a command exits with a non-zero status during setup.

# --- Configuration ---
REDIS_CONTAINER_NAME="bio_redis_local"
FILEBROWSER_CONTAINER_NAME="bio_filebrowser_local"
FRONTEND_DIR="frontend_app"
FRONTEND_ENV_FILE="$FRONTEND_DIR/.env.local"
REDIS_URL="redis://localhost:6379/0"
QUEUE_NAME="pipeline_tasks"
TLS_DIR="./tls"
TLS_KEY="$TLS_DIR/server.key"
TLS_CERT="$TLS_DIR/server.crt"
LOGS_DIR="./logs_dev"

# --- Content for .env.local ---
read -r -d '' FRONTEND_ENV_CONTENT << EOM
NEXT_PUBLIC_API_BASE_URL=https://localhost:8000
NEXT_PUBLIC_FILEBROWSER_URL=https://localhost:8081
EOM

# --- PIDs Storage ---
BACKEND_PID_FILE="$LOGS_DIR/backend.pid"
WORKER_PID_FILE="$LOGS_DIR/worker.pid"
FRONTEND_PID_FILE="$LOGS_DIR/frontend.pid"
TAIL_PID_FILE="$LOGS_DIR/tail.pid"

# --- Functions ---
log_info() {
  echo "[INFO] $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_warn() {
  echo "[WARN] $(date '+%Y-%m-%d %H:%M:%S') - $1" >&2
}

log_error() {
  echo "[ERROR] $(date '+%Y-%m-%d %H:%M:%S') - $1" >&2
  exit 1
}

kill_pid_file() {
  local pid_file="$1"
  local process_name="$2"
  if [ -f "$pid_file" ]; then
    local pid=$(cat "$pid_file")
    if [ -n "$pid" ] && ps -p "$pid" > /dev/null; then
      log_info "Stopping $process_name (PID: $pid)..."
      kill -TERM "$pid" 2>/dev/null
      sleep 1
      if ps -p "$pid" > /dev/null; then
        log_warn "$process_name (PID: $pid) did not stop gracefully, sending KILL signal..."
        kill -KILL "$pid" 2>/dev/null
      fi
      rm "$pid_file"
    else
      if [ -n "$pid" ]; then
          log_info "$process_name PID ($pid from file) not found or already stopped."
      else
          log_info "$process_name PID file was empty."
      fi
      rm "$pid_file"
    fi
  fi
}

cleanup() {
  log_info "--- Initiating Cleanup ---"
  kill_pid_file "$TAIL_PID_FILE" "Log Tail Process"
  kill_pid_file "$FRONTEND_PID_FILE" "Frontend (npm run dev)"
  kill_pid_file "$WORKER_PID_FILE" "RQ Worker"
  kill_pid_file "$BACKEND_PID_FILE" "Backend (python main.py)"
  log_info "--- Cleanup Finished ---"
  exit 0
}

trap cleanup INT TERM

# --- Main Script ---
log_info "Starting Development Environment..."
PROJECT_ROOT=$(pwd)

mkdir -p "$LOGS_DIR"
log_info "Log files will be stored in '$LOGS_DIR/'"

# 1. Start Docker Containers
log_info "Attempting to start Redis container ($REDIS_CONTAINER_NAME)..."
if ! docker start "$REDIS_CONTAINER_NAME" > /dev/null 2>&1; then
    log_warn "Failed to start $REDIS_CONTAINER_NAME. Check if container exists ('docker ps -a') and Docker is running."
else
    sleep 2
    if ! docker ps -q --filter "name=^/${REDIS_CONTAINER_NAME}$" --filter "status=running" | grep -q .; then
        log_warn "$REDIS_CONTAINER_NAME is not running after start attempt. Check 'docker logs $REDIS_CONTAINER_NAME'."
    else
        log_info "$REDIS_CONTAINER_NAME started successfully."
    fi
fi

log_info "Attempting to start FileBrowser container ($FILEBROWSER_CONTAINER_NAME)..."
if ! docker start "$FILEBROWSER_CONTAINER_NAME" > /dev/null 2>&1; then
    log_warn "Failed to start $FILEBROWSER_CONTAINER_NAME. Check if container exists ('docker ps -a') and Docker is running."
else
    sleep 2
    if ! docker ps -q --filter "name=^/${FILEBROWSER_CONTAINER_NAME}$" --filter "status=running" | grep -q .; then
        log_warn "$FILEBROWSER_CONTAINER_NAME is not running after start attempt. Check 'docker logs $FILEBROWSER_CONTAINER_NAME'."
    else
        log_info "$FILEBROWSER_CONTAINER_NAME started successfully."
    fi
fi

# 2. Ensure TLS Certificate and Key exist
log_info "Checking TLS certificate and key in '$TLS_DIR'..."
if [ -f "$TLS_KEY" ] && [ -f "$TLS_CERT" ]; then
    log_info "TLS key and certificate found."
else
    if [ -e "$TLS_KEY" ] || [ -e "$TLS_CERT" ]; then
        log_warn "'$TLS_KEY' or '$TLS_CERT' exists but is not a regular file (or one is missing). Regenerating..."
    else
         log_warn "TLS key or certificate not found. Generating..."
    fi
    log_info "Attempting to remove existing '$TLS_DIR' (if any) using sudo..."
    sudo rm -rf "$TLS_DIR" || log_warn "Could not remove existing '$TLS_DIR' with sudo."
    log_info "Creating new '$TLS_DIR' directory..."
    mkdir -p "$TLS_DIR" || log_error "Failed to create '$TLS_DIR'."
    log_info "Generating self-signed TLS certificate and key..."
    openssl req -x509 -newkey rsa:4096 \
            -keyout "$TLS_KEY" \
            -out "$TLS_CERT" \
            -sha256 -days 365 -nodes \
            -subj "/CN=localhost" || log_error "Failed to generate TLS certificate/key."
    log_info "TLS certificate and key generated successfully in '$TLS_DIR'."
    sudo chown "$(id -u):$(id -g)" "$TLS_KEY" "$TLS_CERT"
fi

set +e

# 3. Start Backend (in background, redirect logs)
log_info "Starting FastAPI backend -> $LOGS_DIR/backend.log"
python -u main.py > "$LOGS_DIR/backend.log" 2>&1 &
BACKEND_PID=$!
echo $BACKEND_PID > "$BACKEND_PID_FILE"
sleep 3
if ! ps -p $BACKEND_PID > /dev/null; then
    log_error "Backend process (PID: $BACKEND_PID stored in $BACKEND_PID_FILE) failed to start or crashed immediately. Check '$LOGS_DIR/backend.log'."
elif ! curl --output /dev/null --silent --head --fail -k https://localhost:8000/health; then
    log_warn "Backend process started (PID: $BACKEND_PID) but health check failed. It might still be initializing or encountered an error. Check '$LOGS_DIR/backend.log'."
else
    log_info "Backend started (PID: $BACKEND_PID) and responding to health check."
fi

# 4. Start RQ Worker (in background, redirect logs)
log_info "Starting RQ worker -> $LOGS_DIR/worker.log"
export PYTHONPATH="$PROJECT_ROOT"
rq worker "$QUEUE_NAME" --url "$REDIS_URL" > "$LOGS_DIR/worker.log" 2>&1 &
WORKER_PID=$!
echo $WORKER_PID > "$WORKER_PID_FILE"
sleep 2
if ! ps -p $WORKER_PID > /dev/null; then
    log_error "RQ Worker process (PID: $WORKER_PID stored in $WORKER_PID_FILE) failed to start or crashed immediately. Check '$LOGS_DIR/worker.log'."
else
    if grep -q -E "Listening on|Registering birth" "$LOGS_DIR/worker.log"; then
        log_info "RQ Worker started (PID: $WORKER_PID) and appears to be listening."
    else
        log_warn "RQ Worker process started (PID: $WORKER_PID) but expected startup message not found in log yet. Check '$LOGS_DIR/worker.log'."
    fi
fi

# 5. Ensure Frontend .env.local exists
log_info "Checking Frontend .env.local file at '$FRONTEND_ENV_FILE'..."
if [ -f "$FRONTEND_ENV_FILE" ]; then
    log_info "'$FRONTEND_ENV_FILE' already exists."
else
    if [ -d "$FRONTEND_DIR" ]; then
        log_warn "'$FRONTEND_ENV_FILE' not found. Creating it..."
        printf "%s\n" "$FRONTEND_ENV_CONTENT" > "$FRONTEND_ENV_FILE" || log_error "Failed to create '$FRONTEND_ENV_FILE'."
        log_info "'$FRONTEND_ENV_FILE' created successfully."
    else
        log_warn "Frontend directory '$FRONTEND_DIR' not found. Cannot create '$FRONTEND_ENV_FILE'."
    fi
fi

# 6. Start Frontend (in background, redirect logs)
FRONTEND_NPM_PID=""
if [ -d "$FRONTEND_DIR" ]; then
  log_info "Starting Frontend dev server -> $LOGS_DIR/frontend.log"
  cd "$FRONTEND_DIR"
  npm run dev > "../$LOGS_DIR/frontend.log" 2>&1 &
  FRONTEND_NPM_PID=$!
  echo $FRONTEND_NPM_PID > "../$FRONTEND_PID_FILE"
  cd "$PROJECT_ROOT"
  sleep 5
  if ! ps -p $FRONTEND_NPM_PID > /dev/null; then
      log_error "Frontend process (PID: $FRONTEND_NPM_PID stored in $FRONTEND_PID_FILE) failed to start or crashed immediately. Check '$LOGS_DIR/frontend.log'."
  elif ! curl --output /dev/null --silent --head --fail http://localhost:3000; then
      log_warn "Frontend process started (PID: $FRONTEND_NPM_PID) but failed to respond at http://localhost:3000. Check '$LOGS_DIR/frontend.log'."
  else
      log_info "Frontend started (PID: $FRONTEND_NPM_PID) and responding."
  fi
else
  log_warn "Frontend directory '$FRONTEND_DIR' not found. Skipping frontend start."
fi

log_info "--------------------------------------"
log_info "Development environment startup sequence complete."
log_info "Tailing logs from '$LOGS_DIR/'. Press Ctrl+C to stop all components."
log_info "Access Services:"
log_info "  - Backend API: https://localhost:8000/health"
log_info "  - Frontend UI: http://localhost:3000"
log_info "  - FileBrowser: https://localhost:8081"
log_info "--------------------------------------"

touch "$LOGS_DIR/backend.log" "$LOGS_DIR/worker.log" "$LOGS_DIR/frontend.log" 2>/dev/null || true
tail -f "$LOGS_DIR"/*.log &
TAIL_PID=$!
echo $TAIL_PID > "$TAIL_PID_FILE"

wait $TAIL_PID

log_warn "Log tailing process ended unexpectedly. Cleaning up..."
cleanup

--- END FILE: ./start_dev.sh ---

--- START FILE: ./how-to-deploy.txt (Size: 1505 bytes) ---
# AUTO DEPLOYMENT

# PREREQUISITES
install miniforge3 conda mamba

cd bioinfo-webapp

mamba env create -f conda_env.yml -n sarek-webapp

mamba activate sarek-webapp

# MANUAL DPELOYMENT

# GENERATE TLS FOR server.crt AND server.key IN ./tls/

# GENERATE .env.local FOR THE frontend_app IN ./frontend_app/

# RUN EACH OF THE COMPONENTS MANUALLY FOR TESTING AND DEVELOPMENT

1. redis server (on container):
docker pull redis:7-alpine
cd bioinformatics-webapp
docker run \
  -d \
  --name bio_redis_local \
  -p 6379:6379 \
  -v redis_data_vol:/data \
  --restart unless-stopped \
  redis:7-alpine

2. backend : 
cd bioinformatics-webapp
python main.py

3. frontend:
cd bioinformatics-webapp/frontend_app
npm install
npm run dev

4. rq :
cd bioinformatics-webapp
export PYTHONPATH=$(pwd)
rq worker pipeline_tasks --url redis://localhost:6379/0

5. filebrowser (on container):
cd bioinformatics-webapp
docker pull ghcr.io/mikha-22/bioinformatics-webapp/filebrowser:latest
docker run \
  -d \
  --name bio_filebrowser_local \
  -p 8081:8080 \
  -u "$(id -u):$(id -g)" \
  -v "/home/admin01/work/mnt/nas/mikha_temp/data:/srv/data" \
  -v "/home/admin01/work/mnt/nas/mikha_temp/results:/srv/results" \
  -v "$(pwd)/docker/filebrowser.db:/config/filebrowser.db" \
  -v "$(pwd)/docker/settings.json:/config/settings.json" \
  -v "$(pwd)/tls/server.crt:/config/server.crt" \
  -v "$(pwd)/tls/server.key:/config/server.key" \
  --restart unless-stopped \
  ghcr.io/mikha-22/bioinformatics-webapp/filebrowser:latest


--- END FILE: ./how-to-deploy.txt ---

--- START FILE: ./backend/app/models/pipeline.py (Size: 4157 bytes) ---
# backend/app/models/pipeline.py
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any # Make sure Any is imported

# --- Existing Models ---
class SampleInfo(BaseModel):
    patient: str = Field(..., description="Patient identifier")
    sample: str = Field(..., description="Sample identifier")
    sex: str = Field(..., description="Sex (e.g., XX, XY)")
    status: int = Field(..., description="Status (0 for normal, 1 for tumor)")
    # *** ADD lane field ***
    lane: str = Field(..., description="Lane identifier (e.g., L001)")
    # ********************
    fastq_1: str = Field(..., description="Path to first FASTQ file relative to data dir")
    fastq_2: str = Field(..., description="Path to second FASTQ file relative to data dir")

class PipelineInput(BaseModel):
    # Sample information (from frontend form)
    samples: List[SampleInfo] = Field(..., description="List of sample information")

    # Required parameters (from Sarek docs / frontend form)
    genome: str = Field(..., description="Genome build to use (e.g., GRCh38, GRCh37)")

    # Optional files (from Sarek docs / frontend form)
    intervals_file: Optional[str] = Field(None, description="Path to BED file with target regions (relative to data dir)")
    dbsnp: Optional[str] = Field(None, description="Path to dbSNP VCF file (relative to data dir)")
    known_indels: Optional[str] = Field(None, description="Path to known indels VCF file (relative to data dir)")
    pon: Optional[str] = Field(None, description="Path to Panel of Normals (PoN) VCF file (relative to data dir)")

    # Optional parameters (from Sarek docs / frontend form)
    # *** UPDATED: Accept list of strings for tools from frontend ***
    tools: Optional[List[str]] = Field(None, description="List of tools (e.g., ['strelka', 'mutect2'])")
    # ***************************************************************
    step: Optional[str] = Field(None, description="Pipeline step to start from (e.g., mapping, variant_calling)")
    profile: Optional[str] = Field(None, description="Nextflow profile (e.g., docker, singularity)")
    aligner: Optional[str] = Field(None, description="Aligner to use (e.g., bwa-mem, dragmap)")

    # Boolean flags (from Sarek docs / frontend form)
    joint_germline: Optional[bool] = Field(False, description="Perform joint germline calling")
    wes: Optional[bool] = Field(False, description="Data is from Whole Exome Sequencing")
    trim_fastq: Optional[bool] = Field(False, description="Enable adapter trimming")
    skip_qc: Optional[bool] = Field(False, description="Skip QC steps")
    skip_annotation: Optional[bool] = Field(False, description="Skip annotation steps")
    skip_baserecalibrator: Optional[bool] = Field(False, description="Skip base quality score recalibration")

    # Optional metadata (from frontend form)
    description: Optional[str] = Field(None, description="Optional description of the pipeline run")

# --- Existing Models ---
class JobResourceInfo(BaseModel):
    peak_memory_mb: Optional[float] = None
    average_cpu_percent: Optional[float] = None
    duration_seconds: Optional[float] = None

class JobStatusDetails(BaseModel):
    job_id: str = Field(..., description="The unique ID of the RQ job")
    status: str = Field(..., description="Current status of the job (e.g., queued, started, finished, failed)")
    description: Optional[str] = Field(None, description="Job description")
    enqueued_at: Optional[float] = Field(None, description="Unix timestamp when the job was enqueued")
    started_at: Optional[float] = Field(None, description="Unix timestamp when the job started execution")
    ended_at: Optional[float] = Field(None, description="Unix timestamp when the job finished or failed")
    result: Optional[Any] = Field(None, description="Result returned by the job if successful")
    error: Optional[str] = Field(None, description="Error message if the job failed")
    meta: Dict[str, Any] = Field({}, description="Metadata associated with the job")
    resources: Optional[JobResourceInfo] = Field(None, description="Resource usage statistics")
# --- END Existing Models ---

--- END FILE: ./backend/app/models/pipeline.py ---

--- START FILE: ./backend/app/tasks.py (Size: 24162 bytes) ---
# backend/app/tasks.py
import subprocess
import logging
from pathlib import Path
import time
import psutil
import math
import json
import os
import select # <--- IMPORT select module
from typing import Optional, List, Dict, Any

# --- RQ Import ---
from rq import get_current_job

# --- Configure Logging ---
# (Keep logging config as is)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(process)d - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# --- Path Definitions ---
MONITORING_INTERVAL_SECONDS = 5
METADATA_FILENAME = "run_metadata.json"

# --- Import Config ---
from .core.config import RESULTS_DIR

def get_current_job_id():
    job = get_current_job()
    return job.id if job else "N/A (Not in RQ context)"

# --- Updated Function Signature ---
# (Keep function signature as is)
def run_pipeline_task(
    input_csv_path_str: str,
    outdir_base_path_str: str,
    genome: str,
    tools: Optional[str],
    step: Optional[str],
    profile: Optional[str],
    intervals_path_str: Optional[str] = None,
    dbsnp_path_str: Optional[str] = None,
    known_indels_path_str: Optional[str] = None,
    pon_path_str: Optional[str] = None,
    aligner: Optional[str] = None,
    joint_germline: bool = False,
    wes: bool = False,
    trim_fastq: bool = False,
    skip_qc: bool = False,
    skip_annotation: bool = False,
    skip_baserecalibrator: bool = False,
    is_rerun: bool = False, # Keep is_rerun
) -> Dict[str, Any]:
    job = get_current_job()
    job_id = job.id if job else "N/A (Not in RQ context)"
    final_results_dir = None

    # --- Logging Parameters (Keep as is) ---
    logger.info(f"[Job {job_id}] Starting Sarek pipeline task...")
    logger.info(f"[Job {job_id}] Input CSV: {input_csv_path_str}")
    logger.info(f"[Job {job_id}] Output Base Dir: {outdir_base_path_str}")
    logger.info(f"[Job {job_id}] Genome: {genome}")
    logger.info(f"[Job {job_id}] Tools: {tools}")
    logger.info(f"[Job {job_id}] Step: {step}")
    logger.info(f"[Job {job_id}] Profile: {profile}")
    logger.info(f"[Job {job_id}] Aligner: {aligner}")
    if intervals_path_str: logger.info(f"[Job {job_id}] Intervals: {intervals_path_str}")
    if dbsnp_path_str: logger.info(f"[Job {job_id}] dbSNP: {dbsnp_path_str}")
    if known_indels_path_str: logger.info(f"[Job {job_id}] Known Indels: {known_indels_path_str}")
    if pon_path_str: logger.info(f"[Job {job_id}] PoN: {pon_path_str}")
    logger.info(f"[Job {job_id}] Joint Germline: {joint_germline}")
    logger.info(f"[Job {job_id}] WES: {wes}")
    logger.info(f"[Job {job_id}] Trim FASTQ: {trim_fastq}")
    logger.info(f"[Job {job_id}] Skip QC: {skip_qc}")
    logger.info(f"[Job {job_id}] Skip Annotation: {skip_annotation}")
    logger.info(f"[Job {job_id}] Skip Base Recalibrator: {skip_baserecalibrator}")
    logger.info(f"[Job {job_id}] Is Rerun: {is_rerun}")


    # --- Command Construction (Keep as is) ---
    script_path = Path(__file__).resolve().parent / "sarek_pipeline.sh"
    if not script_path.exists():
         logger.error(f"[Job {job_id}] CRITICAL: Sarek wrapper script not found at {script_path}")
         raise FileNotFoundError(f"Sarek wrapper script not found: {script_path}")
    # Ensure script is executable (redundant check based on ls -l, but good practice)
    if not os.access(script_path, os.X_OK):
        logger.error(f"[Job {job_id}] CRITICAL: Sarek wrapper script is not executable: {script_path}")
        # Optionally try to chmod it here, but better to fix permissions manually
        # os.chmod(script_path, 0o755)
        raise PermissionError(f"Sarek wrapper script not executable: {script_path}")


    command = [
        "bash", str(script_path),
        input_csv_path_str,       # $1
        outdir_base_path_str,     # $2
        genome,                   # $3
        tools if tools else "",   # $4
        step if step else "",     # $5
        profile if profile else "", # $6
        aligner if aligner else "", # $7
        intervals_path_str if intervals_path_str else "", # $8
        dbsnp_path_str if dbsnp_path_str else "",         # $9
        known_indels_path_str if known_indels_path_str else "", # $10
        pon_path_str if pon_path_str else "",             # $11
        "true" if joint_germline else "false",  # $12
        "true" if wes else "false",            # $13
        "true" if trim_fastq else "false",     # $14
        "true" if skip_qc else "false",        # $15
        "true" if skip_annotation else "false", # $16
        "true" if skip_baserecalibrator else "false",  # $17
        "true" if is_rerun else "false",       # $18
    ]
    script_working_dir = script_path.parent
    logger.info(f"[Job {job_id}] Running command in directory: {script_working_dir}")
    logger.info(f"[Job {job_id}] Command: {' '.join(command)}")

    # Set HOME and NXF_HOME environment variables for the subprocess
    subprocess_env = os.environ.copy()
    user_home = os.path.expanduser("~")
    subprocess_env["HOME"] = user_home
    subprocess_env["NXF_HOME"] = os.path.join(user_home, ".nextflow")
    logger.info(f"[Job {job_id}] Setting HOME={subprocess_env['HOME']} and NXF_HOME={subprocess_env['NXF_HOME']} for subprocess.")
    subprocess_env["NXF_ANSI_LOG"] = "false"


    # --- Resource Monitoring Variables (Keep as is) ---
    peak_memory_mb = 0
    cpu_percentages = []
    process_psutil = None
    start_time = time.time()
    process = None # Initialize process variable

    try:
        # --- Process Execution (Use subprocess_env) ---
        # *** ADDED LOGGING AROUND POPEN ***
        logger.info(f"[Job {job_id}] Preparing to execute Popen...")
        try:
            process = subprocess.Popen(
                command,
                cwd=str(script_working_dir),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True, # Keep text mode for easier handling
                bufsize=1, # Line buffering
                env=subprocess_env # Pass the modified environment
            )
            logger.info(f"[Job {job_id}] Popen successful. PID: {process.pid}")
            # --- ADDED Immediate Stream Check ---
            if process.stdout:
                logger.info(f"[Job {job_id}] stdout fileno: {process.stdout.fileno()}, readable: {process.stdout.readable()}, closed: {process.stdout.closed}")
            else:
                 logger.error(f"[Job {job_id}] process.stdout is None after Popen!")
            if process.stderr:
                 logger.info(f"[Job {job_id}] stderr fileno: {process.stderr.fileno()}, readable: {process.stderr.readable()}, closed: {process.stderr.closed}")
            else:
                 logger.error(f"[Job {job_id}] process.stderr is None after Popen!")
            # --- END Immediate Stream Check ---

        except Exception as popen_err:
            logger.exception(f"[Job {job_id}] CRITICAL ERROR during subprocess.Popen!")
            raise popen_err # Re-raise to fail the job
        # *** END ADDED LOGGING ***

        # --- Initialize psutil (Keep as is) ---
        try:
            process_psutil = psutil.Process(process.pid)
            process_psutil.cpu_percent(interval=None)
            time.sleep(0.1)
        except psutil.NoSuchProcess: logger.warning(f"[Job {job_id}] Process {process.pid} finished before monitoring could start."); process_psutil = None
        except Exception as init_monitor_err: logger.error(f"[Job {job_id}] Error initializing resource monitor for PID {process.pid}: {init_monitor_err}"); process_psutil = None

        # --- Non-Blocking Read Loop using select ---
        stdout_lines = []
        stderr_lines = []
        stdout_buffer = ""
        stderr_buffer = ""

        # Ensure streams are valid before adding to select list
        streams_to_select = []
        if process.stdout: streams_to_select.append(process.stdout)
        if process.stderr: streams_to_select.append(process.stderr)

        if not streams_to_select:
             logger.error(f"[Job {job_id}] Both stdout and stderr are None/invalid after Popen. Cannot read output.")
             # Handle this error appropriately, maybe raise an exception
             raise IOError("Subprocess streams are not available.")


        while streams_to_select:
            # Check if process has finished
            return_code = process.poll()
            if return_code is not None and not streams_to_select: # Process finished and streams closed
                 logger.debug(f"[Job {job_id}] Process finished and streams closed, exiting read loop.")
                 break

            # Use select to wait for data availability (timeout for monitoring)
            try:
                 readable, _, _ = select.select(streams_to_select, [], [], MONITORING_INTERVAL_SECONDS)
            except ValueError as select_err:
                 # This can happen if a file descriptor becomes invalid (e.g., closed unexpectedly)
                 logger.error(f"[Job {job_id}] Error during select (stream likely closed unexpectedly): {select_err}")
                 # Attempt to clean up streams and break
                 for stream in streams_to_select[:]:
                     if stream and not stream.closed:
                         try:
                             if stream.fileno() < 0: # Check for invalid fd
                                 logger.warning(f"[Job {job_id}] Removing stream with invalid fileno from select list.")
                                 streams_to_select.remove(stream)
                             else:
                                 logger.warning(f"[Job {job_id}] Stream {stream.fileno()} caused select error, closing it.")
                                 stream.close()
                                 streams_to_select.remove(stream)
                         except Exception as close_err:
                             logger.error(f"[Job {job_id}] Error closing stream during select error handling: {close_err}")
                             if stream in streams_to_select: streams_to_select.remove(stream) # Ensure removal
                     elif stream in streams_to_select:
                         streams_to_select.remove(stream) # Remove if already closed or None
                 if not streams_to_select: break # Exit loop if no valid streams left
                 continue # Try select again if some streams remain

            if not readable and return_code is not None:
                 # Process finished, and no more data to read from open streams in this timeout cycle
                 logger.debug(f"[Job {job_id}] Process finished, select timed out, checking stream closure.")
                 # Check if streams are actually closed now
                 all_closed = True
                 for stream in streams_to_select[:]:
                     if stream and not stream.closed:
                         all_closed = False
                         # Try one last read attempt
                         try:
                             data = stream.read()
                             if data:
                                 logger.warning(f"[Job {job_id}] Read remaining data after process end: {len(data)} bytes from stream {stream.fileno()}")
                                 if stream is process.stdout: stdout_buffer += data
                                 elif stream is process.stderr: stderr_buffer += data
                             stream.close()
                             streams_to_select.remove(stream)
                         except Exception as final_read_err:
                             logger.error(f"[Job {job_id}] Error during final read: {final_read_err}")
                             if stream in streams_to_select:
                                 try: stream.close()
                                 except: pass
                                 streams_to_select.remove(stream)

                     elif stream in streams_to_select: # Remove if None or already closed
                         streams_to_select.remove(stream)

                 if all_closed or not streams_to_select:
                     break # Exit loop

            for stream in readable:
                if not stream or stream.closed: # Double check stream validity
                    if stream in streams_to_select: streams_to_select.remove(stream)
                    continue

                try:
                    # Read available data (might be partial line)
                    data = stream.read(4096) # Read in chunks
                    if not data:
                        # End of stream reached, remove it from select list
                        logger.debug(f"[Job {job_id}] EOF reached for stream {stream.fileno()}. Closing.")
                        stream.close()
                        streams_to_select.remove(stream)
                        continue

                    if stream is process.stdout:
                        stdout_buffer += data
                        while '\n' in stdout_buffer:
                            line, stdout_buffer = stdout_buffer.split('\n', 1)
                            line += '\n' # Add newline back for consistency
                            stdout_lines.append(line)
                            logger.info(f"[Job {job_id}][STDOUT] {line.strip()}")
                            if "Results directory:" in line:
                                try: final_results_dir = line.split("Results directory:", 1)[1].strip()
                                except IndexError: logger.warning(f"[Job {job_id}] Could not parse results directory from line: {line.strip()}")
                    elif stream is process.stderr:
                        stderr_buffer += data
                        while '\n' in stderr_buffer:
                            line, stderr_buffer = stderr_buffer.split('\n', 1)
                            line += '\n'
                            stderr_lines.append(line)
                            logger.warning(f"[Job {job_id}][STDERR] {line.strip()}")

                except (OSError, ValueError) as e: # Handle potential errors during read/close
                     logger.error(f"[Job {job_id}] Error reading from stream {stream.fileno()}: {e}")
                     if stream in streams_to_select:
                         try: stream.close()
                         except: pass
                         streams_to_select.remove(stream)


            # --- Resource Monitoring (executed periodically due to select timeout) ---
            if process_psutil and process.poll() is None: # Check if process still running
                 try:
                     cpu = process_psutil.cpu_percent(interval=0.1) # Non-blocking after first call
                     if cpu is not None: cpu_percentages.append(cpu)
                     mem_info = process_psutil.memory_info()
                     current_memory_mb = mem_info.rss / (1024 * 1024)
                     peak_memory_mb = max(peak_memory_mb, current_memory_mb)
                 except psutil.NoSuchProcess:
                     logger.warning(f"[Job {job_id}] Process {process.pid} ended during monitoring check.")
                     process_psutil = None # Stop monitoring
                 except Exception as monitor_err:
                     logger.error(f"[Job {job_id}] Error during resource monitoring: {monitor_err}")
                     # Optionally stop monitoring if errors persist

            # Final check if process finished after reading round
            if return_code is None:
                 return_code = process.poll()
            if return_code is not None and not streams_to_select:
                 logger.debug(f"[Job {job_id}] Process finished and streams list empty, exiting read loop.")
                 break # Ensure exit if process finished and streams closed


        # --- Process Finished ---
        logger.info(f"[Job {job_id}] Exited read loop.")
        # Capture any remaining buffered data after loop exits
        if stdout_buffer:
            logger.info(f"[Job {job_id}][STDOUT] {stdout_buffer.strip()}")
            stdout_lines.append(stdout_buffer)
            if not final_results_dir and "Results directory:" in stdout_buffer:
                 for rem_line in stdout_buffer.splitlines():
                     if "Results directory:" in rem_line:
                          try: final_results_dir = rem_line.split("Results directory:", 1)[1].strip(); break
                          except IndexError: logger.warning(f"[Job {job_id}] Could not parse results directory from final stdout buffer line: {rem_line.strip()}")
        if stderr_buffer:
             logger.warning(f"[Job {job_id}][STDERR] {stderr_buffer.strip()}")
             stderr_lines.append(stderr_buffer)

        # Get final return code if not already set
        if return_code is None:
            logger.warning(f"[Job {job_id}] Process return code was None after loop, calling process.wait().")
            return_code = process.wait() # Final blocking wait if needed

        end_time = time.time()
        duration_seconds = end_time - start_time
        logger.info(f"[Job {job_id}] Pipeline process {process.pid} finished with code {return_code} after {duration_seconds:.2f}s.")
        average_cpu = sum(cpu_percentages) / len(cpu_percentages) if cpu_percentages else 0

        # --- Update Job Meta with Final Stats ---
        if job:
            job.meta['peak_memory_mb'] = round(peak_memory_mb, 1)
            job.meta['average_cpu_percent'] = round(average_cpu, 1)
            job.meta['duration_seconds'] = round(duration_seconds, 2)
            job.save_meta()
            logger.info(f"[Job {job_id}] Saved final resource stats to job meta.")

        # --- Success/Failure Check ---
        full_stdout = "".join(stdout_lines)
        full_stderr = "".join(stderr_lines)

        # Check for known critical error patterns in stdout or stderr
        # Use the patterns from the original script version
        script_reported_error = "ERROR ~" in full_stderr or \
                                "Validation of pipeline parameters failed" in full_stderr or \
                                "[SCRIPT DETECTED ERROR]" in full_stderr or \
                                "ERROR ~" in full_stdout or \
                                "Validation of pipeline parameters failed" in full_stdout or \
                                "ERROR:" in full_stderr # Added generic ERROR check for script errors

        # Check for specific status markers from the simplified script
        script_reported_success = "status::success" in full_stdout
        script_reported_failure = "status::failed" in full_stdout

        # Determine final status
        # Prioritize script's explicit status echo if available
        if script_reported_success and return_code == 0:
            final_status_success = True
        elif script_reported_failure or return_code != 0:
            final_status_success = False
        elif not script_reported_success and not script_reported_failure:
            # Fallback to exit code if no status:: marker found
             final_status_success = (return_code == 0 and not script_reported_error)
        else: # Should not happen unless status markers are inconsistent with exit code
             logger.warning(f"[Job {job_id}] Inconsistent status markers and exit code ({return_code}). Assuming failure.")
             final_status_success = False


        if final_status_success:
            logger.info(f"[Job {job_id}] Sarek pipeline finished successfully (Exit Code {return_code}, Status marker found or no errors detected).")
            if final_results_dir:
                results_path_obj = Path(final_results_dir)
                if results_path_obj.is_dir():
                     logger.info(f"[Job {job_id}] Confirmed results directory exists: {final_results_dir}")
                     if job:
                          job.meta['results_path'] = final_results_dir
                          job.save_meta()
                          # Save Metadata File
                          try:
                              metadata_to_save = job.meta
                              metadata_file_path = results_path_obj / METADATA_FILENAME
                              with open(metadata_file_path, 'w') as f:
                                  json.dump(metadata_to_save, f, indent=4)
                              logger.info(f"[Job {job_id}] Saved run metadata to {metadata_file_path}")
                          except Exception as meta_err:
                               logger.error(f"[Job {job_id}] Failed to save run metadata file: {meta_err}")
                     return { "status": "success", "results_path": final_results_dir, "resources": job.meta if job else {} }
                else:
                     logger.error(f"[Job {job_id}] Pipeline reported success, but results directory '{final_results_dir}' not found!")
                     error_message = f"Pipeline finished successfully, but results directory '{final_results_dir}' was not found."
                     if job: job.meta['error_message'] = error_message; job.save_meta()
                     raise RuntimeError(error_message)
            else:
                logger.warning(f"[Job {job_id}] Pipeline finished successfully, but could not determine results directory from output.")
                if job: job.meta['warning_message'] = "Pipeline finished, results dir unclear."; job.save_meta()
                return { "status": "success", "message": "Pipeline finished, results directory unclear.", "resources": job.meta if job else {} }
        else:
            # Failure Path
            error_message = f"Sarek pipeline failed. Exit Code: {return_code}."
            if script_reported_error: error_message += " Critical error detected in logs."
            elif script_reported_failure: error_message += " Script reported status::failed."

            logger.error(f"[Job {job_id}] {error_message}")
            # Log more stderr on failure
            stderr_to_log = full_stderr[-2000:] # Log last 2000 chars of stderr
            logger.error(f"[Job {job_id}] STDERR Tail:\n{stderr_to_log}")
            if job:
                job.meta['error_message'] = error_message
                job.meta['stderr_snippet'] = stderr_to_log # Store more stderr
                job.save_meta()
            raise subprocess.CalledProcessError(return_code or 1, command, output=full_stdout, stderr=full_stderr)

    # --- Exception Handling (Keep as is) ---
    except subprocess.TimeoutExpired as e:
        logger.error(f"[Job {job_id}] Sarek pipeline timed out.")
        stderr_output = e.stderr if e.stderr else 'N/A'
        if job: job.meta['error_message'] = "Pipeline timed out."; job.meta['stderr_snippet'] = stderr_output[:1000]; job.save_meta()
        raise # Re-raise to fail the RQ job
    except FileNotFoundError as e:
         logger.error(f"[Job {job_id}] Error executing pipeline: {e}")
         if job: job.meta['error_message'] = f"Task execution error: {e}"; job.save_meta()
         raise # Re-raise to fail the RQ job
    except Exception as e:
        # Catch any other unexpected error during execution or result processing
        logger.exception(f"[Job {job_id}] An unexpected error occurred during pipeline execution.")
        error_msg = f"Unexpected task error: {type(e).__name__}"
        if job:
            job.meta['error_message'] = error_msg
            job.meta['error_details'] = str(e)
            job.save_meta()
        # Re-raise the original exception to ensure RQ marks the job as failed
        raise e
    finally:
        # --- Cleanup temporary CSV file (Keep as is) ---
        if input_csv_path_str and Path(input_csv_path_str).exists():
            try:
                os.remove(input_csv_path_str)
                logger.info(f"[Job {job_id}] Cleaned up temporary CSV file: {input_csv_path_str}")
            except OSError as remove_e:
                logger.warning(f"[Job {job_id}] Could not clean up temporary CSV file {input_csv_path_str}: {remove_e}")

--- END FILE: ./backend/app/tasks.py ---

--- START FILE: ./backend/app/core/config.py (Size: 3013 bytes) ---
# backend/app/core/config.py
import logging
from pathlib import Path
import os

logger = logging.getLogger(__name__)

# --- Path Definitions ---
try:
    # Keep project structure paths if needed elsewhere (e.g., finding scripts)
    APP_FILE_PATH = Path(__file__).resolve() # Path to this config.py file
    CORE_DIR = APP_FILE_PATH.parent
    BACKEND_APP_DIR = CORE_DIR.parent
    PROJECT_ROOT = BACKEND_APP_DIR.parents[1]
    DOCKER_DIR = PROJECT_ROOT / "docker"
    SAREK_PIPELINE_SCRIPT_PATH = BACKEND_APP_DIR / "sarek_pipeline.sh"

    # --- *** OVERRIDE DATA and RESULTS paths for local execution *** ---
    # Use the specified absolute host paths directly
    DATA_DIR = Path("/home/admin01/work/mnt/nas/mikha_temp/data").resolve()
    RESULTS_DIR = Path("/home/admin01/work/mnt/nas/mikha_temp/results").resolve()
    # --- *** END OVERRIDE *** ---

    # Log the paths being used
    logger.info(f"PROJECT_ROOT determined as: {PROJECT_ROOT}")
    logger.info(f"BACKEND_APP_DIR determined as: {BACKEND_APP_DIR}")
    logger.info(f"DATA_DIR OVERRIDDEN TO: {DATA_DIR}")
    logger.info(f"RESULTS_DIR OVERRIDDEN TO: {RESULTS_DIR}")
    logger.info(f"SAREK_PIPELINE_SCRIPT_PATH set to: {SAREK_PIPELINE_SCRIPT_PATH}")

    # Optional: Check if these overridden directories exist at startup
    if not DATA_DIR.is_dir():
        logger.warning(f"Configured DATA_DIR does not exist or is not a directory: {DATA_DIR}")
    if not RESULTS_DIR.is_dir():
        logger.warning(f"Configured RESULTS_DIR does not exist or is not a directory: {RESULTS_DIR}")
        # Optionally create it?
        # try:
        #     RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        #     logger.info(f"Created RESULTS_DIR: {RESULTS_DIR}")
        # except OSError as e:
        #     logger.error(f"Failed to create RESULTS_DIR {RESULTS_DIR}: {e}")


except Exception as e:
    logger.exception("CRITICAL: Failed to calculate essential paths.", exc_info=True)
    raise RuntimeError(f"Failed to calculate essential paths: {e}")


# --- Redis/RQ Configuration ---
# Point to the Redis container name or IP accessible from the host
REDIS_HOST = os.getenv("REDIS_HOST", "localhost") # Use localhost if Redis is exposed on host port 6379
REDIS_PORT = 6379
REDIS_DB = 0
PIPELINE_QUEUE_NAME = "pipeline_tasks"
STAGED_JOBS_KEY = "staged_pipeline_jobs" # Key for Redis Hash storing staged jobs

logger.info(f"Using REDIS_HOST: {REDIS_HOST}")

# --- Job Settings ---
DEFAULT_JOB_TIMEOUT = '2h' # Default timeout for RQ job itself
DEFAULT_RESULT_TTL = 86400  # Keep successful job result 1 day
DEFAULT_FAILURE_TTL = 604800 # Keep failed job result 1 week
MAX_REGISTRY_JOBS = 50 # Max finished/failed jobs to fetch for the list view

# --- Sarek Pipeline Configuration ---
SAREK_DEFAULT_PROFILE = "docker"  # Default container system to use
SAREK_DEFAULT_TOOLS = "strelka,mutect2"  # Default variant calling tools
SAREK_DEFAULT_STEP = "mapping"  # Default pipeline step to start from
SAREK_DEFAULT_ALIGNER = "bwa-mem" # Default aligner

--- END FILE: ./backend/app/core/config.py ---

--- START FILE: ./backend/app/core/redis_rq.py (Size: 1543 bytes) ---
# backend/app/core/redis_rq.py
import logging
import redis
from rq import Queue
from .config import REDIS_HOST, REDIS_PORT, REDIS_DB, PIPELINE_QUEUE_NAME

logger = logging.getLogger(__name__)

redis_conn = None
pipeline_queue = None

try:
    # decode_responses=False is important for RQ compatibility (RQ handles serialization)
    redis_conn = redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        db=REDIS_DB,
        decode_responses=False
    )
    redis_conn.ping()
    logger.info(f"Successfully connected to Redis at {REDIS_HOST}:{REDIS_PORT} DB:{REDIS_DB}")
    pipeline_queue = Queue(PIPELINE_QUEUE_NAME, connection=redis_conn)
    logger.info(f"RQ Queue '{PIPELINE_QUEUE_NAME}' initialized.")
except redis.exceptions.ConnectionError as e:
    logger.error(f"FATAL: Could not connect to Redis at {REDIS_HOST}:{REDIS_PORT}. RQ and Job Management will NOT work. Error: {e}")
    # Keep redis_conn and pipeline_queue as None
except Exception as e:
    logger.error(f"FATAL: An unexpected error occurred during Redis/RQ initialization: {e}", exc_info=True)
    # Keep redis_conn and pipeline_queue as None

def get_redis_connection():
    """ Dependency function to get the Redis connection. """
    if not redis_conn:
        raise ConnectionError("Redis connection is not available.")
    return redis_conn

def get_pipeline_queue():
    """ Dependency function to get the RQ Pipeline Queue. """
    if not pipeline_queue:
        raise ConnectionError("RQ pipeline queue is not available.")
    return pipeline_queue

--- END FILE: ./backend/app/core/redis_rq.py ---

--- START FILE: ./backend/app/sarek_pipeline.sh (Size: 10301 bytes) ---
#!/bin/bash

# Sarek Pipeline Wrapper Script
# This script wraps the Sarek Nextflow pipeline with improved logging for debugging.

# --- Logging Setup ---
log() {
    # Adding PID for clarity in logs
    echo "[$(date '+%Y-%m-%d %H:%M:%S')][PID:$$] $1"
}

# --- Environment Variables ---
# Ensure HOME is set (should be set by tasks.py, but double-check)
if [ -z "$HOME" ]; then
    log "WARNING: HOME environment variable not set. Using current user's home directory."
    # Attempt to get home directory reliably
    export HOME=$(getent passwd $(id -u) | cut -d: -f6)
    if [ -z "$HOME" ]; then
        log "ERROR: Failed to determine HOME directory automatically. Exiting."
        exit 1
    fi
fi

# Ensure NXF_HOME is set (defaults to $HOME/.nextflow)
if [ -z "$NXF_HOME" ]; then
    log "NXF_HOME not set. Using default: $HOME/.nextflow"
    export NXF_HOME="$HOME/.nextflow"
fi

# Create NXF_HOME directory if it doesn't exist
# Check write permissions too
if [ ! -d "$NXF_HOME" ]; then
    log "Creating NXF_HOME directory: $NXF_HOME"
    mkdir -p "$NXF_HOME"
    if [ $? -ne 0 ]; then
        log "ERROR: Failed to create NXF_HOME directory: $NXF_HOME. Check permissions." >&2
        exit 1
    fi
elif [ ! -w "$NXF_HOME" ]; then
     log "ERROR: NXF_HOME directory ($NXF_HOME) is not writable by user $(whoami)." >&2
     exit 1
fi


# --- Input Validation (Argument Count) ---
if [ $# -lt 18 ]; then
    log "ERROR: Insufficient arguments provided. Expected 18, got $#."
    log "Usage: $0 <input_csv> <outdir> <genome> <tools> <step> <profile> <aligner> <intervals> <dbsnp> <known_indels> <pon> <joint_germline> <wes> <trim_fastq> <skip_qc> <skip_annotation> <skip_baserecalibrator> <is_rerun>"
    exit 1
fi

# --- Argument Parsing (Positional, MUST match order in tasks.py) ---
input_csv="$1"
outdir_base="$2"  # Base directory for output, not the final dir
genome="$3"
tools="$4"
step="$5"
profile="$6"
aligner="$7"
intervals="$8"
dbsnp="$9"
known_indels="${10}"
pon="${11}"
joint_germline_flag="${12}"
wes_flag="${13}"
trim_fastq_flag="${14}"
skip_qc_flag="${15}"
skip_annotation_flag="${16}"
skip_baserecalibrator_flag="${17}"
is_rerun="${18}"  # New parameter to indicate if this is a re-run

# --- Basic Validation (Required Args) ---
# Added logging for clarity
log "Validating required arguments..."
if [ -z "$input_csv" ]; then
    log "ERROR: Missing required argument: input_csv (Argument #1)" >&2
    exit 1
fi
if [ -z "$outdir_base" ]; then
    log "ERROR: Missing required argument: outdir_base (Argument #2)" >&2
    exit 1
fi
if [ -z "$genome" ]; then
    log "ERROR: Missing required argument: genome (Argument #3)" >&2
    exit 1
fi
log "Input CSV: $input_csv"
log "Output Base Dir: $outdir_base"
log "Genome: $genome"
# Log optional args only if they have a value
[ -n "$tools" ] && log "Tools: $tools"
[ -n "$step" ] && log "Step: $step"
[ -n "$profile" ] && log "Profile: $profile"
[ -n "$aligner" ] && log "Aligner: $aligner"
[ -n "$intervals" ] && log "Intervals: $intervals"
[ -n "$dbsnp" ] && log "dbSNP: $dbsnp"
[ -n "$known_indels" ] && log "Known Indels: $known_indels"
[ -n "$pon" ] && log "PoN: $pon"
log "Joint Germline: $joint_germline_flag"
log "WES: $wes_flag"
log "Trim FASTQ: $trim_fastq_flag"
log "Skip QC: $skip_qc_flag"
log "Skip Annotation: $skip_annotation_flag"
log "Skip Base Recalibrator: $skip_baserecalibrator_flag"
log "Is Rerun: $is_rerun"


# --- Generate Timestamped Results Directory ---
log "Generating results directory..."
timestamp=$(date +"%Y%m%d_%H%M%S")
csv_basename=$(basename "$input_csv" .csv)
results_dir="${outdir_base}/sarek_run_${timestamp}_${csv_basename}"

# Attempt to create directory, check permissions
mkdir -p "$results_dir"
if [ $? -ne 0 ]; then
    log "ERROR: Failed to create results directory: ${results_dir}. Check permissions for base directory: $outdir_base" >&2
    exit 1
fi
if [ ! -w "$results_dir" ]; then
     log "ERROR: Results directory (${results_dir}) is not writable by user $(whoami)." >&2
     # Optional: attempt to fix if possible, or just exit
     # chmod u+w "$results_dir" || exit 1
     exit 1
fi

# Echo the final path for backend parsing (keep this simple echo for easy parsing by tasks.py)
echo "Results directory: ${results_dir}"
log "Successfully created results directory: ${results_dir}"

# --- Define Paths ---
# Use absolute path for nextflow executable found in the worker's PATH
# Make sure this path matches the output of 'which nextflow' in your working environment
NXF_EXECUTABLE="/usr/local/bin/nextflow"
# Define config path variable - MAKE SURE THIS FILE EXISTS AND IS READABLE
NEXTFLOW_CONFIG="/home/admin01/lab/temp/nextflow.config"

# --- Build the Sarek Command ---
log "Building Nextflow command..."
# Start with the absolute path to nextflow
cmd="$NXF_EXECUTABLE run nf-core/sarek -r 3.5.1" # Use the specific version
cmd+=" --input ${input_csv}"
cmd+=" --outdir ${results_dir}"
cmd+=" --genome ${genome}"
cmd+=" -c ${NEXTFLOW_CONFIG}" # Use variable for config path

# Add wes flag if true
if [ "$wes_flag" = "true" ]; then
    cmd+=" --wes"
fi

# Add skip_baserecalibrator flag if true (MUST be before other tools)
if [ "$skip_baserecalibrator_flag" = "true" ]; then
    cmd+=" --skip_tools baserecalibrator"
fi

# Add tools if specified and not empty
# Check specifically against " " which might have been passed if tools was None in Python
if [ -n "$tools" ] && [ "$tools" != " " ]; then
    cmd+=" --tools ${tools}"
fi

# Add step if specified and not empty
if [ -n "$step" ] && [ "$step" != " " ]; then
    cmd+=" --step ${step}"
fi

# Add profile (use provided profile or default to docker)
# Use the variable $profile here
effective_profile="${profile:-docker}" # Default to 'docker' if $profile is empty or null
if [ -n "$effective_profile" ] && [ "$effective_profile" != " " ]; then
    cmd+=" -profile ${effective_profile}"
fi

# Add aligner if specified and not empty
if [ -n "$aligner" ] && [ "$aligner" != " " ]; then
    cmd+=" --aligner ${aligner}"
fi

# Add intervals if specified and not empty
if [ -n "$intervals" ] && [ "$intervals" != " " ]; then
    cmd+=" --intervals ${intervals}"
fi

# Add dbsnp if specified and not empty
if [ -n "$dbsnp" ] && [ "$dbsnp" != " " ]; then
    cmd+=" --dbsnp ${dbsnp}"
fi

# Add known_indels if specified and not empty
if [ -n "$known_indels" ] && [ "$known_indels" != " " ]; then
    cmd+=" --known_indels ${known_indels}"
fi

# Add pon if specified and not empty
if [ -n "$pon" ] && [ "$pon" != " " ]; then
    cmd+=" --pon ${pon}"
fi

# Add joint_germline flag if true
if [ "$joint_germline_flag" = "true" ]; then
    cmd+=" --joint_germline"
fi

# Add trim_fastq flag if true
if [ "$trim_fastq_flag" = "true" ]; then
    cmd+=" --trim_fastq"
fi

# Add skip_qc flag if true
if [ "$skip_qc_flag" = "true" ]; then
    cmd+=" --skip_qc"
fi

# Add skip_annotation flag if true
if [ "$skip_annotation_flag" = "true" ]; then
    cmd+=" --skip_annotation"
fi

# Add resume flag only for re-runs
if [ "$is_rerun" = "true" ]; then
    cmd+=" -resume"
fi

# --- DIAGNOSTIC BLOCK ---
log "--- Worker Environment ---"
log "User: $(whoami) (UID: $(id -u))"
log "Current Directory: $(pwd)"
log "PATH: $PATH"
log "JAVA_HOME: ${JAVA_HOME:-<not set>}" # Check if JAVA_HOME is explicitly set
log "NXF_HOME: $NXF_HOME"
log "NXF_VER used in script: $(${NXF_EXECUTABLE} -v | head -n 1 || echo '<failed>') " # Show resolved NF version
log "--- Sanity Checks ---"
log "Checking Java..."
if command -v java >/dev/null 2>&1; then
    log "Java Path: $(command -v java)"
    log "Java Version: $(java -version 2>&1 | head -n 1)"
else
    log "ERROR: 'java' command not found in PATH ($PATH)" >&2
fi
log "Checking Nextflow executable..."
if [ -x "$NXF_EXECUTABLE" ]; then
     log "Nextflow executable found and is executable: $NXF_EXECUTABLE"
     log "Nextflow Version Check Output:"
     $NXF_EXECUTABLE -v || log "ERROR: 'nextflow -v' command failed using path $NXF_EXECUTABLE" >&2
else
     log "ERROR: Nextflow executable not found or not executable at $NXF_EXECUTABLE" >&2
     exit 1 # Exit if NF is not executable
fi
log "Checking Input CSV..."
if [ -f "$input_csv" ] && [ -r "$input_csv" ]; then
    log "Input CSV found and readable: $input_csv"
else
    log "ERROR: Input CSV not found or not readable: $input_csv" >&2
    ls -l "$input_csv" # Show details if possible
    exit 1 # Exit if input CSV missing
fi
log "Checking Config File..."
if [ -f "$NEXTFLOW_CONFIG" ] && [ -r "$NEXTFLOW_CONFIG" ]; then
    log "Config file found and readable: $NEXTFLOW_CONFIG"
else
    log "ERROR: Config file not found or not readable: $NEXTFLOW_CONFIG" >&2
    ls -l "$NEXTFLOW_CONFIG" # Show details if possible
    exit 1 # Exit if config file missing
fi
log "--- End Sanity Checks ---"
# --- END DIAGNOSTIC BLOCK ---


# --- Execute the pipeline ---
log "Executing Nextflow Command:"
log "$cmd"
# Log the command to a file as well for easier debugging
echo "Timestamp: $(date)" > "${results_dir}/pipeline_command.log"
echo "Executing User: $(whoami) (UID: $(id -u))" >> "${results_dir}/pipeline_command.log"
echo "Working Directory: $(pwd)" >> "${results_dir}/pipeline_command.log"
echo "Command: ${cmd}" >> "${results_dir}/pipeline_command.log"
echo "---------------------" >> "${results_dir}/pipeline_command.log"

# *** MODIFIED EXECUTION LINE ***
# Execute directly, merging stdout/stderr, let Python capture it
$cmd 2>&1

exit_code=$? # Get exit code directly from the nextflow command
# *** END MODIFIED EXECUTION LINE ***

log "Nextflow command finished with exit code: ${exit_code}"
echo "---------------------" >> "${results_dir}/pipeline_command.log"
echo "Exit Code: ${exit_code}" >> "${results_dir}/pipeline_command.log"
echo "Finished: $(date)" >> "${results_dir}/pipeline_command.log"

# --- Check Final Status ---
if [ $exit_code -eq 0 ]; then
    log "Pipeline completed successfully (Exit Code 0)."
    # Echo simple status for Python - can enhance later if needed
    # e.g., could parse pipeline_command.log for specific success markers
    echo "status::success"
    exit 0
else
    log "ERROR: Pipeline failed with exit code ${exit_code}" >&2
    # Echo simple status for Python
    echo "status::failed"
    # It's important to exit with the non-zero code so RQ knows it failed
    exit ${exit_code}
fi

--- END FILE: ./backend/app/sarek_pipeline.sh ---

--- START FILE: ./backend/app/app.py (Size: 3909 bytes) ---
# backend/app/app.py
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
# REMOVED: from fastapi.staticfiles import StaticFiles
# REMOVED: Jinja2Templates initialization was in core.templating

# Import config and routers
# Import config first to ensure paths and settings are loaded early
from .core import config
# Import the routers defined in the routers sub-package
# REMOVED: from .routers import pages
from .routers import data, jobs # Keep data and jobs routers

# --- Basic Logging Setup ---
# Configure logging level, format, and date format.
# Consider moving to a more sophisticated logging setup (e.g., file-based) for production.
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(process)d - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)
logger.info("--- Initializing Bioinformatics Webapp Backend API ---")

# --- FastAPI App Initialization ---
# Define metadata for OpenAPI documentation tags to organize endpoints
tags_metadata = [
    # REMOVED: {"name": "HTML Pages", "description": "Routes serving the web interface pages."},
    {"name": "Data Access", "description": "API endpoints for retrieving file/result lists."},
    {"name": "Jobs Management", "description": "API endpoints for staging, starting, monitoring, and managing pipeline jobs."},
    {"name": "Health Check", "description": "Basic application health status."},
]

# Create the FastAPI application instance
app = FastAPI(
    title="Bioinformatics Webapp API", # Updated title
    description="Backend API for staging, running, and managing Sarek bioinformatics pipelines using FastAPI and RQ.", # Updated description
    version="0.3.0", # Example version number update
    openapi_tags=tags_metadata # Assign the tags metadata
)

# --- Jinja2 Templates (REMOVED) ---
# No longer needed as frontend is handled by Next.js

# --- Static Files Mounting (REMOVED) ---
# No longer needed as frontend is handled by Next.js

# --- CORS Configuration ---
# Configure Cross-Origin Resource Sharing (CORS) middleware.
# Be more restrictive with 'allow_origins' in production environments.
# '*' is okay for development when frontend runs on a different port (e.g., 3000)
# and backend on 8000. For production, list the specific frontend origin.
allowed_origins = ["*"] # TODO: Restrict in production

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins, # List of allowed origins
    allow_credentials=True, # Allow cookies to be included in cross-origin requests
    allow_methods=["GET", "POST", "DELETE", "OPTIONS"], # Allow specific HTTP methods
    allow_headers=["*"], # Allow all headers
)
logger.info(f"CORS middleware configured. Allowed origins: {allowed_origins}")

# --- Include Routers ---
# Add the routers from the sub-package to the main application.
# These routers contain the actual endpoints for different functionalities.
# REMOVED: app.include_router(pages.router)
app.include_router(data.router, prefix="/api") # Add prefix for data endpoints
app.include_router(jobs.router, prefix="/api") # Add prefix for jobs endpoints
logger.info("Included API routers: data, jobs (prefixed with /api).")

# --- Optional: Root endpoint for health check ---
# Provides a simple endpoint to verify the application is running.
@app.get("/health", tags=["Health Check"], summary="Basic Health Check")
async def health_check():
    """Returns a simple 'ok' status for health checks."""
    # Future enhancement: Check connections to Redis etc. here.
    return {"status": "ok", "message": "Backend API is running"}

logger.info("--- Bioinformatics Webapp Backend API Initialization Complete. Ready to serve requests. ---")

# Note: Running the app (e.g., using uvicorn) is handled by the main.py
# script in the project root directory, which imports 'app' from this file.

--- END FILE: ./backend/app/app.py ---

--- START FILE: ./backend/app/utils/time.py (Size: 269 bytes) ---
# backend/app/utils/time.py
import datetime
from typing import Optional

def dt_to_timestamp(dt: Optional[datetime.datetime]) -> Optional[float]:
    """Converts a datetime object to a Unix timestamp (float), handling None."""
    return dt.timestamp() if dt else None

--- END FILE: ./backend/app/utils/time.py ---

--- START FILE: ./backend/app/utils/files.py (Size: 9417 bytes) ---
# backend/app/utils/files.py
import logging
import json
import os
import urllib.parse
from pathlib import Path
from typing import List, Dict, Any
from fastapi import HTTPException

# Import paths from config
from ..core.config import DOCKER_DIR, RESULTS_DIR, DATA_DIR # Added DATA_DIR

logger = logging.getLogger(__name__)

def get_filebrowser_config() -> Dict[str, Any]:
    """Loads File Browser base URL from settings.json"""
    settings_path = DOCKER_DIR / "settings.json"
    config = {"baseURL": "filebrowser"} # Default fallback
    try:
        if settings_path.is_file():
            with open(settings_path, 'r') as f:
                fb_settings = json.load(f)
                base_url = fb_settings.get("baseURL", "/filebrowser").strip('/')
                config["baseURL"] = base_url if base_url else "filebrowser"
            logger.info(f"Loaded File Browser config: baseURL='{config['baseURL']}'")
        else:
            logger.warning(f"File Browser settings not found at {settings_path}, using default baseURL '{config['baseURL']}'.")
    except (json.JSONDecodeError, OSError) as e:
        logger.error(f"Error reading File Browser settings: {e}, using default baseURL '{config['baseURL']}'.")
    return config

def get_safe_path(base_dir: Path, requested_path_str: str) -> Path:
    """
    Safely join a base directory and a requested path string, preventing path traversal.
    Decodes URL encoding from the requested path string.
    Raises HTTPException 400 for invalid or traversal attempts.
    Raises HTTPException 404 if the final path doesn't exist (optional check).
    """
    if not requested_path_str:
        # Allow empty path to refer to the base directory itself (e.g., for listing root)
        # But handle potential None or truly empty strings explicitly if needed elsewhere
        return base_dir.resolve() # Return resolved base if path is empty

    try:
        # Decode URL-encoded characters (like %20 for space)
        decoded_path_str = urllib.parse.unquote(requested_path_str)
        # Normalize path separators for the OS
        normalized_path_str = os.path.normpath(decoded_path_str)
        requested_path = Path(normalized_path_str)
    except Exception as e:
         logger.error(f"Error decoding/normalizing requested path '{requested_path_str}': {e}")
         raise HTTPException(status_code=400, detail="Invalid encoding or format in requested path.")

    # Prevent absolute paths or paths starting with known traversal patterns
    if requested_path.is_absolute() or normalized_path_str.strip().startswith(("..", "/")):
        logger.warning(f"Attempted path traversal with absolute or '..' start: {requested_path_str}")
        raise HTTPException(status_code=400, detail="Invalid path requested (absolute or traversal).")

    # Prevent '..' components within the path after normalization
    if '..' in requested_path.parts:
        logger.warning(f"Attempted path traversal with '..' component: {requested_path_str}")
        raise HTTPException(status_code=400, detail="Invalid path requested (contains '..').")

    # Resolve the full path (resolves symlinks, normalizes path)
    try:
        # Ensure base_dir exists and is a directory before resolving
        if not base_dir.is_dir():
             logger.error(f"Base directory '{base_dir}' does not exist or is not a directory.")
             raise HTTPException(status_code=500, detail="Server configuration error: Base directory invalid.")

        full_path = (base_dir / requested_path).resolve()
    except Exception as e:
         # Catch potential errors during resolution (e.g., path too long, permissions)
         logger.error(f"Error resolving path '{base_dir / requested_path}': {e}")
         raise HTTPException(status_code=400, detail="Invalid file name or path structure.")

    # Crucial check: Ensure the resolved path is *still* within the base directory.
    try:
        # Check if base_dir is a parent of full_path or if they are the same
        is_within_base = base_dir.resolve() in full_path.parents or base_dir.resolve() == full_path
    except OSError as e:
         logger.error(f"OSError during path comparison for '{full_path}' against base '{base_dir}': {e}")
         raise HTTPException(status_code=500, detail="Server error during path validation.")


    if not is_within_base:
        logger.warning(f"Path traversal attempt: Resolved path '{full_path}' is outside base directory '{base_dir.resolve()}'. Original request: '{requested_path_str}'")
        raise HTTPException(status_code=400, detail="Invalid path requested (resolved outside base).")

    # Optional: Check if the final path actually exists (depends on use case, can be done by caller)
    # if check_exists and not full_path.exists():
    #     logger.warning(f"Requested path does not exist: {full_path}")
    #     raise HTTPException(status_code=404, detail="Requested resource not found.")

    return full_path


# --- Updated get_directory_contents ---
def get_directory_contents(
    directory_to_list: Path, # The specific directory whose contents we want
    base_dir_for_relative_path: Path, # The top-level dir (e.g., RESULTS_DIR) for calculating relative paths
    list_dirs: bool = False,
    list_files: bool = False,
    fb_base_url: str = "filebrowser"
    ) -> List[Dict[str, Any]]:
    """
    Retrieves metadata for items in a directory, calculating relative paths.
    Assumes 'directory_to_list' path is already validated and exists.
    """
    items = []
    if not directory_to_list.is_dir():
        logger.warning(f"Directory not found or is not a directory: {directory_to_list}")
        return items # Return empty list if directory doesn't exist

    try:
        # Sort: Directories first, then alphabetically ignoring case
        sorted_paths = sorted(
            list(directory_to_list.iterdir()),
            key=lambda p: (not p.is_dir(), p.name.lower())
        )

        for item_path in sorted_paths:
            try:
                stat_result = item_path.stat() # Can raise FileNotFoundError if item disappears
                is_dir = item_path.is_dir() # Check type after stat

                if (is_dir and list_dirs) or (not is_dir and list_files):
                    fb_link = None
                    # --- Calculate relative path ---
                    # Use os.path.relpath for robust relative path calculation
                    try:
                        relative_path = os.path.relpath(item_path, base_dir_for_relative_path)
                        # Ensure consistent separators (e.g., use '/')
                        relative_path = Path(relative_path).as_posix()
                    except ValueError as e:
                        # This might happen if paths are on different drives on Windows
                        logger.error(f"Could not determine relative path for {item_path} from base {base_dir_for_relative_path}: {e}")
                        relative_path = item_path.name # Fallback to just the name

                    # Construct File Browser link IF it's a top-level directory listing for RESULTS_DIR
                    # Link construction logic might need adjustment based on FileBrowser root config
                    if directory_to_list.resolve() == RESULTS_DIR.resolve():
                         # Assumes File Browser root is '/srv' containing 'data' and 'results'
                         # fb_link target: /filebrowser/files/results/run_xyz or /filebrowser/files/results/run_xyz/subfolder
                         # The relative path needs to be prefixed with 'results/'
                         fb_target_path = Path("results") / relative_path # Use calculated relative path
                         # Ensure path separators are URL-friendly (forward slashes)
                         fb_link = f"/{fb_base_url}/files/{urllib.parse.quote(fb_target_path.as_posix())}"

                    item_info = {
                        "name": item_path.name,
                        "is_dir": is_dir,
                        "modified_time": stat_result.st_mtime, # Unix timestamp
                        "size": stat_result.st_size if not is_dir else None,
                        "extension": item_path.suffix.lower() if not is_dir else None,
                        "filebrowser_link": fb_link, # Include link if generated
                        "relative_path": relative_path # --- ADDED ---
                    }
                    items.append(item_info)

            except FileNotFoundError:
                logger.warning(f"Item '{item_path.name}' disappeared while listing directory '{directory_to_list}'. Skipping.")
                continue # Skip this item
            except OSError as stat_e:
                logger.error(f"Could not get stat info for item {item_path}: {stat_e}")
                items.append({
                    "name": item_path.name,
                    "is_dir": item_path.is_dir(), # Best guess
                    "error": "Could not access item metadata.",
                    "relative_path": item_path.name, # Fallback relative path
                })
    except OSError as list_e:
        logger.error(f"Error reading directory {directory_to_list}: {list_e}")
        raise HTTPException(status_code=500, detail=f"Server error reading directory: {directory_to_list.name}") from list_e

    return items
# --- End updated get_directory_contents ---

--- END FILE: ./backend/app/utils/files.py ---

--- START FILE: ./backend/app/utils/validation.py (Size: 13325 bytes) ---
# backend/app/utils/validation.py
import logging
import csv
import tempfile
import os
import re # Import regex module
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from fastapi import HTTPException

# Import the updated model
from ..models.pipeline import PipelineInput, SampleInfo
# Import config (now pointing to host paths) and safe path function
from ..core.config import DATA_DIR, RESULTS_DIR, SAREK_DEFAULT_TOOLS, SAREK_DEFAULT_PROFILE, SAREK_DEFAULT_STEP, SAREK_DEFAULT_ALIGNER
from .files import get_safe_path

logger = logging.getLogger(__name__)

# --- Sarek 3.5.1 Valid Parameter Options ---
VALID_SAREK_TOOLS = ["strelka", "mutect2", "freebayes", "mpileup", "vardict", "manta", "cnvkit"]
VALID_SAREK_STEPS = ["mapping", "markduplicates", "prepare_recalibration", "recalibrate", "variant_calling", "annotation"]
VALID_SAREK_PROFILES = ["docker", "singularity", "conda", "podman", "test", "test_annotation", "test_tumor_only", "test_tumor_normal", "test_joint_germline"]
VALID_SAREK_ALIGNERS = ["bwa-mem", "dragmap"]
VALID_SAREK_GENOMES = [
    "GATK.GRCh37", "GATK.GRCh38", "Ensembl.GRCh37", "NCBI.GRCh38", "CHM13",
    "GRCm38", "TAIR10", "EB2", "UMD3.1", "WBcel235", "CanFam3.1", "GRCz10",
    "BDGP6", "EquCab2", "EB1", "Galgal4", "Gm01", "Mmul_1", "IRGSP-1.0",
    "CHIMP2.1.4", "Rnor_5.0", "Rnor_6.0", "R64-1-1", "EF2", "Sbi1",
    "Sscrofa10.2", "AGPv3", "hg38", "hg19", "mm10", "bosTau8", "ce10",
    "canFam3", "danRer10", "dm6", "equCab2", "galGal4", "panTro4", "rn6",
    "sacCer3", "susScr3", "testdata.nf-core.sarek"
]

# Regex for validating patient/sample IDs (no spaces)
NO_SPACES_REGEX = re.compile(r"^[^\s]+$")
# Allowed suffixes for intervals file
ALLOWED_INTERVAL_SUFFIXES = ['.bed', '.list', '.interval_list']
# Tools requiring tumor sample
SOMATIC_TOOLS_REQUIRING_TUMOR = ["mutect2", "strelka"] # Adjust if needed

# Updated function signature and return type
def validate_pipeline_input(input_data: PipelineInput) -> tuple[Dict[str, Optional[Path]], List[str]]:
    """
    Validates Sarek pipeline input files and parameters based on the PipelineInput model.
    Generates a temporary samplesheet CSV with direct HOST paths (since webapp/worker run locally).

    Returns:
        - A dictionary mapping logical file keys ('input_csv', 'intervals', 'dbsnp', etc.)
          to their validated absolute Path objects (or None if not provided/invalid).
          The 'input_csv' path is the path to the generated temporary CSV.
        - A list of validation error strings.
    Raises HTTPException for critical errors like inaccessible DATA_DIR.
    """
    validation_errors: List[str] = []
    paths_map: Dict[str, Optional[Path]] = {
        "input_csv": None,
        "intervals": None,
        "dbsnp": None,
        "known_indels": None,
        "pon": None,
    }
    temp_csv_file_path: Optional[str] = None
    has_tumor_sample_in_sheet = False # Flag to track if any tumor sample exists

    try:
        # Ensure DATA_DIR exists (now checks the host path)
        if not DATA_DIR.is_dir():
            logger.critical(f"CRITICAL: Data directory not found at configured path: {DATA_DIR}")
            raise HTTPException(status_code=500, detail=f"Server configuration error: Cannot access data directory {DATA_DIR}.")

        # --- Validate Sample Information and Prepare CSV Rows ---
        if not input_data.samples or len(input_data.samples) == 0:
            validation_errors.append("At least one sample must be provided.")
        else:
            sample_rows_for_csv = [] # Store rows with host paths for CSV
            for i, sample in enumerate(input_data.samples):

                # Validate Patient and Sample IDs for spaces
                if not sample.patient or not NO_SPACES_REGEX.match(sample.patient):
                    validation_errors.append(f"Sample #{i+1}: Patient ID '{sample.patient}' is invalid (cannot be empty or contain spaces).")
                if not sample.sample or not NO_SPACES_REGEX.match(sample.sample):
                     validation_errors.append(f"Sample #{i+1} (Patient '{sample.patient}'): Sample ID '{sample.sample}' is invalid (cannot be empty or contain spaces).")

                # *** ADDED: Validate Lane format ***
                if not sample.lane or not re.match(r"^L\d{3}$", sample.lane):
                    validation_errors.append(f"Sample #{i+1} (Patient '{sample.patient}'): Lane '{sample.lane}' is invalid (must be like L001).")
                # **********************************

                # Track if we have a tumor sample
                if sample.status == 1:
                    has_tumor_sample_in_sheet = True

                # Validate FASTQ files relative to the HOST DATA_DIR
                validated_fastq_1_host: Optional[Path] = None
                validated_fastq_2_host: Optional[Path] = None

                try:
                    validated_fastq_1_host = get_safe_path(DATA_DIR, sample.fastq_1)
                    if not validated_fastq_1_host.is_file():
                        validation_errors.append(f"Sample '{sample.sample}': FASTQ_1 file not found: {sample.fastq_1} (in {DATA_DIR})")
                except HTTPException as e:
                    validation_errors.append(f"Sample '{sample.sample}' FASTQ_1: {e.detail}")
                except Exception as e:
                    logger.error(f"Unexpected error validating FASTQ_1 file '{sample.fastq_1}' for sample '{sample.sample}': {e}")
                    validation_errors.append(f"Sample '{sample.sample}': Error validating FASTQ_1 file.")

                try:
                    validated_fastq_2_host = get_safe_path(DATA_DIR, sample.fastq_2)
                    if not validated_fastq_2_host.is_file():
                        validation_errors.append(f"Sample '{sample.sample}': FASTQ_2 file not found: {sample.fastq_2} (in {DATA_DIR})")
                except HTTPException as e:
                    validation_errors.append(f"Sample '{sample.sample}' FASTQ_2: {e.detail}")
                except Exception as e:
                    logger.error(f"Unexpected error validating FASTQ_2 file '{sample.fastq_2}' for sample '{sample.sample}': {e}")
                    validation_errors.append(f"Sample '{sample.sample}': Error validating FASTQ_2 file.")

                # Use HOST Paths for CSV
                fastq_1_path_for_csv = str(validated_fastq_1_host) if validated_fastq_1_host else sample.fastq_1
                fastq_2_path_for_csv = str(validated_fastq_2_host) if validated_fastq_2_host else sample.fastq_2

                # *** MODIFIED: Append sample.sex, sample.status, AND sample.lane ***
                sample_rows_for_csv.append([
                    sample.patient, sample.sample, sample.sex, sample.status, sample.lane, # Added lane
                    fastq_1_path_for_csv, fastq_2_path_for_csv
                ])
                # *********************************************************************

            # --- Create Samplesheet CSV ---
            if not validation_errors: # Only create if NO errors so far
                try:
                    with tempfile.NamedTemporaryFile(mode='w', newline='', suffix='.csv', delete=False) as temp_csv:
                        csv_writer = csv.writer(temp_csv)
                        # *** MODIFIED: Add 'sex', 'status', and 'lane' to header ***
                        csv_writer.writerow(['patient', 'sample', 'sex', 'status', 'lane', 'fastq_1', 'fastq_2'])
                        # ***********************************************************
                        csv_writer.writerows(sample_rows_for_csv)
                        temp_csv_file_path = temp_csv.name
                        logger.info(f"Created temporary samplesheet CSV with host paths: {temp_csv_file_path}")
                        paths_map["input_csv"] = Path(temp_csv_file_path)
                except (OSError, csv.Error) as e:
                     logger.error(f"Failed to create temporary samplesheet CSV: {e}")
                     validation_errors.append("Internal server error: Could not create samplesheet.")
                     paths_map["input_csv"] = None
                     if temp_csv_file_path and os.path.exists(temp_csv_file_path):
                         os.remove(temp_csv_file_path)

        # --- Validate Optional Files (relative to HOST DATA_DIR) ---
        optional_files_map = {
            "intervals": (input_data.intervals_file, "Intervals"),
            "dbsnp": (input_data.dbsnp, "dbSNP"),
            "known_indels": (input_data.known_indels, "Known Indels"),
            "pon": (input_data.pon, "Panel of Normals"),
        }

        for key, (filename, display_name) in optional_files_map.items():
            if filename and filename.strip().lower() not in ["", "none"]:
                try:
                    file_path = get_safe_path(DATA_DIR, filename)
                    if not file_path.is_file():
                        validation_errors.append(f"{display_name} file not found: {filename} (in {DATA_DIR})")
                    else:
                        # Validate suffix for intervals file if provided
                        if key == "intervals" and file_path.suffix.lower() not in ALLOWED_INTERVAL_SUFFIXES:
                            validation_errors.append(f"{display_name} file must end with one of: {', '.join(ALLOWED_INTERVAL_SUFFIXES)}")
                        else:
                            paths_map[key] = file_path # Store validated host path
                except HTTPException as e:
                    validation_errors.append(f"{display_name}: {e.detail}")
                except Exception as e:
                    logger.error(f"Unexpected error validating {key} file '{filename}': {e}")
                    validation_errors.append(f"Error validating {display_name} file.")

        # --- Validate Sarek Parameters ---
        if not input_data.genome:
            validation_errors.append("Genome build must be specified.")
        elif input_data.genome not in VALID_SAREK_GENOMES:
             validation_errors.append(f"Invalid genome key specified: '{input_data.genome}'. Please choose a valid key (e.g., GATK.GRCh38, hg38).")

        selected_tools = []
        # *** Convert list back to comma-separated string for validation logic below ***
        tools_str = ",".join(input_data.tools) if input_data.tools else None
        if tools_str:
            selected_tools = [tool.strip() for tool in tools_str.split(",") if tool.strip()]
        # *****************************************************************************
            invalid_tools = [tool for tool in selected_tools if tool not in VALID_SAREK_TOOLS]
            if invalid_tools:
                validation_errors.append(f"Invalid tools specified: {', '.join(invalid_tools)}. Valid options are: {', '.join(VALID_SAREK_TOOLS)}")

        if input_data.step and input_data.step not in VALID_SAREK_STEPS:
            validation_errors.append(f"Invalid starting step specified: {input_data.step}. Valid options are: {', '.join(VALID_SAREK_STEPS)}")

        if input_data.profile and input_data.profile not in VALID_SAREK_PROFILES:
            validation_errors.append(f"Invalid profile specified: {input_data.profile}. Valid options include: {', '.join(VALID_SAREK_PROFILES)}")

        if input_data.aligner and input_data.aligner not in VALID_SAREK_ALIGNERS:
            validation_errors.append(f"Invalid aligner specified: {input_data.aligner}. Valid options are: {', '.join(VALID_SAREK_ALIGNERS)}")

        # *** Check for tumor sample if somatic tools selected ***
        somatic_callers_selected = any(tool in selected_tools for tool in SOMATIC_TOOLS_REQUIRING_TUMOR)
        if somatic_callers_selected and not has_tumor_sample_in_sheet:
            validation_errors.append(f"Selected tools ({', '.join(s for s in selected_tools if s in SOMATIC_TOOLS_REQUIRING_TUMOR)}) require at least one sample with Status=1 (Tumor).")
        # -------------------------------------------------------------


    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.exception(f"Unexpected error during input validation: {e}")
        validation_errors.append("An unexpected internal error occurred during validation.")

    # Check errors again *after* all validation steps
    if validation_errors:
        # If CSV was created but other errors occurred, clean it up
        if paths_map.get("input_csv") and paths_map["input_csv"].exists():
             try:
                 os.remove(paths_map["input_csv"])
                 logger.info(f"Cleaned up temporary CSV file due to validation errors: {paths_map['input_csv']}")
                 paths_map["input_csv"] = None # Nullify path in map
             except OSError as e:
                 logger.warning(f"Could not clean up temporary CSV file {paths_map['input_csv']}: {e}")
        # Raise HTTPException with accumulated errors
        error_message = "Validation errors:\n" + "\n".join(f"- {error}" for error in validation_errors)
        logger.warning(f"Validation failed: {error_message}")
        raise HTTPException(status_code=400, detail=error_message)


    # Return the map of validated HOST paths and an empty error list
    return paths_map, validation_errors

--- END FILE: ./backend/app/utils/validation.py ---

--- START FILE: ./backend/app/routers/data.py (Size: 11622 bytes) ---
# backend/app/routers/data.py
import logging
import json
import os
import zipfile
import io
from pathlib import Path
from fastapi import APIRouter, Depends, HTTPException, Request
from fastapi.responses import StreamingResponse, FileResponse # Added StreamingResponse, FileResponse
from typing import List, Dict, Any, Generator, Optional # Added Generator, Optional
from pydantic import BaseModel # For parameter response model

# Import config and utils
from ..core.config import DATA_DIR, RESULTS_DIR
from ..utils.files import get_directory_contents, get_safe_path, get_filebrowser_config

logger = logging.getLogger(__name__)
router = APIRouter(
    tags=["Data Access"] # Tag for OpenAPI docs
    # prefix="/api" # Prefix is added in app.py
)

# --- Models ---
# Define a model for the parameters response (can mirror JobMetaInputParams if desired)
class RunParametersResponse(BaseModel):
    input_filenames: Optional[Dict[str, Optional[str]]] = None
    sarek_params: Optional[Dict[str, Any]] = None
    sample_info: Optional[List[Dict[str, Any]]] = None
    # Add other fields if the metadata file contains more

# --- Helper Functions ---
def zip_directory_generator(directory: Path) -> Generator[bytes, None, None]:
    """ Generator function to stream a zip archive of a directory. """
    buffer = io.BytesIO()
    # Use compression for potentially large files
    with zipfile.ZipFile(buffer, "w", zipfile.ZIP_DEFLATED, allowZip64=True) as zipf:
        for file_path in directory.rglob('*'): # Recursively glob all files/dirs
            if file_path.is_file():
                try:
                    # Calculate arcname relative to the directory being zipped
                    arcname = file_path.relative_to(directory).as_posix()
                    zipf.write(file_path, arcname=arcname)
                    # Yield the buffer content periodically to stream
                    # This simple approach yields after each file, might need optimization
                    buffer.seek(0)
                    yield buffer.read()
                    buffer.seek(0)
                    buffer.truncate() # Reset buffer for next chunk
                except Exception as e:
                     logger.warning(f"Error adding file {file_path} to zip: {e}")
                     # Optionally write an error marker to the zip?
                     # zipf.writestr(f"{arcname}.zip_error", f"Error adding file: {e}")

            elif file_path.is_dir() and not any(file_path.iterdir()):
                 # Add empty directories explicitly if needed
                 arcname = file_path.relative_to(directory).as_posix() + '/'
                 zipi = zipfile.ZipInfo(arcname)
                 zipi.external_attr = 0o40755 << 16 # drwxr-xr-x
                 zipf.writestr(zipi, '')

    # Yield any remaining data in the buffer
    buffer.seek(0)
    yield buffer.read()

# --- Existing Endpoints (Modified) ---

@router.get("/get_data", response_model=List[Dict[str, str]], summary="List Data Files")
async def get_data():
    """ Lists files (not directories) directly within the configured DATA_DIR. """
    if not DATA_DIR.exists() or not DATA_DIR.is_dir():
        logger.error(f"Configured DATA_DIR does not exist or is not a directory: {DATA_DIR}")
        raise HTTPException(status_code=500, detail="Server configuration error: Data directory not found.")

    fb_config = get_filebrowser_config()
    try:
        # Pass DATA_DIR as both directory_to_list and base_dir_for_relative_path
        contents = get_directory_contents(
            DATA_DIR, DATA_DIR, list_dirs=False, list_files=True, fb_base_url=fb_config["baseURL"]
        )
        response_data = [
            {"name": item.get("name", "Unknown"), "type": "file"}
            for item in contents if not item.get("is_dir") and "name" in item
        ]
        return response_data
    except HTTPException as e:
        logger.error(f"HTTPException processing /get_data: {e.detail}")
        raise e
    except Exception as e:
        logger.exception(f"Unexpected error listing data directory contents for /get_data: {e}")
        raise HTTPException(status_code=500, detail="Internal server error processing data list.")


@router.get("/get_results", response_model=List[Dict[str, Any]], summary="List Result Run Directories")
async def get_results_runs(fb_config: Dict = Depends(get_filebrowser_config)):
    """ Lists the subdirectories (pipeline runs) within the main RESULTS_DIR. """
    if not RESULTS_DIR.exists():
        logger.warning(f"Results directory not found: {RESULTS_DIR}. Returning empty list.")
        return []

    # Pass RESULTS_DIR as both directory_to_list and base_dir_for_relative_path
    return get_directory_contents(RESULTS_DIR, RESULTS_DIR, list_dirs=True, list_files=False, fb_base_url=fb_config["baseURL"])


@router.get("/get_results/{run_dir_name:path}", response_model=List[Dict[str, Any]], summary="List Files in a Specific Run Directory")
async def get_results_run_files(run_dir_name: str, fb_config: Dict = Depends(get_filebrowser_config)):
    """
    Lists the files and subdirectories within a specific pipeline run directory.
    The run_dir_name is taken as a path segment.
    """
    logger.info(f"Request to list files for run directory: '{run_dir_name}'")
    try:
        # Validate the run_dir_name and resolve the full path safely within RESULTS_DIR
        target_run_dir = get_safe_path(RESULTS_DIR, run_dir_name)

        if not target_run_dir.is_dir():
             logger.warning(f"Requested run directory not found or not a directory after validation: {target_run_dir}")
             raise HTTPException(status_code=404, detail=f"Run directory '{run_dir_name}' not found.")

    except HTTPException as e:
         raise e
    except Exception as e:
        logger.exception(f"Unexpected error validating path for run '{run_dir_name}': {e}")
        raise HTTPException(status_code=500, detail="Internal server error during path validation.")

    # Pass target_run_dir as directory_to_list, and RESULTS_DIR as base_dir_for_relative_path
    return get_directory_contents(
        target_run_dir,
        RESULTS_DIR, # Base for relative paths calculation
        list_dirs=True,
        list_files=True,
        fb_base_url=fb_config["baseURL"]
    )

# --- New Endpoints ---

@router.get("/results/{run_dir_name:path}/parameters", response_model=RunParametersResponse, summary="Get Parameters for a Run")
async def get_run_parameters(run_dir_name: str):
    """
    Attempts to read and return the parameters used for a specific pipeline run
    from a metadata file (e.g., run_metadata.json) within the run directory.
    """
    logger.info(f"Request for parameters for run: '{run_dir_name}'")
    try:
        target_run_dir = get_safe_path(RESULTS_DIR, run_dir_name)
        if not target_run_dir.is_dir():
            raise HTTPException(status_code=404, detail=f"Run directory '{run_dir_name}' not found.")

        # --- Look for metadata file ---
        # Option A: Look for run_metadata.json (preferred, assumes task saves it)
        metadata_file = target_run_dir / "run_metadata.json"
        # Option B: Fallback to pipeline_command.log (more complex parsing needed)
        # command_log_file = target_run_dir / "pipeline_command.log"

        parameters = {}
        if metadata_file.is_file():
             try:
                 with open(metadata_file, 'r') as f:
                     # Assuming the file contains the same structure as JobMeta
                     data = json.load(f)
                     # Extract relevant parts for the response model
                     parameters = RunParametersResponse(
                         input_filenames=data.get("input_params"),
                         sarek_params=data.get("sarek_params"),
                         sample_info=data.get("sample_info")
                     ).model_dump(exclude_none=True) # Use model_dump for Pydantic v2

                 logger.info(f"Successfully loaded parameters from {metadata_file}")
                 return parameters
             except (json.JSONDecodeError, OSError, KeyError) as e:
                 logger.warning(f"Failed to read or parse parameters from {metadata_file}: {e}")
                 # Continue to potentially look for other sources or return empty

        # Add parsing logic for pipeline_command.log here if needed as a fallback

        # If no parameters found from any source
        logger.warning(f"No parameter metadata found for run '{run_dir_name}'")
        # Return empty object instead of 404, frontend can display "not found"
        return RunParametersResponse().model_dump()


    except HTTPException as e:
        raise e
    except Exception as e:
        logger.exception(f"Unexpected error fetching parameters for run '{run_dir_name}': {e}")
        raise HTTPException(status_code=500, detail="Internal server error fetching run parameters.")


@router.get("/download_result/{run_dir_name:path}", summary="Download Run Directory as Zip")
async def download_result_run(run_dir_name: str):
    """
    Creates a zip archive of the specified run directory and streams the download.
    """
    logger.info(f"Request to download run directory: '{run_dir_name}'")
    try:
        target_run_dir = get_safe_path(RESULTS_DIR, run_dir_name)
        if not target_run_dir.is_dir():
            raise HTTPException(status_code=404, detail=f"Run directory '{run_dir_name}' not found.")

        # Create a safe filename for the download
        safe_filename = "".join(c if c.isalnum() or c in ['_', '-'] else '_' for c in run_dir_name) + ".zip"

        return StreamingResponse(
            zip_directory_generator(target_run_dir),
            media_type="application/zip",
            headers={"Content-Disposition": f"attachment; filename=\"{safe_filename}\""}
        )

    except HTTPException as e:
        raise e
    except Exception as e:
        logger.exception(f"Unexpected error creating zip for run '{run_dir_name}': {e}")
        raise HTTPException(status_code=500, detail="Internal server error creating zip archive.")


@router.get("/download_file/{run_dir_name:path}/{file_path:path}", summary="Download Single Result File")
async def download_result_file(run_dir_name: str, file_path: str):
    """
    Downloads a specific file from within a run directory.
    """
    logger.info(f"Request to download file '{file_path}' from run '{run_dir_name}'")
    try:
        # First, validate the run directory itself exists within RESULTS_DIR
        target_run_dir = get_safe_path(RESULTS_DIR, run_dir_name)
        if not target_run_dir.is_dir():
            raise HTTPException(status_code=404, detail=f"Run directory '{run_dir_name}' not found.")

        # Second, validate the file path exists within the validated run directory
        target_file_path = get_safe_path(target_run_dir, file_path)
        if not target_file_path.is_file():
             raise HTTPException(status_code=404, detail=f"File '{file_path}' not found within run '{run_dir_name}'.")

        # Extract filename for Content-Disposition
        filename = target_file_path.name

        return FileResponse(
            path=target_file_path,
            filename=filename,
            media_type='application/octet-stream' # Generic type for download
        )

    except HTTPException as e:
        raise e
    except Exception as e:
        logger.exception(f"Unexpected error downloading file '{file_path}' from run '{run_dir_name}': {e}")
        raise HTTPException(status_code=500, detail="Internal server error downloading file.")

--- END FILE: ./backend/app/routers/data.py ---

--- START FILE: ./backend/app/routers/jobs.py (Size: 51235 bytes) ---
# backend/app/routers/jobs.py
import logging
import json
import uuid
import time
import redis # Import redis exceptions
import os # Import os for cleanup
from typing import List, Dict, Any, Optional
from pathlib import Path # Import Path
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import JSONResponse

# RQ Imports
from rq import Queue, Worker
from rq.job import Job, JobStatus
from rq.exceptions import NoSuchJobError, InvalidJobOperation
from rq.registry import StartedJobRegistry, FinishedJobRegistry, FailedJobRegistry
from rq.command import send_stop_job_command

# App specific imports
from ..core.config import (
    STAGED_JOBS_KEY, DEFAULT_JOB_TIMEOUT,
    DEFAULT_RESULT_TTL, DEFAULT_FAILURE_TTL, MAX_REGISTRY_JOBS,
    SAREK_DEFAULT_PROFILE, SAREK_DEFAULT_TOOLS, SAREK_DEFAULT_STEP, SAREK_DEFAULT_ALIGNER,
    RESULTS_DIR
)
from ..core.redis_rq import get_redis_connection, get_pipeline_queue
# Import updated models AND the new JobStatusDetails
from ..models.pipeline import PipelineInput, SampleInfo, JobStatusDetails, JobResourceInfo # <-- ADD JobStatusDetails & JobResourceInfo HERE
# Import updated validation function
from ..utils.validation import validate_pipeline_input
from ..utils.time import dt_to_timestamp
# Import the task function
from ..tasks import run_pipeline_task

logger = logging.getLogger(__name__)
router = APIRouter(
    tags=["Jobs Management"] # Tag for OpenAPI docs
    # prefix="/api" # Prefix is added in app.py
)

# --- Job Staging and Control Routes ---

@router.post("/run_pipeline", status_code=200, summary="Stage Pipeline Job")
async def stage_pipeline_job(
    input_data: PipelineInput, # Use updated PipelineInput model
    redis_conn: redis.Redis = Depends(get_redis_connection)
):
    """
    Validates input, generates samplesheet, and stages a new Sarek pipeline job.
    The job details (including paths and parameters) are stored in Redis hash
    but not yet enqueued for execution.
    Returns a staged job ID that can be used to start the job later.
    """
    logger.info(f"Received staging request for Sarek pipeline with {len(input_data.samples)} samples.")

    # --- Validate Input and Generate Samplesheet ---
    paths_map: Dict[str, Optional[Path]]
    validation_errors: List[str]
    # Use the reverted validation function that writes host paths to CSV
    # This now includes the lane field
    paths_map, validation_errors = validate_pipeline_input(input_data)

    input_csv_path = paths_map.get("input_csv")
    if not input_csv_path and not any("At least one sample" in e for e in validation_errors):
        if "Internal server error: Could not create samplesheet." not in validation_errors:
             validation_errors.append("Failed to generate samplesheet from provided sample data.")

    if validation_errors:
        if input_csv_path and input_csv_path.exists():
             try:
                 os.remove(input_csv_path)
                 logger.info(f"Cleaned up temporary CSV file due to validation errors: {input_csv_path}")
             except OSError as e:
                 logger.warning(f"Could not clean up temporary CSV file {input_csv_path}: {e}")
        error_message = "Validation errors:\n" + "\n".join(f"- {error}" for error in validation_errors)
        logger.warning(f"Validation errors staging job: {error_message}")
        raise HTTPException(status_code=400, detail=error_message)

    if not isinstance(input_csv_path, Path):
         logger.error("Validation passed but input_csv_path is not a Path object. Aborting staging.")
         raise HTTPException(status_code=500, detail="Internal server error during job staging preparation.")

    logger.info(f"Input validation successful. Samplesheet: {input_csv_path}")

    # --- Prepare Job Details for Staging ---
    try:
        staged_job_id = f"staged_{uuid.uuid4()}"

        input_filenames = {
            "intervals_file": input_data.intervals_file,
            "dbsnp": input_data.dbsnp,
            "known_indels": input_data.known_indels,
            "pon": input_data.pon
        }
        # Include lane in the sample info being stored
        sample_info_list = [s.model_dump() for s in input_data.samples]

        # Convert tools list to comma-separated string for storage in Redis
        tools_str = ",".join(input_data.tools) if input_data.tools else None

        # Store absolute HOST paths (as strings) and parameters needed for the task execution
        # These paths come directly from paths_map which contains validated host paths now
        job_details = {
            # --- Paths ---
            "input_csv_path": str(input_csv_path), # Validated CSV path (temp file on host)
            "intervals_path": str(paths_map["intervals"]) if paths_map.get("intervals") else None,
            "dbsnp_path": str(paths_map["dbsnp"]) if paths_map.get("dbsnp") else None,
            "known_indels_path": str(paths_map["known_indels"]) if paths_map.get("known_indels") else None,
            "pon_path": str(paths_map["pon"]) if paths_map.get("pon") else None,
            "outdir_base_path": str(RESULTS_DIR), # Base directory for results (host path)

            # --- Sarek Parameters ---
            "genome": input_data.genome,
            # *** Store the comma-separated string ***
            "tools": tools_str,
            # *****************************************
            # Use defaults only if the user didn't provide a value
            "step": input_data.step if input_data.step is not None else SAREK_DEFAULT_STEP,
            "profile": input_data.profile if input_data.profile is not None else SAREK_DEFAULT_PROFILE,
            "aligner": input_data.aligner if input_data.aligner is not None else SAREK_DEFAULT_ALIGNER,

            # --- Sarek Flags ---
            "joint_germline": input_data.joint_germline or False,
            "wes": input_data.wes or False,
            "trim_fastq": input_data.trim_fastq or False,
            "skip_qc": input_data.skip_qc or False,
            "skip_annotation": input_data.skip_annotation or False,
            "skip_baserecalibrator": input_data.skip_baserecalibrator or False,

            # --- Metadata ---
            "description": input_data.description or f"Sarek run ({len(input_data.samples)} samples, Genome: {input_data.genome})",
            "staged_at": time.time(),
            "input_filenames": input_filenames, # Original relative filenames from user input
            "sample_info": sample_info_list # Sample details from user input (now includes lane)
        }

        redis_conn.hset(STAGED_JOBS_KEY, staged_job_id.encode('utf-8'), json.dumps(job_details).encode('utf-8'))
        logger.info(f"Staged Sarek job '{staged_job_id}' with {len(input_data.samples)} samples.")

        return JSONResponse(status_code=200, content={"message": "Job staged successfully.", "staged_job_id": staged_job_id})

    except redis.exceptions.RedisError as e:
        logger.error(f"Redis error staging job: {e}")
        if input_csv_path and input_csv_path.exists():
             try:
                 os.remove(input_csv_path)
                 logger.info(f"Cleaned up temporary CSV file due to Redis error: {input_csv_path}")
             except OSError as remove_e:
                 logger.warning(f"Could not clean up temporary CSV file {input_csv_path} after Redis error: {remove_e}")
        raise HTTPException(status_code=503, detail="Service unavailable: Could not stage job due to storage error.")
    except Exception as e:
         logger.exception(f"Unexpected error during job staging for input: {input_data}")
         if input_csv_path and input_csv_path.exists():
             try:
                 os.remove(input_csv_path)
                 logger.info(f"Cleaned up temporary CSV file due to unexpected error: {input_csv_path}")
             except OSError as remove_e:
                 logger.warning(f"Could not clean up temporary CSV file {input_csv_path} after error: {remove_e}")
         raise HTTPException(status_code=500, detail="Internal server error during job staging.")


@router.post("/start_job/{staged_job_id}", status_code=202, summary="Enqueue Staged Job")
async def start_job(
    staged_job_id: str,
    redis_conn: redis.Redis = Depends(get_redis_connection),
    queue: Queue = Depends(get_pipeline_queue)
):
    """
    Retrieves staged job details from Redis, enqueues it to RQ for execution
    via run_pipeline_task, and removes the staged entry upon success.
    Returns 202 Accepted with the new RQ job ID.
    """
    logger.info(f"Attempting to start job from staged ID: {staged_job_id}")
    job_details = None
    try:
        job_details_bytes = redis_conn.hget(STAGED_JOBS_KEY, staged_job_id.encode('utf-8'))
        if not job_details_bytes:
            logger.warning(f"Start job request failed: Staged job ID '{staged_job_id}' not found.")
            raise HTTPException(status_code=404, detail=f"Staged job '{staged_job_id}' not found.")

        try:
            job_details = json.loads(job_details_bytes.decode('utf-8'))
        except (json.JSONDecodeError, UnicodeDecodeError) as e:
            logger.error(f"Corrupted staged job data for {staged_job_id}: {e}. Removing entry.")
            redis_conn.hdel(STAGED_JOBS_KEY, staged_job_id.encode('utf-8'))
            raise HTTPException(status_code=500, detail="Corrupted staged job data found. Please try staging again.")

        # --- Validate required keys (use defaults if needed during enqueue) ---
        required_base_keys = ["input_csv_path", "outdir_base_path", "genome"]
        if not all(key in job_details for key in required_base_keys):
            missing_keys = [key for key in required_base_keys if key not in job_details]
            logger.error(f"Corrupted staged job data for {staged_job_id}: Missing required keys: {missing_keys}. Data: {job_details}")
            redis_conn.hdel(STAGED_JOBS_KEY, staged_job_id.encode('utf-8'))
            raise HTTPException(status_code=500, detail="Incomplete staged job data found. Please try staging again.")

        # --- Prepare arguments for the RQ task (run_pipeline_task) ---
        # Pass the comma-separated string 'tools' value directly. Task handles default.
        job_args = (
            job_details["input_csv_path"],
            job_details["outdir_base_path"],
            job_details["genome"],
            job_details.get("tools"), # Pass stored comma-separated string or None
            job_details.get("step", SAREK_DEFAULT_STEP), # Use default if not set in staging
            job_details.get("profile", SAREK_DEFAULT_PROFILE),
            job_details.get("intervals_path"), # Will be None if not provided
            job_details.get("dbsnp_path"),     # Will be None if not provided
            job_details.get("known_indels_path"), # Will be None if not provided
            job_details.get("pon_path"),       # Will be None if not provided
            job_details.get("aligner", SAREK_DEFAULT_ALIGNER),
            job_details.get("joint_germline", False),
            job_details.get("wes", False),
            job_details.get("trim_fastq", False),
            job_details.get("skip_qc", False),
            job_details.get("skip_annotation", False),
            job_details.get("skip_baserecalibrator", False),
            # *** ADDED is_rerun argument ***
            job_details.get("is_rerun", False),
            # *******************************
        )

        # --- Enqueue the job to RQ ---
        try:
            # Use a new job ID for the RQ job, derived from the staged ID but distinct
            rq_job_id = staged_job_id.replace("staged_", "running_")
            if rq_job_id == staged_job_id: # Ensure it actually changed
                rq_job_id = f"running_{uuid.uuid4()}" # Fallback to totally new ID

            # Check if this RQ job ID already exists (e.g., from a previous failed attempt to start)
            try:
                 existing_job = Job.fetch(rq_job_id, connection=redis_conn)
                 if existing_job:
                     logger.warning(f"RQ job {rq_job_id} already exists (Status: {existing_job.get_status()}). Generating new ID.")
                     rq_job_id = f"running_{uuid.uuid4()}"
            except NoSuchJobError:
                pass # Job ID is available

            # Store original staged parameters within the RQ job's meta for later reference (like rerun)
            job_meta_to_store = {
                 "staged_job_id_origin": staged_job_id,
                 "input_params": job_details.get("input_filenames"),
                 "sarek_params": {
                     "genome": job_details.get("genome"),
                     "tools": job_details.get("tools"), # Store the comma-separated string
                     "step": job_details.get("step"),
                     "profile": job_details.get("profile"),
                     "aligner": job_details.get("aligner"),
                     "joint_germline": job_details.get("joint_germline", False),
                     "wes": job_details.get("wes", False),
                     "trim_fastq": job_details.get("trim_fastq", False),
                     "skip_qc": job_details.get("skip_qc", False),
                     "skip_annotation": job_details.get("skip_annotation", False),
                     "skip_baserecalibrator": job_details.get("skip_baserecalibrator", False),
                 },
                 "sample_info": job_details.get("sample_info"), # Includes lane
                 "description": job_details.get("description"),
                 # Store the input CSV path used, in case needed for debugging later
                 "input_csv_path_used": job_details.get("input_csv_path"),
                 # Store the is_rerun flag used for this specific execution
                 "is_rerun_execution": job_details.get("is_rerun", False),
            }


            rq_job = queue.enqueue(
                run_pipeline_task,
                args=job_args,
                job_timeout=DEFAULT_JOB_TIMEOUT,
                result_ttl=DEFAULT_RESULT_TTL,
                failure_ttl=DEFAULT_FAILURE_TTL,
                job_id=rq_job_id,
                meta=job_meta_to_store # Store the parameters in meta
            )
            logger.info(f"Successfully enqueued job {rq_job.id} to RQ queue.")
        except Exception as e:
            logger.error(f"Failed to enqueue job to RQ: {e}")
            raise HTTPException(status_code=503, detail="Service unavailable: Could not enqueue job for execution.")

        # --- Clean up staged job entry ---
        try:
            redis_conn.hdel(STAGED_JOBS_KEY, staged_job_id.encode('utf-8'))
            logger.info(f"Removed staged job entry {staged_job_id} after successful enqueue.")
        except redis.exceptions.RedisError as e:
            logger.warning(f"Could not remove staged job entry {staged_job_id} after enqueue: {e}")
            # Don't fail the request if cleanup fails

        return JSONResponse(
            status_code=202,
            content={
                "message": "Job enqueued for execution.",
                "job_id": rq_job.id,
                "status": "queued"
            }
        )

    except redis.exceptions.RedisError as e:
        logger.error(f"Redis error starting job: {e}")
        raise HTTPException(status_code=503, detail="Service unavailable: Could not start job due to storage error.")
    except Exception as e:
        logger.exception(f"Unexpected error starting job {staged_job_id}")
        raise HTTPException(status_code=500, detail="Internal server error during job start.")


# --- Job Listing and Status Routes ---

@router.get("/jobs_list", response_model=List[Dict[str, Any]], summary="List All Relevant Jobs (Staged & RQ)")
async def get_jobs_list(
    redis_conn: redis.Redis = Depends(get_redis_connection),
    queue: Queue = Depends(get_pipeline_queue) # Need queue for serializer info
):
    """
    Fetches and combines jobs from the staging area (Redis Hash) and
    various RQ registries (queued, started, finished, failed).
    Returns a list sorted by enqueue/stage time descending (newest first).
    """
    all_jobs_dict = {}

    # 1. Get Staged Jobs from Redis Hash
    try:
        staged_jobs_raw = redis_conn.hgetall(STAGED_JOBS_KEY)
        for job_id_bytes, job_details_bytes in staged_jobs_raw.items():
            try:
                job_id = job_id_bytes.decode('utf-8')
                details = json.loads(job_details_bytes.decode('utf-8'))
                # Construct meta based on *stored* details
                # Ensure sample_info includes lane if present in stored details
                staged_meta = {
                    "input_params": details.get("input_filenames", {}),
                    "sarek_params": {
                         "genome": details.get("genome"),
                         "tools": details.get("tools"), # Reflects stored value (comma-separated string or None)
                         "step": details.get("step"),
                         "profile": details.get("profile"),
                         "aligner": details.get("aligner"),
                         "joint_germline": details.get("joint_germline", False),
                         "wes": details.get("wes", False),
                         "trim_fastq": details.get("trim_fastq", False),
                         "skip_qc": details.get("skip_qc", False),
                         "skip_annotation": details.get("skip_annotation", False),
                         "skip_baserecalibrator": details.get("skip_baserecalibrator", False),
                    },
                    "sample_info": details.get("sample_info", []), # Should contain lane if stored correctly
                    "staged_job_id_origin": job_id
                }
                all_jobs_dict[job_id] = {
                    "id": job_id,
                    "status": "staged",
                    "description": details.get("description", f"Staged: {job_id[:8]}..."),
                    "enqueued_at": None,
                    "started_at": None,
                    "ended_at": None,
                    "result": None,
                    "error": None,
                    "meta": staged_meta,
                    "staged_at": details.get("staged_at"),
                    "resources": None
                }
            except (UnicodeDecodeError, json.JSONDecodeError, TypeError) as e:
                logger.error(f"Error decoding/parsing staged job data for key {job_id_bytes}: {e}. Skipping entry.")
    except redis.exceptions.RedisError as e:
        logger.error(f"Redis error fetching staged jobs from '{STAGED_JOBS_KEY}': {e}")

    # 2. Get RQ Jobs from Relevant Registries
    registries_to_check = {
        "queued": queue,
        "started": StartedJobRegistry(queue=queue),
        "finished": FinishedJobRegistry(queue=queue),
        "failed": FailedJobRegistry(queue=queue),
    }
    rq_job_ids_to_fetch = set()
    for status_name, registry_or_queue in registries_to_check.items():
        try:
            job_ids = []
            limit = MAX_REGISTRY_JOBS if status_name in ["finished", "failed"] else -1
            if isinstance(registry_or_queue, Queue):
                job_ids = registry_or_queue.get_job_ids()
            elif isinstance(registry_or_queue, (StartedJobRegistry, FinishedJobRegistry, FailedJobRegistry)):
                # Fetch most recent MAX_REGISTRY_JOBS
                total_count = registry_or_queue.count
                start_index = max(0, total_count - limit) if limit > 0 else 0
                end_index = total_count -1
                if start_index <= end_index:
                     job_ids = registry_or_queue.get_job_ids(start_index, end_index)
                     job_ids.reverse() # Show newest first within the limit

            else:
                 logger.warning(f"Unsupported type for job fetching: {type(registry_or_queue)}")
                 continue
            if job_ids:
                 rq_job_ids_to_fetch.update(job_ids)
        except redis.exceptions.RedisError as e:
            logger.error(f"Redis error fetching job IDs from {status_name} registry/queue: {e}")
        except Exception as e:
            logger.exception(f"Unexpected error fetching job IDs from {status_name} registry/queue.")

    # Fetch all unique RQ job IDs found across registries
    if rq_job_ids_to_fetch:
        try:
            # Use a separate connection for fetch_many that decodes responses=False
            redis_conn_bytes = redis.Redis(
                host=redis_conn.connection_pool.connection_kwargs.get('host', 'localhost'),
                port=redis_conn.connection_pool.connection_kwargs.get('port', 6379),
                db=redis_conn.connection_pool.connection_kwargs.get('db', 0),
                decode_responses=False # Crucial for RQ job fetching
            )
            fetched_jobs = Job.fetch_many(list(rq_job_ids_to_fetch), connection=redis_conn_bytes, serializer=queue.serializer)

            for job in fetched_jobs:
                if job:
                    job.refresh() # Fetch latest status and meta
                    current_status = job.get_status(refresh=False) # Use cached status after refresh
                    error_summary = None
                    job_meta = job.meta or {} # Use fetched meta

                    if current_status == JobStatus.FAILED:
                         error_summary = job_meta.get('error_message', "Job failed processing")
                         stderr_snippet = job_meta.get('stderr_snippet')
                         # Use exc_info if available and error_message is generic
                         if error_summary == "Job failed processing" and job.exc_info:
                             try: error_summary = job.exc_info.strip().split('\n')[-1]
                             except Exception: pass # Ignore errors parsing exc_info
                         if stderr_snippet: error_summary += f" (stderr: {stderr_snippet}...)"

                    # Extract resource info from meta
                    resources = {
                        "peak_memory_mb": job_meta.get("peak_memory_mb"),
                        "average_cpu_percent": job_meta.get("average_cpu_percent"),
                        "duration_seconds": job_meta.get("duration_seconds")
                    }

                    # Add or update the job in our dictionary
                    # Ensure we don't overwrite a running/finished job with a stale staged entry if IDs clash
                    if job.id not in all_jobs_dict or all_jobs_dict[job.id].get('status') == 'staged':
                         all_jobs_dict[job.id] = {
                            "id": job.id,
                            "status": current_status,
                            # Use description from meta if available, fallback to job.description or generic
                            "description": job_meta.get("description") or job.description or f"RQ job {job.id[:12]}...",
                            "enqueued_at": dt_to_timestamp(job.enqueued_at),
                            "started_at": dt_to_timestamp(job.started_at),
                            "ended_at": dt_to_timestamp(job.ended_at),
                            "result": job.result, # Result directly from job object
                            "error": error_summary,
                            "meta": job_meta, # Use the full meta fetched
                            "staged_at": None, # Not a staged job anymore
                            "resources": resources if any(v is not None for v in resources.values()) else None
                        }
        except redis.exceptions.RedisError as e:
             logger.error(f"Redis error during Job.fetch_many: {e}")
             # Don't raise HTTPException here, return potentially partial list
        except Exception as e:
            logger.exception("Unexpected error fetching RQ job details.")
            # Don't raise HTTPException here

    # 3. Sort the Combined List
    try:
        # Sort primarily by last update time (ended > started > enqueued > staged) descending
        all_jobs_list = sorted(
            all_jobs_dict.values(),
            key=lambda j: j.get('ended_at') or j.get('started_at') or j.get('enqueued_at') or j.get('staged_at') or 0,
            reverse=True
        )
    except Exception as e:
        logger.exception("Error sorting combined jobs list.")
        all_jobs_list = list(all_jobs_dict.values()) # Fallback to unsorted if error

    # Limit the number of finished/failed jobs returned if MAX_REGISTRY_JOBS is set
    if MAX_REGISTRY_JOBS > 0:
        final_list = []
        finished_failed_count = 0
        for job_item in all_jobs_list:
             status = job_item.get('status', '').lower()
             is_terminal = status in ['finished', 'failed', 'stopped', 'canceled']
             if is_terminal:
                 if finished_failed_count < MAX_REGISTRY_JOBS:
                     final_list.append(job_item)
                     finished_failed_count += 1
             else:
                 final_list.append(job_item) # Always include non-terminal jobs
        all_jobs_list = final_list


    return all_jobs_list


@router.get("/job_status/{job_id}", response_model=JobStatusDetails, summary="Get RQ Job Status and Details")
async def get_job_status(
    job_id: str,
    redis_conn: redis.Redis = Depends(get_redis_connection),
    queue: Queue = Depends(get_pipeline_queue) # Need queue for serializer
):
    """
    Fetches the status, result/error, metadata, and resource usage for a specific RQ job ID.
    Handles cases where the job might not be found in RQ (checks staged).
    """
    logger.debug(f"Fetching status for job ID: {job_id}")
    try:
        # --- Check if it's an RQ Job ID first ---
        if not job_id.startswith("staged_"):
            try:
                redis_conn_bytes = redis.Redis(
                    host=redis_conn.connection_pool.connection_kwargs.get('host', 'localhost'),
                    port=redis_conn.connection_pool.connection_kwargs.get('port', 6379),
                    db=redis_conn.connection_pool.connection_kwargs.get('db', 0),
                    decode_responses=False # Required for RQ
                )
                job = Job.fetch(job_id, connection=redis_conn_bytes, serializer=queue.serializer)
                job.refresh() # Get the latest status and meta

                status = job.get_status(refresh=False)
                result = None
                meta_data = job.meta or {}
                error_info_summary = None

                try:
                    if status == JobStatus.FINISHED:
                        result = job.result
                    elif status == JobStatus.FAILED:
                        error_info_summary = meta_data.get('error_message', "Job failed processing")
                        stderr_snippet = meta_data.get('stderr_snippet')
                        if error_info_summary == "Job failed processing" and job.exc_info:
                            try: error_info_summary = job.exc_info.strip().split('\n')[-1]
                            except Exception: pass
                        if stderr_snippet: error_info_summary += f" (stderr: {stderr_snippet}...)"
                except Exception as e:
                    logger.exception(f"Error accessing result/error info for job {job_id} (status: {status}).")
                    error_info_summary = error_info_summary or "Could not retrieve job result/error details."

                resource_stats = {
                    "peak_memory_mb": meta_data.get("peak_memory_mb"),
                    "average_cpu_percent": meta_data.get("average_cpu_percent"),
                    "duration_seconds": meta_data.get("duration_seconds")
                }

                return JobStatusDetails(
                    job_id=job.id,
                    status=status,
                    description=meta_data.get("description") or job.description, # Prefer meta description
                    enqueued_at=dt_to_timestamp(job.enqueued_at),
                    started_at=dt_to_timestamp(job.started_at),
                    ended_at=dt_to_timestamp(job.ended_at),
                    result=result,
                    error=error_info_summary,
                    meta=meta_data,
                    resources=JobResourceInfo(**resource_stats) if any(v is not None for v in resource_stats.values()) else None
                )

            except NoSuchJobError:
                logger.warning(f"RQ Job ID '{job_id}' not found.")
                # Fall through to check staged jobs if it wasn't found in RQ
            except redis.exceptions.RedisError as e:
                logger.error(f"Redis error fetching RQ job {job_id}: {e}")
                raise HTTPException(status_code=503, detail="Service unavailable: Could not connect to status backend.")
            except Exception as e:
                logger.exception(f"Unexpected error fetching or refreshing RQ job {job_id}.")
                raise HTTPException(status_code=500, detail="Internal server error fetching job status.")


        # --- Check if it's a Staged Job ID ---
        if job_id.startswith("staged_"):
            try:
                 staged_details_bytes = redis_conn.hget(STAGED_JOBS_KEY, job_id.encode('utf-8'))
                 if staged_details_bytes:
                     logger.info(f"Job ID {job_id} corresponds to a currently staged job.")
                     try:
                         details = json.loads(staged_details_bytes.decode('utf-8'))
                         # Construct meta including sample_info with lane
                         staged_meta = {
                             "input_params": details.get("input_filenames", {}),
                             "sarek_params": {
                                  "genome": details.get("genome"), "tools": details.get("tools"),
                                  "step": details.get("step"), "profile": details.get("profile"),
                                  "aligner": details.get("aligner"),
                                  "joint_germline": details.get("joint_germline", False),
                                  "wes": details.get("wes", False), "trim_fastq": details.get("trim_fastq", False),
                                  "skip_qc": details.get("skip_qc", False),
                                  "skip_annotation": details.get("skip_annotation", False),
                                  "skip_baserecalibrator": details.get("skip_baserecalibrator", False),
                             },
                             "sample_info": details.get("sample_info", []), # Should contain lane
                             "staged_job_id_origin": job_id,
                             "description": details.get("description"),
                         }
                         return JobStatusDetails(
                             job_id=job_id, status="staged",
                             description=details.get("description"),
                             enqueued_at=None, started_at=None, ended_at=None,
                             result=None, error=None, meta=staged_meta, resources=None
                         )
                     except (json.JSONDecodeError, UnicodeDecodeError, TypeError) as parse_err:
                         logger.error(f"Error parsing staged job details for {job_id} in status check: {parse_err}")
                         # Fall through to 404 if parsing fails
                 else:
                     logger.warning(f"Staged job ID '{job_id}' not found in Redis hash.")

            except redis.exceptions.RedisError as e:
                 logger.error(f"Redis error checking staged status for {job_id}: {e}")
                 # Fall through to 404


        # --- If not found in RQ or Staged ---
        raise HTTPException(status_code=404, detail=f"Job '{job_id}' not found.")

    except HTTPException as http_exc:
        raise http_exc # Re-raise FastAPI exceptions
    except Exception as e:
         logger.exception(f"Unexpected error in get_job_status for {job_id}.")
         raise HTTPException(status_code=500, detail="Internal server error retrieving job status.")


@router.post("/stop_job/{job_id}", status_code=200, summary="Cancel Running/Queued RQ Job")
async def stop_job(
    job_id: str,
    redis_conn: redis.Redis = Depends(get_redis_connection)
):
    """
    Sends a stop signal to a specific RQ job if it's running or queued.
    Does NOT stop 'staged' jobs.
    """
    if job_id.startswith("staged_"):
        raise HTTPException(status_code=400, detail="Cannot stop a 'staged' job. Remove it instead.")

    logger.info(f"Received request to stop RQ job: {job_id}")
    try:
        redis_conn_bytes = redis.Redis(host=redis_conn.connection_pool.connection_kwargs.get('host', 'localhost'),
                                       port=redis_conn.connection_pool.connection_kwargs.get('port', 6379),
                                       db=redis_conn.connection_pool.connection_kwargs.get('db', 0),
                                       decode_responses=False)

        job = Job.fetch(job_id, connection=redis_conn_bytes)
        status = job.get_status(refresh=True)

        if job.is_finished or job.is_failed or job.is_stopped or job.is_canceled:
            logger.warning(f"Attempted to stop job {job_id} which is already in state: {status}")
            return JSONResponse(status_code=200, content={"message": f"Job already in terminal state: {status}.", "job_id": job_id})

        logger.info(f"Job {job_id} is in state {status}. Attempting to send stop signal.")
        message = f"Stop signal sent to job {job_id}."
        try:
            send_stop_job_command(redis_conn_bytes, job.id)
            logger.info(f"Successfully sent stop signal command via RQ for job {job_id}.")
        except Exception as sig_err:
            logger.warning(f"Could not send stop signal command via RQ for job {job_id}. Worker may not stop immediately. Error: {sig_err}")
            message = f"Stop signal attempted for job {job_id} (check worker logs)."

        return JSONResponse(status_code=200, content={"message": message, "job_id": job_id})

    except NoSuchJobError:
        logger.warning(f"Stop job request failed: Job ID '{job_id}' not found.")
        raise HTTPException(status_code=404, detail=f"Cannot stop job: Job '{job_id}' not found.")
    except redis.exceptions.RedisError as e:
         logger.error(f"Redis error interacting with job {job_id} for stopping: {e}")
         raise HTTPException(status_code=503, detail="Service unavailable: Could not connect to job backend.")
    except Exception as e:
        logger.exception(f"Unexpected error stopping job {job_id}.")
        raise HTTPException(status_code=500, detail="Internal server error attempting to stop job.")


@router.delete("/remove_job/{job_id}", status_code=200, summary="Remove Staged or RQ Job Data")
async def remove_job(
    job_id: str,
    redis_conn: redis.Redis = Depends(get_redis_connection),
    queue: Queue = Depends(get_pipeline_queue)
):
    """
    Removes a job's data from Redis. Handles both 'staged_*' IDs and RQ job IDs.
    Also cleans up the temporary input CSV file if found in meta.
    """
    logger.info(f"Request received to remove job/data for ID: {job_id}")
    csv_path_to_remove = None

    # --- Case 1: Handle Staged Jobs ---
    if job_id.startswith("staged_"):
        logger.info(f"Attempting to remove staged job '{job_id}' from hash '{STAGED_JOBS_KEY}'.")
        try:
            job_details_bytes = redis_conn.hget(STAGED_JOBS_KEY, job_id.encode('utf-8'))
            if job_details_bytes:
                try:
                    details = json.loads(job_details_bytes.decode('utf-8'))
                    csv_path_to_remove = details.get("input_csv_path")
                except (json.JSONDecodeError, UnicodeDecodeError):
                     logger.warning(f"Could not parse details for staged job {job_id} during removal, cannot identify CSV.")

            num_deleted = redis_conn.hdel(STAGED_JOBS_KEY, job_id.encode('utf-8'))

            if num_deleted == 1:
                logger.info(f"Successfully removed staged job entry: {job_id}")
                # Attempt cleanup outside the main try/except for Redis errors
            else:
                logger.warning(f"Staged job '{job_id}' not found in hash for removal.")
                raise HTTPException(status_code=404, detail=f"Staged job '{job_id}' not found.")

        except redis.exceptions.RedisError as e:
            logger.error(f"Redis error removing staged job {job_id}: {e}")
            raise HTTPException(status_code=503, detail="Service unavailable: Could not remove job due to storage error.")
        except HTTPException as e:
            raise e # Re-raise 404 etc.
        except Exception as e:
            logger.exception(f"Unexpected error during staged job removal check for {job_id}.")
            raise HTTPException(status_code=500, detail="Internal server error removing staged job.")

    # --- Case 2: Handle RQ Jobs ---
    else:
        logger.info(f"Attempting to remove RQ job '{job_id}' data.")
        try:
            redis_conn_bytes = redis.Redis(
                host=redis_conn.connection_pool.connection_kwargs.get('host', 'localhost'),
                port=redis_conn.connection_pool.connection_kwargs.get('port', 6379),
                db=redis_conn.connection_pool.connection_kwargs.get('db', 0),
                decode_responses=False,
                socket_timeout=5,
                socket_connect_timeout=5
            )

            try:
                job = Job.fetch(job_id, connection=redis_conn_bytes, serializer=queue.serializer)
                # Get CSV path from meta before potentially deleting the job
                if job and job.meta:
                    csv_path_to_remove = job.meta.get("input_csv_path_used") or job.meta.get("input_csv_path")

            except NoSuchJobError:
                logger.warning(f"RQ Job '{job_id}' not found for removal.")
                raise HTTPException(status_code=404, detail=f"RQ Job '{job_id}' not found.")
            except Exception as fetch_err:
                logger.error(f"Error fetching job {job_id}: {fetch_err}")
                raise HTTPException(status_code=500, detail=f"Could not fetch job {job_id} for removal")

            if not job:
                 raise HTTPException(status_code=404, detail=f"Job {job_id} not found") # Should be caught by NoSuchJobError

            try: job_status = job.get_status()
            except Exception as status_err: logger.error(f"Error getting status for job {job_id}: {status_err}"); job_status = None

            if job_status == 'started':
                try:
                    logger.info(f"Sending stop signal to running job {job_id} before removal")
                    send_stop_job_command(redis_conn_bytes, job.id)
                    time.sleep(1) # Brief pause, though stop is not guaranteed synchronous
                except Exception as stop_err:
                    logger.warning(f"Could not stop running job {job_id} before removal: {stop_err}")

            try:
                # Remove from all relevant registries
                for registry_func in [queue.remove, StartedJobRegistry(queue=queue).remove, FinishedJobRegistry(queue=queue).remove, FailedJobRegistry(queue=queue).remove]:
                    try:
                        registry_func(job, delete_job=False) # Remove from registry, don't delete the job data yet
                    except InvalidJobOperation:
                         logger.debug(f"Job {job_id} not in registry for removal.")
                    except Exception as reg_err:
                        logger.warning(f"Error removing job {job_id} from a registry: {reg_err}")

                # Now delete the job data itself
                job.delete(remove_from_registries=False) # Already removed from registries
                logger.info(f"Successfully deleted RQ job data for {job_id}")

            except InvalidJobOperation as e:
                 # This might happen if trying to delete a job that's actively running and locked
                 logger.warning(f"Invalid operation trying to remove RQ job {job_id}: {e}")
                 raise HTTPException(status_code=409, detail=f"Cannot remove job '{job_id}': Invalid operation (job might be active or locked). Try stopping first.")
            except Exception as delete_err:
                logger.error(f"Error deleting job {job_id}: {delete_err}")
                raise HTTPException(status_code=500, detail=f"Could not delete job {job_id}")

        except HTTPException as e: raise e # Re-raise specific HTTP errors
        except redis.exceptions.RedisError as e:
            logger.error(f"Redis error removing RQ job {job_id}: {e}")
            raise HTTPException(status_code=503, detail="Service unavailable: Could not remove RQ job due to storage error.")
        except Exception as e:
            logger.exception(f"Unexpected error during RQ job removal for {job_id}: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Internal server error removing RQ job: {str(e)}")

    # --- Common Cleanup Logic ---
    if csv_path_to_remove:
        try:
            csv_path = Path(csv_path_to_remove)
            # Basic safety check: Ensure it's a CSV file in a plausible temp location or results subdir
            if csv_path.exists() and csv_path.is_file() and csv_path.suffix == '.csv':
                 # More safety: Check if it's within expected parent dirs (e.g., system temp or results base)
                 # This is a basic check, adjust based on where temp files are actually created
                 # temp_dir = Path(tempfile.gettempdir())
                 # if csv_path.parent == temp_dir or RESULTS_DIR in csv_path.parents:
                 os.remove(csv_path)
                 logger.info(f"Cleaned up temporary CSV file for removed job {job_id}: {csv_path}")
                 # else:
                 #     logger.warning(f"Skipping removal of CSV file outside expected temp/results area: {csv_path}")
            else:
                 logger.warning(f"Temporary CSV path {csv_path} not found, invalid, or not a CSV for removal associated with job {job_id}.")
        except OSError as e:
            logger.warning(f"Could not clean up temporary CSV file {csv_path_to_remove} for job {job_id}: {e}")
        except Exception as e:
             logger.warning(f"Unexpected error during CSV cleanup for job {job_id}: {e}")


    return JSONResponse(status_code=200, content={"message": f"Successfully removed job {job_id}.", "removed_id": job_id})


@router.post("/rerun_job/{job_id}", status_code=202, summary="Re-stage Failed/Finished Job")
async def rerun_job(
    job_id: str,
    redis_conn: redis.Redis = Depends(get_redis_connection),
    queue: Queue = Depends(get_pipeline_queue)
):
    """
    Re-stages a previously run (failed or finished) job using the parameters
    stored in its RQ job meta. Creates a *new* staged job entry.
    """
    if job_id.startswith("staged_"):
        raise HTTPException(status_code=400, detail="Cannot re-run a job that is still 'staged'. Start it first.")

    logger.info(f"Attempting to re-stage job based on RQ job: {job_id}")
    try:
        # Fetch the original RQ job details
        redis_conn_bytes = redis.Redis(
            host=redis_conn.connection_pool.connection_kwargs.get('host', 'localhost'),
            port=redis_conn.connection_pool.connection_kwargs.get('port', 6379),
            db=redis_conn.connection_pool.connection_kwargs.get('db', 0),
            decode_responses=False
        )
        try:
             original_job = Job.fetch(job_id, connection=redis_conn_bytes, serializer=queue.serializer)
        except NoSuchJobError:
            logger.warning(f"Re-stage request failed: Original RQ job ID '{job_id}' not found.")
            raise HTTPException(status_code=404, detail=f"Original job '{job_id}' not found to re-stage.")

        if not original_job.meta:
            logger.error(f"Cannot re-stage job {job_id}: Original job metadata is missing.")
            raise HTTPException(status_code=400, detail=f"Cannot re-stage job {job_id}: Missing original parameters.")

        original_meta = original_job.meta
        original_sarek_params = original_meta.get("sarek_params", {})
        original_input_params = original_meta.get("input_params", {})
        original_sample_info = original_meta.get("sample_info", [])
        original_description = original_meta.get("description", f"Sarek run")

        # --- Create a new temporary CSV for the re-run ---
        # We need to recreate the samplesheet as the original temp one was likely deleted.
        if not original_sample_info:
             raise HTTPException(status_code=400, detail=f"Cannot re-stage job {job_id}: Original sample info missing from metadata.")

        new_sample_rows_for_csv = []
        for sample_data in original_sample_info:
             # Assume sample_data structure matches SampleInfo model (including lane)
             new_sample_rows_for_csv.append([
                 sample_data.get('patient'), sample_data.get('sample'), sample_data.get('sex'),
                 sample_data.get('status'), sample_data.get('lane'),
                 sample_data.get('fastq_1'), sample_data.get('fastq_2')
             ])

        new_temp_csv_file_path = None
        try:
            with tempfile.NamedTemporaryFile(mode='w', newline='', suffix='.csv', delete=False) as temp_csv:
                csv_writer = csv.writer(temp_csv)
                csv_writer.writerow(['patient', 'sample', 'sex', 'status', 'lane', 'fastq_1', 'fastq_2'])
                csv_writer.writerows(new_sample_rows_for_csv)
                new_temp_csv_file_path = temp_csv.name
                logger.info(f"Created new temporary samplesheet for re-run: {new_temp_csv_file_path}")
        except (OSError, csv.Error) as e:
             logger.error(f"Failed to create new temporary samplesheet for re-run of {job_id}: {e}")
             raise HTTPException(status_code=500, detail="Internal server error: Could not create samplesheet for re-run.")

        # --- Create details for the new staged job ---
        new_staged_job_id = f"staged_{uuid.uuid4()}"

        # Reconstruct paths from original meta (these should be absolute host paths)
        # Use original_input_params for filenames and reconstruct full paths if needed,
        # but the task function expects full paths directly.
        # Let's assume the original sarek_params and input_params hold enough info.
        intervals_path = original_meta.get("intervals_path") or \
                         (Path(DATA_DIR / original_input_params["intervals_file"]).as_posix() if original_input_params.get("intervals_file") else None)
        dbsnp_path = original_meta.get("dbsnp_path") or \
                     (Path(DATA_DIR / original_input_params["dbsnp"]).as_posix() if original_input_params.get("dbsnp") else None)
        known_indels_path = original_meta.get("known_indels_path") or \
                            (Path(DATA_DIR / original_input_params["known_indels"]).as_posix() if original_input_params.get("known_indels") else None)
        pon_path = original_meta.get("pon_path") or \
                   (Path(DATA_DIR / original_input_params["pon"]).as_posix() if original_input_params.get("pon") else None)

        new_job_details = {
            "input_csv_path": new_temp_csv_file_path, # Use the NEWLY created CSV path
            "intervals_path": intervals_path,
            "dbsnp_path": dbsnp_path,
            "known_indels_path": known_indels_path,
            "pon_path": pon_path,
            "outdir_base_path": str(RESULTS_DIR),

            "genome": original_sarek_params.get("genome", "GATK.GRCh38"), # Provide default if missing
            "tools": original_sarek_params.get("tools"), # Comma-separated string or None
            "step": original_sarek_params.get("step", SAREK_DEFAULT_STEP),
            "profile": original_sarek_params.get("profile", SAREK_DEFAULT_PROFILE),
            "aligner": original_sarek_params.get("aligner", SAREK_DEFAULT_ALIGNER),

            "joint_germline": original_sarek_params.get("joint_germline", False),
            "wes": original_sarek_params.get("wes", False),
            "trim_fastq": original_sarek_params.get("trim_fastq", False),
            "skip_qc": original_sarek_params.get("skip_qc", False),
            "skip_annotation": original_sarek_params.get("skip_annotation", False),
            "skip_baserecalibrator": original_sarek_params.get("skip_baserecalibrator", False),

            "description": f"Re-run of job {job_id} ({original_description})",
            "staged_at": time.time(),
            "input_filenames": original_input_params, # Keep original input filenames for reference
            "sample_info": original_sample_info, # Keep original sample details
            "is_rerun": True, # Mark this specifically for the task execution
            "original_job_id": job_id, # Reference the original job
        }

        # Store the new staged job
        redis_conn.hset(STAGED_JOBS_KEY, new_staged_job_id.encode('utf-8'), json.dumps(new_job_details).encode('utf-8'))
        logger.info(f"Created new staged job {new_staged_job_id} for re-run of {job_id}")

        # Return the staged job ID - user needs to manually start it
        return JSONResponse(
            status_code=200, # Return 200 OK as staging is complete
            content={
                 "message": f"Job {job_id} re-staged successfully as {new_staged_job_id}. Please start the new job.",
                 "staged_job_id": new_staged_job_id # Return the NEW staged ID
            }
        )
        # Alternatively, automatically start the re-staged job:
        # return await start_job(new_staged_job_id, redis_conn, queue)

    except redis.exceptions.RedisError as e:
        logger.error(f"Redis error during job re-stage for {job_id}: {e}")
        # Clean up new CSV if created before error
        if new_temp_csv_file_path and Path(new_temp_csv_file_path).exists():
            try: os.remove(new_temp_csv_file_path)
            except OSError: pass
        raise HTTPException(status_code=503, detail="Service unavailable: Could not access job storage.")
    except HTTPException as e:
         # Clean up new CSV if created before error
        if new_temp_csv_file_path and Path(new_temp_csv_file_path).exists():
            try: os.remove(new_temp_csv_file_path)
            except OSError: pass
        raise e # Re-raise FastAPI exceptions
    except Exception as e:
        logger.exception(f"Unexpected error during job re-stage for {job_id}: {e}")
         # Clean up new CSV if created before error
        if new_temp_csv_file_path and Path(new_temp_csv_file_path).exists():
            try: os.remove(new_temp_csv_file_path)
            except OSError: pass
        raise HTTPException(status_code=500, detail="Internal server error during job re-stage.")

--- END FILE: ./backend/app/routers/jobs.py ---

--- START FILE: ./backend/app/success_log_example (Size: 186265 bytes) ---
Apr-16 00:02:15.204 [main] DEBUG nextflow.cli.Launcher - $> nextflow run nf-core/sarek -r 3.5.1 -profile test,docker --outdir /home/admin01/work/mnt/nas/mikha_temp/results/
Apr-16 00:02:15.276 [main] DEBUG nextflow.cli.CmdRun - N E X T F L O W  ~  version 24.10.5
Apr-16 00:02:15.293 [main] DEBUG nextflow.plugin.PluginsFacade - Setting up plugin manager > mode=prod; embedded=false; plugins-dir=/root/.nextflow/plugins; core-plugins: nf-amazon@2.9.2,nf-azure@1.10.2,nf-cloudcache@0.4.2,nf-codecommit@0.2.2,nf-console@1.1.4,nf-google@1.15.4,nf-tower@1.9.3,nf-wave@1.7.4
Apr-16 00:02:15.310 [main] INFO  o.pf4j.DefaultPluginStatusProvider - Enabled plugins: []
Apr-16 00:02:15.311 [main] INFO  o.pf4j.DefaultPluginStatusProvider - Disabled plugins: []
Apr-16 00:02:15.314 [main] INFO  org.pf4j.DefaultPluginManager - PF4J version 3.12.0 in 'deployment' mode
Apr-16 00:02:15.324 [main] INFO  org.pf4j.AbstractPluginManager - No plugins
Apr-16 00:02:15.333 [main] DEBUG nextflow.scm.ProviderConfig - Using SCM config path: /root/.nextflow/scm
Apr-16 00:02:16.248 [main] DEBUG nextflow.scm.AssetManager - Git config: /root/.nextflow/assets/nf-core/sarek/.git/config; branch: null; remote: origin; url: https://github.com/nf-core/sarek.git
Apr-16 00:02:16.275 [main] DEBUG nextflow.scm.RepositoryFactory - Found Git repository result: [RepositoryFactory]
Apr-16 00:02:16.283 [main] DEBUG nextflow.scm.AssetManager - Git config: /root/.nextflow/assets/nf-core/sarek/.git/config; branch: null; remote: origin; url: https://github.com/nf-core/sarek.git
Apr-16 00:02:17.667 [main] DEBUG nextflow.config.ConfigBuilder - Found config base: /root/.nextflow/assets/nf-core/sarek/nextflow.config
Apr-16 00:02:17.672 [main] DEBUG nextflow.config.ConfigBuilder - Parsing config file: /root/.nextflow/assets/nf-core/sarek/nextflow.config
Apr-16 00:02:17.690 [main] DEBUG n.secret.LocalSecretsProvider - Secrets store: /root/.nextflow/secrets/store.json
Apr-16 00:02:17.697 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@2de6f1bc] - activable => nextflow.secret.LocalSecretsProvider@2de6f1bc
Apr-16 00:02:17.720 [main] DEBUG nextflow.config.ConfigBuilder - Applying config profile: `test,docker`
Apr-16 00:02:20.519 [main] DEBUG nextflow.config.ConfigBuilder - Available config profiles: [uzl_omics, denbi_qbic, mjolnir_globe, uppmax, incliva, uge, icr_alma, rosalind_uge, tools_germline_deepvariant, lugh, mccleary, unibe_ibu, czbiohub_aws, jax, roslin, tes, scw, tigem, tubingen_apg, google, vsc_calcua, pdc_kth, use_gatk_spark, humantechnopole, alignment_from_everything, stjude, daisybio, eddie, alignment_to_fastq, medair, biowulf, bi, cedars, spark, utd_ganymede, charliecloud, targeted, seattlechildrens, arm, rosalind, annotation, cfc, uzh, variantcalling_channels, ccga_dx, crick, recalibrate_bam, ku_sund_danhead, marvin, shifter, biohpc_gen, mana, wehi, test_aws, arcc, imperial, maestro, cannon, nci_gadi, skip_markduplicates, googlebatch, sahmri, kaust, alliance_canada, trimming, cambridge, tools_tumoronly, jex, cheaha, nyu_hpc, test, prepare_recalibration_bam, seg_globe, sanger, tools_somatic_ascat, pasteur, einstein, ethz_euler, tuos_stanage, azurebatch, crukmi, csiro_petrichor, wave, docker, test_azure, test_full_azure, umi, hypatia, psmn, markduplicates_bam, fgcz, conda, sentieon_dedup_bam, mpcdf_viper, pe2, uw_hyak_pedslabs, tools_somatic, genouest, markduplicates_cram, unsw_katana, recalibrate_cram, prepare_recalibration_cram, pair, gitpod, seawulf, uct_hpc, aws_tower, test_full_germline_azure, binac, bih, cfc_dev, ifb_core, embl_hd, alice, giga, ilifu, ki_luria, test_full_germline, vai, ccga_med, unc_longleaf, apollo, ipop_up, googlels, ceci_nic5, apptainer, bigpurple, adcra, tools, pawsey_setonix, vsc_kul_uhasselt, pawsey_nimbus, ucl_myriad, icr_davros, save_bam_mapped, ceres, munin, hasta, test_full_aws, split_fastq, shu_bmrc, ebi_codon_slurm, ebc, mamba, york_viking, unc_lccc, awsbatch, wustl_htcf, test_full_germline_ncbench_agilent, ceci_dragon2, skip_bqsr, software_license, genotoul, abims, test_full_germline_aws, janelia, nu_genomics, oist, mpcdf, leicester, vsc_ugent, create, sage, podman, ebi_codon, xanadu, marjorie, tools_germline, computerome, ucd_sonic, dkfz, bluebear, m3c, test_full, imb, ucl_cscluster, hki, seadragon, qmul_apocrita, engaging, gis, eva, unity, cropdiversityhpc, nygc, crg, singularity, self_hosted_runner, tufts, sentieon_dedup_cram, debug, cbe, no_intervals, phoenix, uod_hpc, fub_curta, fsu_draco]
Apr-16 00:02:20.563 [main] DEBUG nextflow.cli.CmdRun - Applied DSL=2 from script declaration
Apr-16 00:02:20.564 [main] DEBUG nextflow.cli.CmdRun - Launching `https://github.com/nf-core/sarek` [lethal_ekeblad] DSL2 - revision: 5fe5cdff17 [3.5.1]
Apr-16 00:02:20.565 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins declared=[nf-schema@2.2.1, nf-prov@1.2.2]
Apr-16 00:02:20.565 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]
Apr-16 00:02:20.566 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[nf-schema@2.2.1, nf-prov@1.2.2]
Apr-16 00:02:20.566 [main] DEBUG nextflow.plugin.PluginUpdater - Installing plugin nf-schema version: 2.2.1
Apr-16 00:02:20.570 [main] INFO  org.pf4j.AbstractPluginManager - Plugin 'nf-schema@2.2.1' resolved
Apr-16 00:02:20.571 [main] INFO  org.pf4j.AbstractPluginManager - Start plugin 'nf-schema@2.2.1'
Apr-16 00:02:20.574 [main] DEBUG nextflow.plugin.BasePlugin - Plugin started nf-schema@2.2.1
Apr-16 00:02:20.575 [main] DEBUG nextflow.plugin.PluginUpdater - Installing plugin nf-prov version: 1.2.2
Apr-16 00:02:20.575 [main] INFO  org.pf4j.AbstractPluginManager - Plugin 'nf-prov@1.2.2' resolved
Apr-16 00:02:20.576 [main] INFO  org.pf4j.AbstractPluginManager - Start plugin 'nf-prov@1.2.2'
Apr-16 00:02:20.577 [main] DEBUG nextflow.plugin.BasePlugin - Plugin started nf-prov@1.2.2
Apr-16 00:02:20.635 [main] DEBUG nextflow.Session - Session UUID: 3b55d465-1927-432e-a035-4ec4c66b04c3
Apr-16 00:02:20.635 [main] DEBUG nextflow.Session - Run name: lethal_ekeblad
Apr-16 00:02:20.636 [main] DEBUG nextflow.Session - Executor pool size: 48
Apr-16 00:02:20.641 [main] DEBUG nextflow.file.FilePorter - File porter settings maxRetries=3; maxTransfers=50; pollTimeout=null
Apr-16 00:02:20.645 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=144; workQueue=LinkedBlockingQueue[-1]; allowCoreThreadTimeout=false
Apr-16 00:02:20.657 [main] DEBUG nextflow.cli.CmdRun - 
  Version: 24.10.5 build 5935
  Created: 04-03-2025 17:55 UTC 
  System: Linux 5.15.0-105-generic
  Runtime: Groovy 4.0.23 on OpenJDK 64-Bit Server VM 17.0.14+7-Ubuntu-122.04.1
  Encoding: UTF-8 (UTF-8)
  Process: 258033@idsvrugm001 [127.0.1.1]
  CPUs: 48 - Mem: 125.3 GB (12.5 GB) - Swap: 64 GB (61.5 GB)
Apr-16 00:02:20.666 [main] DEBUG nextflow.Session - Work-dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work [ext2/ext3]
Apr-16 00:02:20.680 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]
Apr-16 00:02:20.688 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory
Apr-16 00:02:20.702 [main] DEBUG nextflow.Session - Observer factory: ProvObserverFactory
Apr-16 00:02:20.772 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory
Apr-16 00:02:20.780 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 49; maxThreads: 1000
Apr-16 00:02:20.877 [main] DEBUG nextflow.Session - Session start
Apr-16 00:02:20.879 [main] DEBUG nextflow.trace.TraceFileObserver - Workflow started -- trace file: /home/admin01/work/mnt/nas/mikha_temp/results/pipeline_info/execution_trace_2025-04-16_00-02-19.txt
Apr-16 00:02:21.306 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution
Apr-16 00:02:22.025 [main] DEBUG nextflow.script.IncludeDef - Loading included plugin extensions with names: [paramsSummaryMap:paramsSummaryMap]; plugin Id: nf-schema
Apr-16 00:02:22.738 [main] DEBUG nextflow.script.IncludeDef - Loading included plugin extensions with names: [paramsSummaryLog:paramsSummaryLog]; plugin Id: nf-schema
Apr-16 00:02:22.739 [main] DEBUG nextflow.script.IncludeDef - Loading included plugin extensions with names: [validateParameters:validateParameters]; plugin Id: nf-schema
Apr-16 00:02:22.743 [main] DEBUG nextflow.script.IncludeDef - Loading included plugin extensions with names: [paramsSummaryMap:paramsSummaryMap]; plugin Id: nf-schema
Apr-16 00:02:22.743 [main] DEBUG nextflow.script.IncludeDef - Loading included plugin extensions with names: [samplesheetToList:samplesheetToList]; plugin Id: nf-schema
Apr-16 00:02:33.137 [main] DEBUG nextflow.script.IncludeDef - Loading included plugin extensions with names: [paramsSummaryLog:paramsSummaryLog]; plugin Id: nf-schema
Apr-16 00:02:33.138 [main] DEBUG nextflow.script.IncludeDef - Loading included plugin extensions with names: [validateParameters:validateParameters]; plugin Id: nf-schema
Apr-16 00:02:33.140 [main] DEBUG nextflow.script.IncludeDef - Loading included plugin extensions with names: [paramsSummaryMap:paramsSummaryMap]; plugin Id: nf-schema
Apr-16 00:02:33.140 [main] DEBUG nextflow.script.IncludeDef - Loading included plugin extensions with names: [samplesheetToList:samplesheetToList]; plugin Id: nf-schema
Apr-16 00:02:33.918 [main] INFO  nextflow.Nextflow - 
-[2m----------------------------------------------------[0m-
                                        [0;32m,--.[0;30m/[0;32m,-.[0m
[0;34m        ___     __   __   __   ___     [0;32m/,-._.--~'[0m
[0;34m  |\ | |__  __ /  ` /  \ |__) |__         [0;33m}  {[0m
[0;34m  | \| |       \__, \__/ |  \ |___     [0;32m\`-._,-`-,[0m
                                        [0;32m`._,._,'[0m
[0;37m      ____[0m
[0;37m    . _  `.[0m
[0;37m   /  [0;32m|\[0m`-_ \[0m     [0;34m __        __   ___     [0m
[0;37m  |   [0;32m| \[0m  `-|[0m    [0;34m|__`  /\  |__) |__  |__/[0m
[0;37m   \ [0;32m|   \[0m  /[0m     [0;34m.__| /\ |  \ |___ |  \[0m
[0;37m    `[0;32m|[0m____[0;32m\[0m[0m

[0;35m  nf-core/sarek 3.5.1[0m
-[2m----------------------------------------------------[0m-
[1mInput/output options[0m
  [0;34minput                     : [0;32m/root/.nextflow/assets/nf-core/sarek/tests/csv/3.0/fastq_single.csv[0m
  [0;34moutdir                    : [0;32m/home/admin01/work/mnt/nas/mikha_temp/results/[0m

[1mMain options[0m
  [0;34msplit_fastq               : [0;32m0[0m
  [0;34mintervals                 : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/homo_sapiens/genome/genome.interval_list[0m
  [0;34mtools                     : [0;32mstrelka[0m

[1mAnnotation[0m
  [0;34mbcftools_annotations      : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/sarscov2/illumina/vcf/test2.vcf.gz[0m
  [0;34mbcftools_annotations_tbi  : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/sarscov2/illumina/vcf/test2.vcf.gz.tbi[0m
  [0;34mbcftools_header_lines     : [0;32m/root/.nextflow/assets/nf-core/sarek/tests/config/bcfann_test_header.txt[0m

[1mGeneral reference genome options[0m
  [0;34migenomes_base             : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data/[0m

[1mReference genome options[0m
  [0;34mgenome                    : [0;32mtestdata.nf-core.sarek[0m
  [0;34mdbsnp                     : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz[0m
  [0;34mdbsnp_tbi                 : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz.tbi[0m
  [0;34mdbsnp_vqsr                : [0;32m--resource:dbsnp,known=false,training=true,truth=false,prior=2.0 dbsnp_146.hg38.vcf.gz[0m
  [0;34mdict                      : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/homo_sapiens/genome/genome.dict[0m
  [0;34mfasta                     : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/homo_sapiens/genome/genome.fasta[0m
  [0;34mfasta_fai                 : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/homo_sapiens/genome/genome.fasta.fai[0m
  [0;34mgermline_resource         : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz[0m
  [0;34mgermline_resource_tbi     : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz.tbi[0m
  [0;34mknown_indels              : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz[0m
  [0;34mknown_indels_tbi          : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz.tbi[0m
  [0;34mknown_indels_vqsr         : [0;32m--resource:mills,known=false,training=true,truth=true,prior=10.0 mills_and_1000G.indels.vcf.gz[0m
  [0;34mngscheckmate_bed          : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data//genomics/homo_sapiens/genome/chr21/germlineresources/SNP_GRCh38_hg38_wChr.bed[0m
  [0;34msentieon_dnascope_model   : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Sentieon/SentieonDNAscopeModel1.1.model[0m
  [0;34msnpeff_cache              : [0;32mnull[0m
  [0;34msnpeff_db                 : [0;32mWBcel235.105[0m
  [0;34mvep_cache                 : [0;32mnull[0m
  [0;34mvep_cache_version         : [0;32m113[0m
  [0;34mvep_genome                : [0;32mWBcel235[0m
  [0;34mvep_species               : [0;32mcaenorhabditis_elegans[0m

[1mInstitutional config options[0m
  [0;34mconfig_profile_name       : [0;32mTest profile[0m
  [0;34mconfig_profile_description: [0;32mMinimal test dataset to check pipeline function[0m
  [0;34mmodules_testdata_base_path: [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/modules/data/[0m

[1mCore Nextflow options[0m
  [0;34mrevision                  : [0;32m3.5.1[0m
  [0;34mrunName                   : [0;32mlethal_ekeblad[0m
  [0;34mcontainerEngine           : [0;32mdocker[0m
  [0;34mlaunchDir                 : [0;32m/home/admin01/lab/bioinformatics-webapp/backend/app[0m
  [0;34mworkDir                   : [0;32m/home/admin01/lab/bioinformatics-webapp/backend/app/work[0m
  [0;34mprojectDir                : [0;32m/root/.nextflow/assets/nf-core/sarek[0m
  [0;34muserName                  : [0;32mroot[0m
  [0;34mprofile                   : [0;32mtest,docker[0m
  [0;34mconfigFiles               : [0;32m/root/.nextflow/assets/nf-core/sarek/nextflow.config[0m

!! Only displaying parameters that differ from the pipeline defaults !!
-[2m----------------------------------------------------[0m-
* The pipeline
    https://doi.org/10.12688/f1000research.16665.2
    https://doi.org/10.1093/nargab/lqae031
    https://doi.org/10.5281/zenodo.3476425

* The nf-core framework
    https://doi.org/10.1038/s41587-020-0439-x

* Software dependencies
    https://github.com/nf-core/sarek/blob/master/CITATIONS.md

Apr-16 00:02:33.921 [main] DEBUG nextflow.validation.SchemaValidator - Starting parameters validation
Apr-16 00:02:34.065 [main] DEBUG nextflow.validation.SchemaEvaluator - Started validating /root/.nextflow/assets/nf-core/sarek/tests/csv/3.0/fastq_single.csv
Apr-16 00:02:40.525 [main] DEBUG nextflow.validation.SchemaEvaluator - Validation of file '/root/.nextflow/assets/nf-core/sarek/tests/csv/3.0/fastq_single.csv' passed!
Apr-16 00:02:40.923 [main] DEBUG n.file.http.XFileSystemProvider - Got HTTP error=404 waiting for 250ms (attempt=1)
Apr-16 00:02:41.260 [main] DEBUG n.file.http.XFileSystemProvider - Got HTTP error=404 waiting for 750ms (attempt=2)
Apr-16 00:02:42.091 [main] DEBUG n.file.http.XFileSystemProvider - Got HTTP error=404 waiting for 2250ms (attempt=3)
Apr-16 00:02:46.472 [main] DEBUG n.validation.FormatFilePathEvaluator - S3 paths are not supported by 'FormatFilePathEvaluator': 's3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Sentieon/SentieonDNAscopeModel1.1.model'
Apr-16 00:02:46.472 [main] DEBUG nextflow.validation.ExistsEvaluator - S3 paths are not supported by 'ExistsEvaluator': 's3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Sentieon/SentieonDNAscopeModel1.1.model'
Apr-16 00:02:49.305 [main] DEBUG nextflow.validation.SchemaValidator - Finishing parameters validation
Apr-16 00:02:51.237 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX
Apr-16 00:02:51.247 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM1_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX
Apr-16 00:02:51.267 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.268 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.275 [main] DEBUG nextflow.executor.Executor - [warm up] executor > local
Apr-16 00:02:51.281 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=48; memory=125.3 GB; capacity=48; pollInterval=100ms; dumpInterval=5m
Apr-16 00:02:51.293 [main] DEBUG n.processor.TaskPollingMonitor - >>> barrier register (monitor: local)
Apr-16 00:02:51.340 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX
Apr-16 00:02:51.342 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM2_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX
Apr-16 00:02:51.350 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.350 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.364 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_high` matches labels `process_high` for process with name NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE
Apr-16 00:02:51.365 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:DRAGMAP_HASHTABLE` matches process NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE
Apr-16 00:02:51.372 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.372 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.379 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY
Apr-16 00:02:51.381 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_CREATESEQUENCEDICTIONARY` matches process NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY
Apr-16 00:02:51.388 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.388 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.395 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN
Apr-16 00:02:51.396 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MSISENSORPRO_SCAN` matches process NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN
Apr-16 00:02:51.403 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.403 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.411 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX
Apr-16 00:02:51.412 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:SAMTOOLS_FAIDX` matches process NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX
Apr-16 00:02:51.418 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.418 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.431 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS
Apr-16 00:02:51.432 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS
Apr-16 00:02:51.434 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS
Apr-16 00:02:51.435 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:TABIX_BCFTOOLS_ANNOTATIONS` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS
Apr-16 00:02:51.441 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.441 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.449 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP
Apr-16 00:02:51.450 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP
Apr-16 00:02:51.452 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP
Apr-16 00:02:51.452 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:TABIX_DBSNP` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP
Apr-16 00:02:51.458 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.458 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.465 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE
Apr-16 00:02:51.466 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE
Apr-16 00:02:51.468 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE
Apr-16 00:02:51.468 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:TABIX_GERMLINE_RESOURCE` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE
Apr-16 00:02:51.474 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.474 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.481 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS
Apr-16 00:02:51.482 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS
Apr-16 00:02:51.484 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS
Apr-16 00:02:51.485 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:TABIX_KNOWN_SNPS` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS
Apr-16 00:02:51.491 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.491 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.497 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS
Apr-16 00:02:51.498 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS
Apr-16 00:02:51.500 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS
Apr-16 00:02:51.501 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:TABIX_KNOWN_INDELS` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS
Apr-16 00:02:51.507 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.507 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.513 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_GENOME:TABIX_PON
Apr-16 00:02:51.514 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_PON
Apr-16 00:02:51.516 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_PON
Apr-16 00:02:51.517 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:TABIX_PON` matches process NFCORE_SAREK:PREPARE_GENOME:TABIX_PON
Apr-16 00:02:51.522 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.522 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.613 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED
Apr-16 00:02:51.614 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED
Apr-16 00:02:51.615 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:CREATE_INTERVALS_BED` matches process NFCORE_SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED
Apr-16 00:02:51.621 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.621 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.631 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED
Apr-16 00:02:51.632 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_INTERVALLISTTOBED` matches process NFCORE_SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED
Apr-16 00:02:51.639 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.639 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.673 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT
Apr-16 00:02:51.674 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT
Apr-16 00:02:51.675 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT
Apr-16 00:02:51.676 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:TABIX_BGZIPTABIX_INTERVAL_SPLIT|TABIX_BGZIPTABIX_INTERVAL_COMBINED` matches process NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT
Apr-16 00:02:51.682 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.682 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.694 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED
Apr-16 00:02:51.696 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED
Apr-16 00:02:51.699 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED
Apr-16 00:02:51.700 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:TABIX_BGZIPTABIX_INTERVAL_SPLIT|TABIX_BGZIPTABIX_INTERVAL_COMBINED` matches process NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED
Apr-16 00:02:51.707 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.707 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.757 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_FQ_PAIR
Apr-16 00:02:51.760 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_.*` matches process NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_FQ_PAIR
Apr-16 00:02:51.765 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.765 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.774 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R1_FQ
Apr-16 00:02:51.777 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_.*` matches process NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R1_FQ
Apr-16 00:02:51.781 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.781 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.789 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R2_FQ
Apr-16 00:02:51.792 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_.*` matches process NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R2_FQ
Apr-16 00:02:51.795 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.796 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.820 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP
Apr-16 00:02:51.823 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:SAMTOOLS_VIEW_MAP_MAP` matches process NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP
Apr-16 00:02:51.829 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.829 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.840 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP
Apr-16 00:02:51.842 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:SAMTOOLS_VIEW_UNMAP_UNMAP` matches process NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP
Apr-16 00:02:51.847 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.847 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.861 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_MAP
Apr-16 00:02:51.864 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:SAMTOOLS_VIEW_UNMAP_MAP` matches process NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_MAP
Apr-16 00:02:51.870 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.870 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.885 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_UNMAP
Apr-16 00:02:51.886 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:SAMTOOLS_VIEW_MAP_UNMAP` matches process NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_UNMAP
Apr-16 00:02:51.891 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.891 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.913 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_MERGE_UNMAP
Apr-16 00:02:51.915 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:SAMTOOLS_MERGE_UNMAP` matches process NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_MERGE_UNMAP
Apr-16 00:02:51.920 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.920 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.932 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_UNMAP
Apr-16 00:02:51.934 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:COLLATE_FASTQ_UNMAP` matches process NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_UNMAP
Apr-16 00:02:51.938 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.938 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.946 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_MAP
Apr-16 00:02:51.954 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:COLLATE_FASTQ_MAP` matches process NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_MAP
Apr-16 00:02:51.958 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.959 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:51.971 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:CAT_FASTQ
Apr-16 00:02:51.972 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:CAT_FASTQ` matches process NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:CAT_FASTQ
Apr-16 00:02:51.976 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:51.976 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.000 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:FASTQC
Apr-16 00:02:52.000 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:FASTQC` matches process NFCORE_SAREK:SAREK:FASTQC
Apr-16 00:02:52.004 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.004 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.030 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_high` matches labels `process_high` for process with name NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM
Apr-16 00:02:52.031 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM1_MEM|BWAMEM2_MEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM
Apr-16 00:02:52.032 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM1_MEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM
Apr-16 00:02:52.032 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM.*_MEM|DRAGMAP_ALIGN|SENTIEON_BWAMEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM
Apr-16 00:02:52.032 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM.*_MEM|DRAGMAP_ALIGN` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM
Apr-16 00:02:52.032 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM.*_MEM|SENTIEON_BWAMEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM
Apr-16 00:02:52.036 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.036 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.050 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_high` matches labels `process_high` for process with name NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM
Apr-16 00:02:52.050 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM1_MEM|BWAMEM2_MEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM
Apr-16 00:02:52.051 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM2_MEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM
Apr-16 00:02:52.051 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM.*_MEM|DRAGMAP_ALIGN|SENTIEON_BWAMEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM
Apr-16 00:02:52.051 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM.*_MEM|DRAGMAP_ALIGN` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM
Apr-16 00:02:52.051 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM.*_MEM|SENTIEON_BWAMEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM
Apr-16 00:02:52.056 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.056 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.070 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_high` matches labels `process_high` for process with name NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN
Apr-16 00:02:52.071 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:DRAGMAP_ALIGN` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN
Apr-16 00:02:52.071 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM.*_MEM|DRAGMAP_ALIGN|SENTIEON_BWAMEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN
Apr-16 00:02:52.071 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM.*_MEM|DRAGMAP_ALIGN` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN
Apr-16 00:02:52.075 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.075 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.088 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_high` matches labels `process_high,sentieon` for process with name NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:SENTIEON_BWAMEM
Apr-16 00:02:52.089 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:SENTIEON_BWAMEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:SENTIEON_BWAMEM
Apr-16 00:02:52.089 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM.*_MEM|DRAGMAP_ALIGN|SENTIEON_BWAMEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:SENTIEON_BWAMEM
Apr-16 00:02:52.089 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BWAMEM.*_MEM|SENTIEON_BWAMEM` matches process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:SENTIEON_BWAMEM
Apr-16 00:02:52.093 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.093 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.152 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES
Apr-16 00:02:52.152 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_MARKDUPLICATES|GATK4SPARK_MARKDUPLICATES` matches process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES
Apr-16 00:02:52.153 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_MARKDUPLICATES` matches process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES
Apr-16 00:02:52.157 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.157 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.172 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOOLS_STATS
Apr-16 00:02:52.173 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:NFCORE_SAREK:SAREK:(BAM_MARKDUPLICATES|BAM_MARKDUPLICATES_SPARK):CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOOLS_STATS` matches process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOOLS_STATS
Apr-16 00:02:52.176 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.177 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.194 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH
Apr-16 00:02:52.195 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MOSDEPTH` matches process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH
Apr-16 00:02:52.199 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.199 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.248 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:CRAM_TO_BAM
Apr-16 00:02:52.249 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:SAMTOOLS_CONVERT` matches process NFCORE_SAREK:SAREK:CRAM_TO_BAM
Apr-16 00:02:52.249 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:CRAM_TO_BAM` matches process NFCORE_SAREK:SAREK:CRAM_TO_BAM
Apr-16 00:02:52.250 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:NFCORE_SAREK:SAREK:CRAM_TO_BAM` matches process NFCORE_SAREK:SAREK:CRAM_TO_BAM
Apr-16 00:02:52.254 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.254 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.331 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR
Apr-16 00:02:52.332 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_APPLYBQSR|GATK4SPARK_APPLYBQSR|GATK4_BASERECALIBRATOR|GATK4SPARK_BASERECALIBRATOR|GATK4_GATHERBQSRREPORTS` matches process NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR
Apr-16 00:02:52.332 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_BASERECALIBRATOR|GATK4SPARK_BASERECALIBRATOR` matches process NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR
Apr-16 00:02:52.335 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.335 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.350 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_GATHERBQSRREPORTS
Apr-16 00:02:52.350 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_APPLYBQSR|GATK4SPARK_APPLYBQSR|GATK4_BASERECALIBRATOR|GATK4SPARK_BASERECALIBRATOR|GATK4_GATHERBQSRREPORTS` matches process NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_GATHERBQSRREPORTS
Apr-16 00:02:52.351 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_GATHERBQSRREPORTS` matches process NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_GATHERBQSRREPORTS
Apr-16 00:02:52.353 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.353 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.395 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR
Apr-16 00:02:52.396 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_APPLYBQSR|GATK4SPARK_APPLYBQSR|GATK4_BASERECALIBRATOR|GATK4SPARK_BASERECALIBRATOR|GATK4_GATHERBQSRREPORTS` matches process NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR
Apr-16 00:02:52.396 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_APPLYBQSR|GATK4SPARK_APPLYBQSR` matches process NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR
Apr-16 00:02:52.399 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.399 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.425 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:MERGE_CRAM
Apr-16 00:02:52.427 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:NFCORE_SAREK:SAREK:(BAM_APPLYBQSR|BAM_APPLYBQSR_SPARK):CRAM_MERGE_INDEX_SAMTOOLS:MERGE_CRAM` matches process NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:MERGE_CRAM
Apr-16 00:02:52.429 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.429 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.444 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM
Apr-16 00:02:52.445 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM
Apr-16 00:02:52.447 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:NFCORE_SAREK:SAREK:(BAM_APPLYBQSR|BAM_APPLYBQSR_SPARK):CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM` matches process NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM
Apr-16 00:02:52.449 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.449 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.481 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:CRAM_TO_BAM_RECAL
Apr-16 00:02:52.481 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:SAMTOOLS_CONVERT` matches process NFCORE_SAREK:SAREK:CRAM_TO_BAM_RECAL
Apr-16 00:02:52.482 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:CRAM_TO_BAM_RECAL` matches process NFCORE_SAREK:SAREK:CRAM_TO_BAM_RECAL
Apr-16 00:02:52.488 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.488 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.525 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:SAMTOOLS_STATS
Apr-16 00:02:52.527 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:SAMTOOLS_STATS` matches process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:SAMTOOLS_STATS
Apr-16 00:02:52.530 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.530 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.548 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH
Apr-16 00:02:52.549 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MOSDEPTH` matches process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH
Apr-16 00:02:52.550 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH` matches process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH
Apr-16 00:02:52.553 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.553 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.606 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP
Apr-16 00:02:52.606 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP
Apr-16 00:02:52.607 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BCFTOOLS_MPILEUP` matches process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP
Apr-16 00:02:52.608 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:.*BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP` matches process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP
Apr-16 00:02:52.610 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.611 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.639 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_low` matches labels `process_low` for process with name NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:NGSCHECKMATE_NCM
Apr-16 00:02:52.641 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:.*BAM_NGSCHECKMATE:NGSCHECKMATE_NCM` matches process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:NGSCHECKMATE_NCM
Apr-16 00:02:52.643 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.643 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.754 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:error_retry` matches labels `process_medium,error_retry` for process with name NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE
Apr-16 00:02:52.754 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium,error_retry` for process with name NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE
Apr-16 00:02:52.754 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:STRELKA.*|MANTA.*` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE
Apr-16 00:02:52.755 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:STRELKA_.*` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE
Apr-16 00:02:52.755 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:STRELKA.*|MANTA.*` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE
Apr-16 00:02:52.756 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:STRELKA_.*` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE
Apr-16 00:02:52.780 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.780 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.801 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA
Apr-16 00:02:52.801 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_MERGEVCFS` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA
Apr-16 00:02:52.802 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MERGE_STRELKA.*` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA
Apr-16 00:02:52.802 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MERGE_STRELKA` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA
Apr-16 00:02:52.804 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.804 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.810 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA_GENOME
Apr-16 00:02:52.811 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_MERGEVCFS` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA_GENOME
Apr-16 00:02:52.812 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MERGE_STRELKA.*` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA_GENOME
Apr-16 00:02:52.812 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MERGE_STRELKA_GENOME` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA_GENOME
Apr-16 00:02:52.814 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.814 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.880 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:error_retry` matches labels `process_medium,error_retry` for process with name NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:STRELKA_SOMATIC
Apr-16 00:02:52.880 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium,error_retry` for process with name NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:STRELKA_SOMATIC
Apr-16 00:02:52.880 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:STRELKA.*|MANTA.*` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:STRELKA_SOMATIC
Apr-16 00:02:52.881 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:STRELKA_.*` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:STRELKA_SOMATIC
Apr-16 00:02:52.883 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.883 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.904 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_INDELS
Apr-16 00:02:52.905 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_MERGEVCFS` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_INDELS
Apr-16 00:02:52.906 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MERGE_STRELKA.*` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_INDELS
Apr-16 00:02:52.906 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MERGE_STRELKA_INDELS` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_INDELS
Apr-16 00:02:52.908 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.908 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.913 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_medium` matches labels `process_medium` for process with name NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_SNVS
Apr-16 00:02:52.914 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:GATK4_MERGEVCFS` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_SNVS
Apr-16 00:02:52.915 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MERGE_STRELKA.*` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_SNVS
Apr-16 00:02:52.915 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MERGE_STRELKA_SNVS` matches process NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_SNVS
Apr-16 00:02:52.917 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.917 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:52.989 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS
Apr-16 00:02:52.989 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS
Apr-16 00:02:52.989 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:BCFTOOLS_STATS` matches process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS
Apr-16 00:02:52.992 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:52.992 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:53.024 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT
Apr-16 00:02:53.025 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT
Apr-16 00:02:53.026 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:VCFTOOLS_.*` matches process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT
Apr-16 00:02:53.026 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:VCFTOOLS_TSTV_COUNT` matches process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT
Apr-16 00:02:53.033 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:53.033 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:53.082 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL
Apr-16 00:02:53.083 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL
Apr-16 00:02:53.084 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:VCFTOOLS_.*` matches process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL
Apr-16 00:02:53.084 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:VCFTOOLS_TSTV_QUAL` matches process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL
Apr-16 00:02:53.091 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:53.091 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:53.145 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY
Apr-16 00:02:53.145 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:UNZIP.*|UNTAR.*|TABIX.*|BUILD_INTERVALS|CREATE_INTERVALS_BED|VCFTOOLS|BCFTOOLS.*|SAMTOOLS_INDEX` matches process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY
Apr-16 00:02:53.146 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:VCFTOOLS_.*` matches process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY
Apr-16 00:02:53.146 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:VCFTOOLS_SUMMARY` matches process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY
Apr-16 00:02:53.154 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:53.154 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:53.352 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:process_single` matches labels `process_single` for process with name NFCORE_SAREK:SAREK:MULTIQC
Apr-16 00:02:53.353 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MULTIQC` matches process NFCORE_SAREK:SAREK:MULTIQC
Apr-16 00:02:53.356 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Apr-16 00:02:53.356 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Apr-16 00:02:53.375 [main] DEBUG nextflow.Session - Config process names validation disabled as requested
Apr-16 00:02:53.376 [main] DEBUG nextflow.Session - Igniting dataflow network (176)
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS
Apr-16 00:02:53.386 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_GENOME:TABIX_PON
Apr-16 00:02:53.387 [PathVisitor-3] DEBUG nextflow.plugin.PluginUpdater - Installing plugin nf-amazon version: 2.9.2
Apr-16 00:02:53.387 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED
Apr-16 00:02:53.387 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED
Apr-16 00:02:53.387 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_FQ_PAIR
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R1_FQ
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R2_FQ
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_MAP
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_UNMAP
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_MERGE_UNMAP
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_UNMAP
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_MAP
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:CAT_FASTQ
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:FASTQC
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN
Apr-16 00:02:53.388 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:SENTIEON_BWAMEM
Apr-16 00:02:53.389 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES
Apr-16 00:02:53.389 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOOLS_STATS
Apr-16 00:02:53.389 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH
Apr-16 00:02:53.389 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CRAM_TO_BAM
Apr-16 00:02:53.389 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR
Apr-16 00:02:53.389 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_GATHERBQSRREPORTS
Apr-16 00:02:53.389 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR
Apr-16 00:02:53.389 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:MERGE_CRAM
Apr-16 00:02:53.389 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM
Apr-16 00:02:53.389 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CRAM_TO_BAM_RECAL
Apr-16 00:02:53.390 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:SAMTOOLS_STATS
Apr-16 00:02:53.390 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH
Apr-16 00:02:53.390 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP
Apr-16 00:02:53.390 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:NGSCHECKMATE_NCM
Apr-16 00:02:53.390 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE
Apr-16 00:02:53.390 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA
Apr-16 00:02:53.390 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA_GENOME
Apr-16 00:02:53.391 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:STRELKA_SOMATIC
Apr-16 00:02:53.391 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_INDELS
Apr-16 00:02:53.391 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_SNVS
Apr-16 00:02:53.391 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS
Apr-16 00:02:53.391 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT
Apr-16 00:02:53.391 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL
Apr-16 00:02:53.391 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY
Apr-16 00:02:53.391 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > NFCORE_SAREK:SAREK:MULTIQC
Apr-16 00:02:53.393 [PathVisitor-3] INFO  org.pf4j.AbstractPluginManager - Plugin 'nf-amazon@2.9.2' resolved
Apr-16 00:02:53.393 [PathVisitor-3] INFO  org.pf4j.AbstractPluginManager - Start plugin 'nf-amazon@2.9.2'
Apr-16 00:02:53.398 [main] DEBUG nextflow.script.ScriptRunner - Parsed script files:
  Script_f289bdfe26d7f1cf: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../modules/nf-core/fastqc/main.nf
  Script_2b401a04ae31574b: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/cram_sampleqc/main.nf
  Script_7abe1c051b661ed0: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/fastq_create_umi_consensus_fgbio/../../../modules/nf-core/fgbio/fastqtobam/main.nf
  Script_9a1f744b4af07447: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/annotation_cache_initialisation/main.nf
  Script_e6f540ffb2c944ec: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_mutect2/../../../modules/nf-core/gatk4/calculatecontamination/main.nf
  Script_028a088a254cd6dd: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_freebayes/main.nf
  Script_2719fbd471342244: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/cram_sampleqc/../../../subworkflows/nf-core/bam_ngscheckmate/main.nf
  Script_2b733a645bfb6d5a: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/main.nf
  Script_76c3210580c9fe21: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/vcf_annotate_all/main.nf
  Script_8ff83fc63a22c8fb: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_indexcov/../../../modules/local/samtools/reindex_bam/main.nf
  Script_223ca400878df5b0: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_genome/../../../modules/nf-core/untar/main.nf
  Script_c80bb66b9dac5c97: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/vcf_annotate_all/../../nf-core/vcf_annotate_snpeff/../../../modules/nf-core/snpeff/snpeff/main.nf
  Script_a2aa0d1ede76c3e4: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_mpileup/../../../modules/nf-core/cat/cat/main.nf
  Script_0470df1d00c50fae: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_joint_calling_germline_gatk/../../../modules/nf-core/gatk4/applyvqsr/main.nf
  Script_b8b71093ffc116f1: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/post_variantcalling/../vcf_concatenate_germline/main.nf
  Script_1a83631ea747ef88: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/main.nf
  Script_f34da7b33c26ef52: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_cnvkit/../../../modules/nf-core/cnvkit/export/main.nf
  Script_9f122e32eac0f4c1: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_freebayes/../../../modules/nf-core/freebayes/main.nf
  Script_824368743d45b72e: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_genome/main.nf
  Script_b3af914e27a1ce02: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_sentieon_dedup/../../../modules/nf-core/sentieon/dedup/main.nf
  Script_668d1f0d214380c6: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_genome/../../../modules/nf-core/samtools/faidx/main.nf
  Script_02b13fce0ed21a2e: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_manta/../../../modules/nf-core/manta/somatic/main.nf
  Script_a40ac2ab631fac5b: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_markduplicates_spark/../../../modules/nf-core/gatk4/estimatelibrarycomplexity/main.nf
  Script_129045cbcec43290: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_indexcov/../../../modules/nf-core/goleft/indexcov/main.nf
  Script_1f0a1eb4bf79a7fd: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_sentieon_dedup/main.nf
  Script_b8abeeceaabc9922: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_deepvariant/main.nf
  Script_0ce72e734ad77725: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/utils_nfcore_sarek_pipeline/../../nf-core/utils_nextflow_pipeline/main.nf
  Script_64e8d3d443f4d86f: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_joint_calling_germline_sentieon/main.nf
  Script_bf84ae935173146f: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/cram_sampleqc/../../../subworkflows/local/cram_qc_mosdepth_samtools/main.nf
  Script_ee34bf515398c533: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/utils_nfcore_sarek_pipeline/../samplesheet_to_channel/main.nf
  Script_5e3dda2ff6021e1a: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_mutect2/../../../modules/nf-core/gatk4/filtermutectcalls/main.nf
  Script_819aadb1392fd995: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/vcf_annotate_all/../../nf-core/vcf_annotate_ensemblvep/../../../modules/nf-core/ensemblvep/vep/main.nf
  Script_7a66424f9cfe9c28: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_convert_samtools/../../../modules/nf-core/cat/fastq/main.nf
  Script_2b1c6a73b143b9f1: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_strelka/main.nf
  Script_b6f6b85e4ee95525: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_baserecalibrator/../../../modules/nf-core/gatk4/gatherbqsrreports/main.nf
  Script_057d3de35ec771e9: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_markduplicates_spark/main.nf
  Script_d6715a92b793b124: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_genome/../../../modules/nf-core/bwamem2/index/main.nf
  Script_e5fccee2ba9a7b56: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_applybqsr_spark/../cram_merge_index_samtools/../../../modules/nf-core/samtools/merge/main.nf
  Script_ca1602e337ab4f2a: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_cnvkit/../../../modules/nf-core/cnvkit/genemetrics/main.nf
  Script_b51e866816de7307: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/fastq_create_umi_consensus_fgbio/../../../modules/nf-core/samtools/bam2fq/main.nf
  Script_e6188ba138d07431: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_baserecalibrator/main.nf
  Script_5a1ee4df50ceb976: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/channel_markduplicates_create_csv/main.nf
  Script_af1e34c37a4660bf: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/utils_nfcore_sarek_pipeline/main.nf
  Script_d11b5c79ea543e09: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_controlfreec/main.nf
  Script_12f1b0cba653bc40: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_ascat/main.nf
  Script_319b72dc6ac6baa4: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_genome/../../../modules/nf-core/msisensorpro/scan/main.nf
  Script_eda4c1615e5caaae: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_applybqsr/../cram_merge_index_samtools/main.nf
  Script_1690db9662109ef7: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_sentieon_haplotyper/main.nf
  Script_57ee94b0d1ea8856: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/channel_variant_calling_create_csv/main.nf
  Script_3ff1656b1d66c1f2: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_applybqsr_spark/../cram_merge_index_samtools/../../../modules/nf-core/samtools/index/main.nf
  Script_1dca684e20afa7cb: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_applybqsr/main.nf
  Script_1d72d5d488f8c5ea: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/fastq_create_umi_consensus_fgbio/../../../modules/nf-core/fgbio/callmolecularconsensusreads/main.nf
  Script_4cb94d2c5ce6310a: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_joint_calling_germline_sentieon/../../../modules/nf-core/sentieon/gvcftyper/main.nf
  Script_b586a2201c19b6cb: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/fastq_create_umi_consensus_fgbio/../../../modules/nf-core/samblaster/main.nf
  Script_a9e4dae695eeedc0: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_applybqsr/../../../modules/nf-core/gatk4/applybqsr/main.nf
  Script_ee2021533d309cc4: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_baserecalibrator_spark/main.nf
  Script_dcc0103cf6df5867: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/fastq_align_bwamem_mem2_dragmap_sentieon/../../../modules/nf-core/sentieon/bwamem/main.nf
  Script_bc049ecb86093316: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_sentieon_dnascope/main.nf
  Script_1a1ad9a55706351e: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_genome/../../../modules/nf-core/gatk4/createsequencedictionary/main.nf
  Script_282984c0da057d55: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_sentieon_dnascope/../../../modules/nf-core/sentieon/dnascope/main.nf
  Script_755c4bca437f6e1a: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_convert_samtools/main.nf
  Script_7e1dbf17e4d60bc6: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_sentieon_dedup/../cram_qc_mosdepth_samtools/../../../modules/nf-core/mosdepth/main.nf
  Script_525415b7be4c75ab: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_mutect2/../../../modules/nf-core/gatk4/getpileupsummaries/main.nf
  Script_f956f6322a74991c: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_joint_calling_germline_gatk/../../../modules/nf-core/gatk4/genomicsdbimport/main.nf
  Script_d0b56273bec7c219: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_single_strelka/main.nf
  Script_5063435f9f6c42c1: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_controlfreec/main.nf
  Script_5fd54cc278f5533b: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_mpileup/main.nf
  Script_795a2a24ac2ccf20: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_reference_cnvkit/../../../modules/nf-core/cnvkit/reference/main.nf
  Script_215a636da7ab24a2: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/utils_nfcore_sarek_pipeline/../../nf-core/utils_nfschema_plugin/main.nf
  Script_0d2b067c028ec877: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_indexcov/main.nf
  Script_22493c8980382660: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/post_variantcalling/../vcf_concatenate_germline/../../../modules/nf-core/bcftools/concat/main.nf
  Script_ccf2b8f40d5f2d96: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_baserecalibrator_spark/../../../modules/nf-core/gatk4spark/baserecalibrator/main.nf
  Script_9f6ce129b29ed4a5: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../modules/nf-core/fastp/main.nf
  Script_1513213805431d5d: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_tiddit/../bam_variant_calling_single_tiddit/main.nf
  Script_50a37d59281f26e1: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_markduplicates_spark/../../../modules/nf-core/gatk4spark/markduplicates/main.nf
  Script_b020395cbce5d093: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/vcf_annotate_all/../../nf-core/vcf_annotate_snpeff/main.nf
  Script_5ebb2c36f6b7f624: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_manta/main.nf
  Script_19b54da666a96d27: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/fastq_create_umi_consensus_fgbio/../fastq_align_bwamem_mem2_dragmap_sentieon/main.nf
  Script_c7ac4d9476aa2ba2: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_baserecalibrator/../../../modules/nf-core/gatk4/baserecalibrator/main.nf
  Script_f546fa1a30569187: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_controlfreec/../../../modules/nf-core/controlfreec/assesssignificance/main.nf
  Script_28f941a95f1d6b78: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_genome/../../../modules/nf-core/tabix/tabix/main.nf
  Script_22665c4a579d54f9: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_cnvkit/../../../modules/nf-core/cnvkit/batch/main.nf
  Script_5887836764e3f95a: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/vcf_qc_bcftools_vcftools/main.nf
  Script_40c8af5fd1d6a65d: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_strelka/../../../modules/nf-core/strelka/somatic/main.nf
  Script_2188acb123c22beb: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../vcf_variant_filtering_gatk/../../../modules/nf-core/gatk4/filtervarianttranches/main.nf
  Script_4f4c1dcf01d7366a: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_joint_calling_germline_gatk/main.nf
  Script_ce8725cd2a4af861: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_intervals/../../../modules/local/create_intervals_bed/main.nf
  Script_3a490b10a1aee53b: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_ascat/../../../modules/nf-core/ascat/main.nf
  Script_559f19a480fa4b06: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_mpileup/../../../modules/nf-core/samtools/mpileup/main.nf
  Script_16efaba349dfbe4c: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_mpileup/../../../modules/nf-core/bcftools/mpileup/main.nf
  Script_5db680c933e2ff12: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_mutect2/../../../modules/nf-core/gatk4/gatherpileupsummaries/main.nf
  Script_b428227b4d50d6b9: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/post_variantcalling/../vcf_concatenate_germline/../../../modules/nf-core/tabix/bgziptabix/main.nf
  Script_ea5171a3862b3145: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_intervals/../../../modules/nf-core/gatk4/intervallisttobed/main.nf
  Script_65f7a9bfe841c9f1: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_haplotypecaller/main.nf
  Script_4955288afd8ca61e: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../modules/nf-core/multiqc/main.nf
  Script_35173eb261765cf2: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/download_cache_snpeff_vep/../../../modules/nf-core/snpeff/download/main.nf
  Script_def39566b16f9c71: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_joint_calling_germline_gatk/../../../modules/nf-core/gatk4/genotypegvcfs/main.nf
  Script_6fbcf8477f9d7d4c: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/utils_nfcore_sarek_pipeline/../../nf-core/utils_nfcore_pipeline/main.nf
  Script_22afa208f7c54a51: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_joint_calling_germline_sentieon/../../../modules/nf-core/sentieon/varcal/main.nf
  Script_a596d37d30e0bd03: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_mutect2/main.nf
  Script_a1bcdb7a1cee0441: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_mutect2/../../../modules/nf-core/gatk4/mutect2/main.nf
  Script_986822999eec5083: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_mutect2/../../../modules/nf-core/gatk4/mergemutectstats/main.nf
  Script_07e778c9418e34ce: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/post_variantcalling/../vcf_concatenate_germline/../../../modules/local/add_info_to_vcf/main.nf
  Script_76be394b89bf3625: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/vcf_qc_bcftools_vcftools/../../../modules/nf-core/vcftools/main.nf
  Script_95fc895ef88c0a5e: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_joint_calling_germline_gatk/../../../modules/nf-core/gatk4/variantrecalibrator/main.nf
  Script_9a0341bb8a253b1b: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_applybqsr_spark/main.nf
  Script_398efd73d6615984: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/download_cache_snpeff_vep/main.nf
  Script_dc37288a8ff6ff5e: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../vcf_variant_filtering_gatk/main.nf
  Script_71644ff913245f63: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/vcf_annotate_all/../../nf-core/vcf_annotate_ensemblvep/main.nf
  Script_cc95cdf301222818: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_controlfreec/../../../modules/nf-core/controlfreec/freec2bed/main.nf
  Script_ec7f32831b7e4583: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_mutect2/../../../modules/nf-core/gatk4/learnreadorientationmodel/main.nf
  Script_87d7e2a2e7393ec1: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_tiddit/../../../modules/nf-core/svdb/merge/main.nf
  Script_27bfd85c6357625a: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_merge_index_samtools/main.nf
  Script_106c897be00672c0: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/vcf_annotate_all/../vcf_annotate_bcftools/../../../modules/nf-core/bcftools/annotate/main.nf
  Script_f57dc866f0b59998: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/main.nf
  Script_bffad0bae920118f: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_joint_calling_germline_sentieon/../../../modules/nf-core/sentieon/applyvarcal/main.nf
  Script_cf2ce48a700fff15: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_cnvkit/../../../modules/nf-core/cnvkit/call/main.nf
  Script_777aa12702bc2e5e: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_sentieon_haplotyper/../../../modules/nf-core/sentieon/haplotyper/main.nf
  Script_3db632c9775b8ab1: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/fastq_create_umi_consensus_fgbio/../../../modules/nf-core/fgbio/groupreadsbyumi/main.nf
  Script_0efe1bd11f3c1ecf: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/fastq_create_umi_consensus_fgbio/../fastq_align_bwamem_mem2_dragmap_sentieon/../../../modules/nf-core/bwa/mem/main.nf
  Script_9a898fc2ef2d97f0: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../../../modules/nf-core/msisensorpro/msisomatic/main.nf
  Script_cd6b697b5cdaf206: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_convert_samtools/../../../modules/nf-core/samtools/view/main.nf
  Script_f0dba0e4af952478: /root/.nextflow/assets/nf-core/sarek/main.nf
  Script_a4bb05978cccb4df: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_cnvkit/main.nf
  Script_3c537a6d105b66a5: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/post_variantcalling/main.nf
  Script_069ae240da88d1f8: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../../../modules/nf-core/sentieon/dnamodelapply/main.nf
  Script_47023b830d2a0f0f: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_single_tiddit/../../../modules/nf-core/tiddit/sv/main.nf
  Script_ea1853a6d4a4e95a: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_lofreq/../../../modules/nf-core/lofreq/callparallel/main.nf
  Script_d86efa1a3c45f294: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_markduplicates/../../../modules/nf-core/gatk4/markduplicates/main.nf
  Script_b530bf25fb72e640: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_intervals/main.nf
  Script_1613b97f11d35ecd: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_reference_cnvkit/main.nf
  Script_49a410273a0d4342: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/fastq_align_bwamem_mem2_dragmap_sentieon/../../../modules/nf-core/bwamem2/mem/main.nf
  Script_5fe56935cbbdbe39: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_mutect2/main.nf
  Script_afe39ca239cc60ee: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_intervals/../../../modules/nf-core/gawk/main.nf
  Script_896566c5c28b0384: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/fastq_create_umi_consensus_fgbio/main.nf
  Script_7eed17253b08a17f: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_lofreq/main.nf
  Script_8d1bad9e1fc84660: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_manta/main.nf
  Script_d8e65b113c3b6186: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_genome/../../../modules/nf-core/bwa/index/main.nf
  Script_66516be8545f71f2: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/channel_align_create_csv/main.nf
  Script_981d89be108d933e: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_manta/../../../modules/nf-core/manta/tumoronly/main.nf
  Script_83136d252c233538: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../modules/nf-core/spring/decompress/main.nf
  Script_a19f9ce9b09443d5: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_controlfreec/../../../modules/nf-core/controlfreec/freec/main.nf
  Script_2dd62d07716b998d: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../modules/nf-core/samtools/convert/main.nf
  Script_889f56729c972eac: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_freebayes/../../../modules/nf-core/bcftools/sort/main.nf
  Script_bc1034bcad96577b: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/channel_applybqsr_create_csv/main.nf
  Script_a57b73412a5be98e: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../vcf_variant_filtering_gatk/../../../modules/nf-core/gatk4/cnnscorevariants/main.nf
  Script_9c1a81a9ee2514ba: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_germline_manta/main.nf
  Script_7a96355ab7d6f1a3: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_markduplicates/main.nf
  Script_c91aa9a4bd1ae3f4: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/main.nf
  Script_602945516a2dd366: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/fastq_align_bwamem_mem2_dragmap_sentieon/../../../modules/nf-core/dragmap/align/main.nf
  Script_a22ce229b6cc00df: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_deepvariant/../../../modules/nf-core/deepvariant/rundeepvariant/main.nf
  Script_0a849060da7056aa: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_controlfreec/../../../modules/nf-core/controlfreec/freec2circos/main.nf
  Script_6df0dcc77a212b33: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_tumor_only_all/../bam_variant_calling_tumor_only_controlfreec/../../../modules/nf-core/controlfreec/makegraph2/main.nf
  Script_81ec1f07ab213590: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_convert_samtools/../../../modules/nf-core/samtools/collatefastq/main.nf
  Script_1bd098db325044b1: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/channel_baserecalibrator_create_csv/main.nf
  Script_e2fb30b813036521: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_germline_manta/../../../modules/nf-core/manta/germline/main.nf
  Script_d5cab7da0897d72e: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_somatic_all/../bam_variant_calling_somatic_tiddit/main.nf
  Script_1c7d7a4569b86317: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_sentieon_dedup/../cram_qc_mosdepth_samtools/../../../modules/nf-core/samtools/stats/main.nf
  Script_ce094188ad6cffa2: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_haplotypecaller/../../../modules/nf-core/gatk4/haplotypecaller/main.nf
  Script_e7e4058441ead3e3: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_single_strelka/../../../modules/nf-core/strelka/germline/main.nf
  Script_bf5fea05e615580c: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_genome/../../../modules/nf-core/dragmap/hashtable/main.nf
  Script_6630dd73aedd8b51: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/download_cache_snpeff_vep/../../../modules/nf-core/ensemblvep/download/main.nf
  Script_187f79be3eba8ea9: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/vcf_annotate_all/../vcf_annotate_bcftools/main.nf
  Script_477105648dabdd41: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/vcf_qc_bcftools_vcftools/../../../modules/nf-core/bcftools/stats/main.nf
  Script_185f79e4a174cc18: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/cram_sampleqc/../../../subworkflows/nf-core/bam_ngscheckmate/../../../modules/nf-core/ngscheckmate/ncm/main.nf
  Script_ba3a5724e6a02fb3: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_reference_cnvkit/../../../modules/nf-core/cnvkit/antitarget/main.nf
  Script_968436927447c3df: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_haplotypecaller/../../../modules/nf-core/gatk4/mergevcfs/main.nf
  Script_6e58dab2f6472f41: /root/.nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_applybqsr_spark/../../../modules/nf-core/gatk4spark/applybqsr/main.nf
  Script_8b560cf26a0f6f39: /root/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_genome/../../../modules/nf-core/unzip/main.nf
Apr-16 00:02:53.398 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination 
Apr-16 00:02:53.398 [main] DEBUG nextflow.Session - Session await
Apr-16 00:02:53.473 [PathVisitor-3] DEBUG nextflow.plugin.BasePlugin - Plugin started nf-amazon@2.9.2
Apr-16 00:02:53.543 [Actor Thread 25] WARN  nextflow.Nextflow - FASTQ file(/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_1.fastq.gz): Cannot extract flowcell ID from @normal#21#998579#1/1
Apr-16 00:02:53.553 [PathVisitor-3] DEBUG nextflow.file.FileHelper - > Added 'S3FileSystemProvider' to list of installed providers [s3]
Apr-16 00:02:53.554 [PathVisitor-3] DEBUG nextflow.file.FileHelper - Started plugin 'nf-amazon' required to handle file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Sentieon/SentieonDNAscopeModel1.1.model
Apr-16 00:02:53.573 [PathVisitor-3] DEBUG nextflow.file.FileHelper - Creating a file system instance for provider: S3FileSystemProvider
Apr-16 00:02:53.619 [PathVisitor-3] DEBUG nextflow.cloud.aws.config.AwsConfig - AWS S3 config properties: {max_error_retry=5}
Apr-16 00:02:53.619 [Actor Thread 50] DEBUG nextflow.sort.BigSort - Sort completed -- entries: 1; slices: 1; internal sort time: 0.002 s; external sort time: 0.025 s; total time: 0.027 s
Apr-16 00:02:53.619 [Actor Thread 4] DEBUG nextflow.sort.BigSort - Sort completed -- entries: 1; slices: 1; internal sort time: 0.002 s; external sort time: 0.025 s; total time: 0.027 s
Apr-16 00:02:53.626 [Actor Thread 50] DEBUG nextflow.file.FileCollector - Saved collect-files list to: /home/admin01/lab/bioinformatics-webapp/backend/app/work/collect-file/e13d3cac2f7e5f1987817038c130e1ea
Apr-16 00:02:53.626 [Actor Thread 4] DEBUG nextflow.file.FileCollector - Saved collect-files list to: /home/admin01/lab/bioinformatics-webapp/backend/app/work/collect-file/a3af30e013630e792bde467af8245352
Apr-16 00:02:53.649 [Actor Thread 50] DEBUG nextflow.file.FileCollector - Deleting file collector temp dir: /tmp/nxf-11455264030501300301
Apr-16 00:02:53.649 [Actor Thread 4] DEBUG nextflow.file.FileCollector - Deleting file collector temp dir: /tmp/nxf-11944051588663515651
Apr-16 00:02:53.659 [Actor Thread 25] WARN  nextflow.Nextflow - FASTQ file(/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_1.fastq.gz): Cannot extract flowcell ID from @normal#21#998579#1/1
Apr-16 00:02:55.665 [FileTransfer-1] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/b1/0f0e1c6ba24026502ff729716efc8d/mills_and_1000G.indels.vcf.gz
Apr-16 00:02:55.749 [FileTransfer-2] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/sarscov2/illumina/vcf/test2.vcf.gz to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/42/463389707b707a26518be6ea41fe11/test2.vcf.gz
Apr-16 00:02:55.750 [FileTransfer-3] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/be/4794cc7e175083f52fd79c85f739b6/genome.fasta
Apr-16 00:02:55.751 [FileTransfer-4] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/fd/470d58eef47f7eab7711a1c6ee9be3/dbsnp_146.hg38.vcf.gz
Apr-16 00:02:55.752 [FileTransfer-5] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/06/217701e83180d9685f636e4767bcc2/genome.interval_list
Apr-16 00:02:55.752 [FileTransfer-6] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/99/08c090c16e7bf20ff9d02914f569d5/gnomAD.r2.1.1.vcf.gz
Apr-16 00:02:55.907 [FileTransfer-7] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_1.fastq.gz to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/ee/9684ce6b836de09a51fda0c1ba20e8/test_1.fastq.gz
Apr-16 00:02:55.909 [FileTransfer-8] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_2.fastq.gz to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/29/64b921205c2cceba8b5e056639f663/test_2.fastq.gz
Apr-16 00:02:56.022 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:02:56.024 [Task submitter] INFO  nextflow.Session - [fa/c88657] Submitted process > NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX (genome.fasta)
Apr-16 00:02:56.037 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:02:56.038 [Task submitter] INFO  nextflow.Session - [3f/585f87] Submitted process > NFCORE_SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED (genome.interval_list)
Apr-16 00:02:56.048 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:02:56.048 [Task submitter] INFO  nextflow.Session - [98/d76dff] Submitted process > NFCORE_SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED (genome)
Apr-16 00:02:57.594 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:02:57.595 [Task submitter] INFO  nextflow.Session - [10/a724c4] Submitted process > NFCORE_SAREK:SAREK:FASTQC (test-test_L2)
Apr-16 00:02:57.606 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:02:57.606 [Task submitter] INFO  nextflow.Session - [b4/773fdb] Submitted process > NFCORE_SAREK:SAREK:FASTQC (test-test_L1)
Apr-16 00:03:25.532 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 2; name: NFCORE_SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED (genome.interval_list); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/3f/585f879dc3e0778e8cb206fc8d72c3]
Apr-16 00:03:25.533 [Task monitor] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'TaskFinalizer' minSize=10; maxSize=144; workQueue=LinkedBlockingQueue[-1]; allowCoreThreadTimeout=false
Apr-16 00:03:25.588 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:03:25.588 [Task submitter] INFO  nextflow.Session - [8e/1360f6] Submitted process > NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (chr22_1-40001)
Apr-16 00:03:33.890 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 8; name: NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX (genome.fasta); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/fa/c88657d46c4694cf8ba769139dc9a0]
Apr-16 00:03:35.820 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:03:35.822 [Task submitter] INFO  nextflow.Session - [11/7d8526] Submitted process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test-test_L2)
Apr-16 00:03:35.831 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:03:35.831 [Task submitter] INFO  nextflow.Session - [29/6b8417] Submitted process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test-test_L1)
Apr-16 00:03:35.954 [FileTransfer-9] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta.fai to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/fb/437e9bd0e466cd60681a502fae4ba1/genome.fasta.fai
Apr-16 00:03:55.782 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 15; name: NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (chr22_1-40001); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/8e/1360f6a9e6d2aacf3acc0a34bda990]
Apr-16 00:03:55.872 [TaskFinalizer-3] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT > Skipping output binding because one or more optional files are missing: fileoutparam<1:2>
Apr-16 00:04:31.873 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 18; name: NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test-test_L1); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/29/6b8417d29bf86c923400a705068725]
Apr-16 00:04:31.879 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 20; name: NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test-test_L2); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/11/7d8526e4c862e33003f45d0ee1db53]
Apr-16 00:04:31.884 [TaskFinalizer-5] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM > Skipping output binding because one or more optional files are missing: fileoutparam<1:1>
Apr-16 00:04:31.884 [TaskFinalizer-5] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM > Skipping output binding because one or more optional files are missing: fileoutparam<2:1>
Apr-16 00:04:31.884 [TaskFinalizer-5] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM > Skipping output binding because one or more optional files are missing: fileoutparam<3:1>
Apr-16 00:04:31.886 [TaskFinalizer-4] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM > Skipping output binding because one or more optional files are missing: fileoutparam<1:1>
Apr-16 00:04:31.886 [TaskFinalizer-4] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM > Skipping output binding because one or more optional files are missing: fileoutparam<2:1>
Apr-16 00:04:31.886 [TaskFinalizer-4] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM > Skipping output binding because one or more optional files are missing: fileoutparam<3:1>
Apr-16 00:04:32.350 [Actor Thread 17] DEBUG nextflow.extension.GroupTupleOp - GroupTuple dynamic size: key=[patient:test, sample:test, sex:XX, status:0, n_fastq:2, data_type:bam, id:test] size=2
Apr-16 00:04:32.350 [Actor Thread 17] DEBUG nextflow.extension.GroupTupleOp - GroupTuple dynamic size: key=[patient:test, sample:test, sex:XX, status:0, n_fastq:2, data_type:bam, id:test] size=2
Apr-16 00:04:32.589 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:04:32.590 [Task submitter] INFO  nextflow.Session - [22/cbb05c] Submitted process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (test)
Apr-16 00:04:36.673 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 13; name: NFCORE_SAREK:SAREK:FASTQC (test-test_L1); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/b4/773fdb1ba877212146581051feb39d]
Apr-16 00:04:36.692 [TaskFinalizer-6] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'PublishDir' minSize=10; maxSize=144; workQueue=LinkedBlockingQueue[-1]; allowCoreThreadTimeout=false
Apr-16 00:04:36.951 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 14; name: NFCORE_SAREK:SAREK:FASTQC (test-test_L2); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/10/a724c4d127b769d02bd53aef55a785]
Apr-16 00:05:06.208 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: NFCORE_SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED (genome); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/98/d76dff8938fcbd55bdc059dee78ef5]
Apr-16 00:05:06.232 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:05:06.233 [Task submitter] INFO  nextflow.Session - [f1/5d2911] Submitted process > NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED (genome)
Apr-16 00:05:07.066 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 25; name: NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED (genome); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/f1/5d29119988cd9a6fb89cb9620c7b11]
Apr-16 00:05:07.072 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED > Skipping output binding because one or more optional files are missing: fileoutparam<1:2>
Apr-16 00:07:05.956 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 24; name: NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/22/cbb05c4bc7c5475995adad5abe5261]
Apr-16 00:07:05.963 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES > Skipping output binding because one or more optional files are missing: fileoutparam<1:1>
Apr-16 00:07:05.963 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES > Skipping output binding because one or more optional files are missing: fileoutparam<3:1>
Apr-16 00:07:06.039 [Actor Thread 32] DEBUG nextflow.sort.BigSort - Sort completed -- entries: 1; slices: 1; internal sort time: 0.0 s; external sort time: 0.001 s; total time: 0.001 s
Apr-16 00:07:06.045 [Actor Thread 32] DEBUG nextflow.file.FileCollector - Saved collect-files list to: /home/admin01/lab/bioinformatics-webapp/backend/app/work/collect-file/341ed45f7932887da99acc2fa8444899
Apr-16 00:07:06.047 [Actor Thread 32] DEBUG nextflow.file.FileCollector - Deleting file collector temp dir: /tmp/nxf-826561285192230849
Apr-16 00:07:06.310 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:06.311 [Task submitter] INFO  nextflow.Session - [b0/d6a860] Submitted process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOOLS_STATS (test)
Apr-16 00:07:06.397 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:06.398 [Task submitter] INFO  nextflow.Session - [1a/442a6c] Submitted process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH (test)
Apr-16 00:07:07.660 [FileTransfer-10] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.dict to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/43/57d66047b149dbe459aa135b4839e9/genome.dict
Apr-16 00:07:07.661 [FileTransfer-1] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz.tbi to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/65/c2c70789f215e771e4e74a654e62fa/dbsnp_146.hg38.vcf.gz.tbi
Apr-16 00:07:07.661 [FileTransfer-2] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz.tbi to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/2e/88b11674097781a8eb19273af8c376/mills_and_1000G.indels.vcf.gz.tbi
Apr-16 00:07:07.850 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:07.850 [Task submitter] INFO  nextflow.Session - [0d/8bc038] Submitted process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (test)
Apr-16 00:07:12.189 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 29; name: NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/0d/8bc038e36dbc42551a95fa52933455]
Apr-16 00:07:12.194 [Actor Thread 23] DEBUG nextflow.extension.GroupTupleOp - GroupTuple dynamic size: key=[patient:test, sample:test, sex:XX, status:0, n_fastq:2, data_type:cram, id:test, num_intervals:1] size=1
Apr-16 00:07:12.299 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:12.300 [Task submitter] INFO  nextflow.Session - [cf/8be1dc] Submitted process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR (test)
Apr-16 00:07:12.318 [Actor Thread 36] DEBUG nextflow.sort.BigSort - Sort completed -- entries: 1; slices: 1; internal sort time: 0.0 s; external sort time: 0.003 s; total time: 0.003 s
Apr-16 00:07:12.604 [Actor Thread 36] DEBUG nextflow.file.FileCollector - Saved collect-files list to: /home/admin01/lab/bioinformatics-webapp/backend/app/work/collect-file/c8615314ac8c6605d7a36095b31a59f2
Apr-16 00:07:12.731 [Actor Thread 36] DEBUG nextflow.file.FileCollector - Deleting file collector temp dir: /tmp/nxf-4851259611326518850
Apr-16 00:07:17.982 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 30; name: NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/cf/8be1dcf0d153b98966d7fa17237d59]
Apr-16 00:07:17.986 [TaskFinalizer-2] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR > Skipping output binding because one or more optional files are missing: fileoutparam<0:1>
Apr-16 00:07:17.987 [Actor Thread 48] DEBUG nextflow.extension.GroupTupleOp - GroupTuple dynamic size: key=[patient:test, sample:test, sex:XX, status:0, n_fastq:2, data_type:cram, id:test, num_intervals:1] size=1
Apr-16 00:07:18.006 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:18.006 [Task submitter] INFO  nextflow.Session - [6c/a0c4dd] Submitted process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM (test)
Apr-16 00:07:21.379 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 31; name: NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/6c/a0c4dd1c190baadf0a2a8c82a9b88c]
Apr-16 00:07:21.383 [TaskFinalizer-3] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM > Skipping output binding because one or more optional files are missing: fileoutparam<0:1>
Apr-16 00:07:21.383 [TaskFinalizer-3] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM > Skipping output binding because one or more optional files are missing: fileoutparam<1:1>
Apr-16 00:07:21.394 [Actor Thread 32] DEBUG nextflow.sort.BigSort - Sort completed -- entries: 1; slices: 1; internal sort time: 0.0 s; external sort time: 0.001 s; total time: 0.001 s
Apr-16 00:07:21.399 [Actor Thread 32] DEBUG nextflow.file.FileCollector - Saved collect-files list to: /home/admin01/lab/bioinformatics-webapp/backend/app/work/collect-file/a55881771009da7b9cf1dc9165cf68c4
Apr-16 00:07:21.400 [Actor Thread 32] DEBUG nextflow.file.FileCollector - Deleting file collector temp dir: /tmp/nxf-12423623615587122273
Apr-16 00:07:21.784 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:21.785 [Task submitter] INFO  nextflow.Session - [5b/191211] Submitted process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:SAMTOOLS_STATS (test)
Apr-16 00:07:21.975 [FileTransfer-5] DEBUG nextflow.file.FilePorter - Copying foreign file https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/chr21/germlineresources/SNP_GRCh38_hg38_wChr.bed to work dir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/stage-3b55d465-1927-432e-a035-4ec4c66b04c3/ea/ec317779c5da0ef24657fb9e530890/SNP_GRCh38_hg38_wChr.bed
Apr-16 00:07:21.991 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:21.993 [Task submitter] INFO  nextflow.Session - [49/78b6bc] Submitted process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE (test)
Apr-16 00:07:22.030 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:22.031 [Task submitter] INFO  nextflow.Session - [cc/b50e56] Submitted process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH (test)
Apr-16 00:07:22.700 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 33; name: NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:SAMTOOLS_STATS (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/5b/191211bf47b9d3d59d0126efac7472]
Apr-16 00:07:23.127 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 26; name: NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOOLS_STATS (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/b0/d6a860d4b9e53ea19e8976c8915767]
Apr-16 00:07:41.599 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 27; name: NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/1a/442a6cef7b09f6fa09899be1097adc]
Apr-16 00:07:41.613 [TaskFinalizer-6] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<3:1>
Apr-16 00:07:41.613 [TaskFinalizer-6] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<4:1>
Apr-16 00:07:41.613 [TaskFinalizer-6] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<5:1>
Apr-16 00:07:41.613 [TaskFinalizer-6] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<8:1>
Apr-16 00:07:41.613 [TaskFinalizer-6] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<9:1>
Apr-16 00:07:41.613 [TaskFinalizer-6] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<10:1>
Apr-16 00:07:41.613 [TaskFinalizer-6] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<11:1>
Apr-16 00:07:41.637 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 34; name: NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/cc/b50e56a68e2ff04da0144a1eb44b80]
Apr-16 00:07:41.650 [TaskFinalizer-7] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<3:1>
Apr-16 00:07:41.650 [TaskFinalizer-7] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<4:1>
Apr-16 00:07:41.651 [TaskFinalizer-7] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<5:1>
Apr-16 00:07:41.651 [TaskFinalizer-7] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<8:1>
Apr-16 00:07:41.652 [TaskFinalizer-7] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<9:1>
Apr-16 00:07:41.652 [TaskFinalizer-7] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<10:1>
Apr-16 00:07:41.652 [TaskFinalizer-7] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH > Skipping output binding because one or more optional files are missing: fileoutparam<11:1>
Apr-16 00:07:51.029 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 36; name: NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/49/78b6bc907c58e5f0fcc632bee8ab89]
Apr-16 00:07:51.048 [Actor Thread 43] DEBUG nextflow.sort.BigSort - Sort completed -- entries: 1; slices: 1; internal sort time: 0.0 s; external sort time: 0.0 s; total time: 0.0 s
Apr-16 00:07:51.055 [Actor Thread 43] DEBUG nextflow.file.FileCollector - Saved collect-files list to: /home/admin01/lab/bioinformatics-webapp/backend/app/work/collect-file/09cc00ed125c22e99d0d6eba708493bc
Apr-16 00:07:51.056 [Actor Thread 43] DEBUG nextflow.file.FileCollector - Deleting file collector temp dir: /tmp/nxf-9435206248152368517
Apr-16 00:07:51.056 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:51.057 [Task submitter] INFO  nextflow.Session - [23/b1afc7] Submitted process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS (test)
Apr-16 00:07:51.070 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:51.071 [Task submitter] INFO  nextflow.Session - [38/e25e2d] Submitted process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT (test)
Apr-16 00:07:51.095 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:51.095 [Task submitter] INFO  nextflow.Session - [65/29b902] Submitted process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY (test)
Apr-16 00:07:51.110 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:07:51.110 [Task submitter] INFO  nextflow.Session - [95/c1d309] Submitted process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL (test)
Apr-16 00:07:51.431 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 4 -- submitted tasks are shown below
~> TaskHandler[id: 40; name: NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS (test); status: RUNNING; exit: -; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/23/b1afc73ec0a0c96d1ed7387bdbf79d]
~> TaskHandler[id: 38; name: NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT (test); status: RUNNING; exit: -; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/38/e25e2db77d780a89c08ae925e38cdb]
~> TaskHandler[id: 37; name: NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY (test); status: RUNNING; exit: -; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/65/29b90297b1d15d44ca43078926778d]
~> TaskHandler[id: 39; name: NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL (test); status: RUNNING; exit: -; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/95/c1d309fc0574eb843f135708b1c519]
Apr-16 00:08:07.620 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 39; name: NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/95/c1d309fc0574eb843f135708b1c519]
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<0:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<1:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<2:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<3:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<4:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<5:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<6:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<7:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<8:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<9:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<10:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<11:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<12:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<13:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<14:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<15:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<16:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<17:1>
Apr-16 00:08:07.666 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<19:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<20:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<21:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<22:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<23:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<24:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<25:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<26:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<27:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<28:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<29:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<30:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<31:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<32:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<33:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<34:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<35:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<36:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<37:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<38:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<39:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<40:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<41:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<42:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<43:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<44:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<45:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<46:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<47:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<48:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<49:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<50:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<51:1>
Apr-16 00:08:07.667 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<52:1>
Apr-16 00:08:07.668 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<53:1>
Apr-16 00:08:07.668 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<54:1>
Apr-16 00:08:07.668 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<55:1>
Apr-16 00:08:07.668 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<56:1>
Apr-16 00:08:07.668 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<57:1>
Apr-16 00:08:07.668 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<58:1>
Apr-16 00:08:07.668 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<59:1>
Apr-16 00:08:07.668 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<60:1>
Apr-16 00:08:07.668 [TaskFinalizer-9] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL > Skipping output binding because one or more optional files are missing: fileoutparam<61:1>
Apr-16 00:08:08.078 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 37; name: NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/65/29b90297b1d15d44ca43078926778d]
Apr-16 00:08:08.097 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 38; name: NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/38/e25e2db77d780a89c08ae925e38cdb]
Apr-16 00:08:08.129 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<0:1>
Apr-16 00:08:08.129 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<1:1>
Apr-16 00:08:08.129 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<2:1>
Apr-16 00:08:08.129 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<3:1>
Apr-16 00:08:08.129 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<4:1>
Apr-16 00:08:08.129 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<5:1>
Apr-16 00:08:08.129 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<6:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<7:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<8:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<9:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<10:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<11:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<12:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<13:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<14:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<15:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<16:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<17:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<18:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<20:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<21:1>
Apr-16 00:08:08.130 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<22:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<23:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<24:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<25:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<26:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<27:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<28:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<29:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<30:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<31:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<32:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<33:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<34:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<35:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<36:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<37:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<38:1>
Apr-16 00:08:08.131 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<39:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<40:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<41:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<42:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<43:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<44:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<45:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<46:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<47:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<48:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<49:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<50:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<51:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<52:1>
Apr-16 00:08:08.133 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<53:1>
Apr-16 00:08:08.134 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<54:1>
Apr-16 00:08:08.134 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<55:1>
Apr-16 00:08:08.134 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<56:1>
Apr-16 00:08:08.134 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<57:1>
Apr-16 00:08:08.134 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<58:1>
Apr-16 00:08:08.134 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<59:1>
Apr-16 00:08:08.134 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<60:1>
Apr-16 00:08:08.134 [TaskFinalizer-10] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY > Skipping output binding because one or more optional files are missing: fileoutparam<61:1>
Apr-16 00:08:08.193 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<0:1>
Apr-16 00:08:08.193 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<1:1>
Apr-16 00:08:08.193 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<2:1>
Apr-16 00:08:08.193 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<3:1>
Apr-16 00:08:08.194 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<4:1>
Apr-16 00:08:08.194 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<5:1>
Apr-16 00:08:08.194 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<6:1>
Apr-16 00:08:08.194 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<7:1>
Apr-16 00:08:08.194 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<8:1>
Apr-16 00:08:08.194 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<9:1>
Apr-16 00:08:08.194 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<10:1>
Apr-16 00:08:08.195 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<11:1>
Apr-16 00:08:08.195 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<12:1>
Apr-16 00:08:08.195 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<13:1>
Apr-16 00:08:08.195 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<14:1>
Apr-16 00:08:08.195 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<15:1>
Apr-16 00:08:08.195 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<16:1>
Apr-16 00:08:08.195 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<18:1>
Apr-16 00:08:08.195 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<19:1>
Apr-16 00:08:08.195 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<20:1>
Apr-16 00:08:08.195 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<21:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<22:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<23:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<24:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<25:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<26:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<27:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<28:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<29:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<30:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<31:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<32:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<33:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<34:1>
Apr-16 00:08:08.196 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<35:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<36:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<37:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<38:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<39:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<40:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<41:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<42:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<43:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<44:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<45:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<46:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<47:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<48:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<49:1>
Apr-16 00:08:08.197 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<50:1>
Apr-16 00:08:08.198 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<51:1>
Apr-16 00:08:08.198 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<52:1>
Apr-16 00:08:08.198 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<53:1>
Apr-16 00:08:08.198 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<54:1>
Apr-16 00:08:08.198 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<55:1>
Apr-16 00:08:08.198 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<56:1>
Apr-16 00:08:08.198 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<57:1>
Apr-16 00:08:08.198 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<58:1>
Apr-16 00:08:08.198 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<59:1>
Apr-16 00:08:08.198 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<60:1>
Apr-16 00:08:08.198 [TaskFinalizer-1] DEBUG nextflow.processor.TaskProcessor - Process NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT > Skipping output binding because one or more optional files are missing: fileoutparam<61:1>
Apr-16 00:08:33.956 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 40; name: NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS (test); status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/23/b1afc73ec0a0c96d1ed7387bdbf79d]
Apr-16 00:08:33.985 [Actor Thread 39] DEBUG nextflow.sort.BigSort - Sort completed -- entries: 12; slices: 1; internal sort time: 0.008 s; external sort time: 0.004 s; total time: 0.012 s
Apr-16 00:08:33.991 [Actor Thread 39] DEBUG nextflow.file.FileCollector - Saved collect-files list to: /home/admin01/lab/bioinformatics-webapp/backend/app/work/collect-file/c7f13d5382a259cfae63420bcea209f2
Apr-16 00:08:33.993 [Actor Thread 39] DEBUG nextflow.file.FileCollector - Deleting file collector temp dir: /tmp/nxf-9196505771329933044
Apr-16 00:08:34.484 [Actor Thread 23] DEBUG nextflow.util.HashBuilder - Hash asset file sha-256: /root/.nextflow/assets/nf-core/sarek/assets/multiqc_config.yml
Apr-16 00:08:34.532 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
Apr-16 00:08:34.532 [Task submitter] INFO  nextflow.Session - [a2/f9cc63] Submitted process > NFCORE_SAREK:SAREK:MULTIQC
Apr-16 00:10:26.895 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 41; name: NFCORE_SAREK:SAREK:MULTIQC; status: COMPLETED; exit: 0; error: -; workDir: /home/admin01/lab/bioinformatics-webapp/backend/app/work/a2/f9cc63a17e035010ff1c6ad449cf51]
Apr-16 00:10:26.909 [main] DEBUG nextflow.Session - Session await > all processes finished
Apr-16 00:10:26.995 [Task monitor] DEBUG n.processor.TaskPollingMonitor - <<< barrier arrives (monitor: local) - terminating tasks monitor poll loop
Apr-16 00:10:26.995 [main] DEBUG nextflow.Session - Session await > all barriers passed
Apr-16 00:10:27.005 [main] DEBUG nextflow.util.ThreadPoolManager - Thread pool 'TaskFinalizer' shutdown completed (hard=false)
Apr-16 00:10:27.234 [main] DEBUG nextflow.util.ThreadPoolManager - Thread pool 'PublishDir' shutdown completed (hard=false)
Apr-16 00:10:27.245 [main] INFO  nextflow.Nextflow - -[0;35m[nf-core/sarek][0;32m Pipeline completed successfully[0m-
Apr-16 00:10:27.256 [main] DEBUG n.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=23; failedCount=0; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=4m 8s; failedDuration=0ms; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=6; peakCpus=19; peakMemory=51 GB; ]
Apr-16 00:10:27.256 [main] DEBUG nextflow.trace.TraceFileObserver - Workflow completed -- saving trace file
Apr-16 00:10:27.259 [main] DEBUG nextflow.trace.ReportObserver - Workflow completed -- rendering execution report
Apr-16 00:10:27.837 [main] DEBUG nextflow.trace.TimelineObserver - Workflow completed -- rendering execution timeline
Apr-16 00:10:28.259 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done
Apr-16 00:10:28.285 [main] INFO  org.pf4j.AbstractPluginManager - Stop plugin 'nf-amazon@2.9.2'
Apr-16 00:10:28.286 [main] DEBUG nextflow.plugin.BasePlugin - Plugin stopped nf-amazon
Apr-16 00:10:28.286 [main] INFO  org.pf4j.AbstractPluginManager - Stop plugin 'nf-prov@1.2.2'
Apr-16 00:10:28.286 [main] DEBUG nextflow.plugin.BasePlugin - Plugin stopped nf-prov
Apr-16 00:10:28.286 [main] INFO  org.pf4j.AbstractPluginManager - Stop plugin 'nf-schema@2.2.1'
Apr-16 00:10:28.286 [main] DEBUG nextflow.plugin.BasePlugin - Plugin stopped nf-schema
Apr-16 00:10:28.287 [main] DEBUG nextflow.util.ThreadPoolManager - Thread pool 'FileTransfer' shutdown completed (hard=false)
Apr-16 00:10:28.288 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye

--- END FILE: ./backend/app/success_log_example ---

--- START FILE: ./conda_env.yml (Size: 753 bytes) ---
# File: conda_env.yml
# Updated for local backend development (FastAPI + RQ Worker)

name: bio-webapp-backend-dev # More specific name for local dev environment
channels:
  - conda-forge
  - defaults # Keep defaults as a fallback channel
dependencies:
  # Core Python version
  - python=3.11

  # FastAPI and Web Server
  - fastapi
  - uvicorn
  - httptools # Dependency for uvicorn[standard]
  - websockets # Dependency for uvicorn[standard]
  - python-multipart # For potential form data handling in FastAPI

  # RQ and Redis
  - rq
  - redis-py # Python client for Redis (package name on conda-forge)

  # Worker specific dependencies
  - psutil # For resource monitoring in tasks

  # Utility
  - pip # Include pip for installing packages if needed

--- END FILE: ./conda_env.yml ---

--- START FILE: ./docker-compose-ghcr.yml (Size: 4284 bytes) ---
# ./docker-compose-ghcr.yml
version: '3.8'

services:
  ##################
  # Frontend Application (Next.js)
  ##################
  frontend:
    image: ghcr.io/mikha-22/bioinformatics-webapp/frontend:latest # Use pre-built image
    container_name: bio_frontend
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    environment:
      - NODE_ENV=production
      # Point to the backend API service (internal Docker network)
      - NEXT_PUBLIC_API_BASE_URL=http://webapp:8000
      # Pass FileBrowser URL from host environment or use default
      - NEXT_PUBLIC_FILEBROWSER_URL=${NEXT_PUBLIC_FILEBROWSER_URL:-http://localhost:8081}
      - PORT=3000
    # No volumes needed when running pre-built image
    depends_on:
      - webapp
    restart: unless-stopped
    networks:
      - app-network
    # user: "nextjs" # Image should handle user internally

  ##################
  # Web Application (FastAPI Backend)
  ##################
  webapp:
    image: ghcr.io/mikha-22/bioinformatics-webapp/webapp:latest
    container_name: bio_webapp
    ports:
      - "${WEBAPP_PORT:-8000}:8000"
    volumes:
      - ./tls:/app/tls
      - ./backend:/app/backend:ro # Optional for dev override
      # --- UPDATED Paths ---
      - results_vol:/app/bioinformatics/results # Container path remains the same
      - data_vol:/app/bioinformatics/data     # Container path remains the same
      # -------------------
      - ./docker/settings.json:/app/docker/settings.json
    environment:
      - REDIS_HOST=redis
      - PYTHONUNBUFFERED=1
    user: "${UID:-1000}:${GID:-1000}"
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - app-network

  ##################
  # RQ Worker
  ##################
  worker:
    image: ghcr.io/mikha-22/bioinformatics-webapp/worker:latest
    container_name: bio_worker
    depends_on:
      - redis
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      # --- UPDATED Paths (Container paths remain the same) ---
      - data_vol:/data
      - results_vol:/results
      # -----------------------------------------------------
      - references_vol:/references
      - nextflow_work_vol:/nf_work
    environment:
      - REDIS_HOST=redis
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - NXF_WORK=/nf_work
    user: "${UID:-1000}:${GID:-1000}"
    restart: unless-stopped
    networks:
      - app-network

  ##################
  # Redis
  ##################
  redis:
    image: redis:7-alpine
    container_name: bio_redis
    volumes:
      - redis_data_vol:/data
    restart: unless-stopped
    networks:
      - app-network

  ##################
  # File Browser
  ##################
  filebrowser:
    image: ghcr.io/mikha-22/bioinformatics-webapp/filebrowser:latest
    container_name: bio_filebrowser
    ports:
      - "${FILEBROWSER_PORT:-8081}:8080"
    volumes:
      # --- UPDATED Paths (Container paths remain the same) ---
      - data_vol:/srv/data
      - results_vol:/srv/results
      # -----------------------------------------------------
      - ./docker/filebrowser.db:/config/filebrowser.db
      - ./docker/settings.json:/config/settings.json
      - ./tls/server.crt:/config/server.crt
      - ./tls/server.key:/config/server.key
    user: "${UID:-1000}:${GID:-1000}"
    restart: unless-stopped
    networks:
      - app-network

##################
# Networks
##################
networks:
  app-network:
    driver: bridge

##################
# Volumes
##################
# Volume definitions remain the same, only device paths change
volumes:
  data_vol:
    driver: local
    driver_opts:
      type: 'none'
      o: 'bind'
      # --- UPDATED Host Path ---
      device: '/home/admin01/work/mnt/nas/mikha_temp/data'
      # -----------------------
  results_vol:
    driver: local
    driver_opts:
      type: 'none'
      o: 'bind'
      # --- UPDATED Host Path ---
      device: '/home/admin01/work/mnt/nas/mikha_temp/results'
      # -----------------------
  references_vol: # Assuming this path remains the same, adjust if needed
    driver: local
    driver_opts:
      type: 'none'
      o: 'bind'
      device: '/home/mikha/labs/bioinformatics-webapp/bioinformatics/references'
  nextflow_work_vol:
    # Define if needed
  redis_data_vol:
    # Docker managed volume

--- END FILE: ./docker-compose-ghcr.yml ---

--- START FILE: ./docker/Dockerfile.worker (Size: 2645 bytes) ---
# ./docker/Dockerfile.worker
ARG PYTHON_VERSION=3.11
FROM python:${PYTHON_VERSION}-slim-bookworm AS python-base

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

WORKDIR /app

# --- Install OS Dependencies ---
# Install Java (for Nextflow), Docker CLI tools, and basic utilities
RUN apt-get update && apt-get install -y --no-install-recommends \
    bash \
    coreutils \
    curl \
    openjdk-17-jre-headless \
    ca-certificates \
    gnupg \
    lsb-release \
    && rm -rf /var/lib/apt/lists/*

# --- Install Docker CLI ---
# Based on official Docker install instructions for Debian
RUN mkdir -p /etc/apt/keyrings && \
    curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg && \
    echo \
      "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \
      $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null && \
    apt-get update && \
    apt-get install -y --no-install-recommends docker-ce-cli && \
    rm -rf /var/lib/apt/lists/*

# --- Install Nextflow ---
# Specify desired Nextflow version
ENV NXF_VER=24.04.3
RUN echo "Installing Nextflow..." && \
    cd /usr/local/bin && \
    curl -fsSL https://get.nextflow.io | bash && \
    chmod +x nextflow && \
    nextflow -v # Verify installation

# --- Create a non-root user ---
# Use the same UID/GID as webapp or define separately if needed
ARG UID=1000
ARG GID=1000
RUN groupadd --gid ${GID} appuser && \
    useradd --uid ${UID} --gid ${GID} --shell /bin/bash --create-home appuser

# --- Install Python Dependencies ---
COPY requirements_worker.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements_worker.txt

# --- Copy necessary application code for the worker ---
# Only copy what tasks.py needs (or mount via compose)
# Make sure the path structure allows `from backend.app.tasks import ...`
COPY ./backend/app/tasks.py /app/backend/app/tasks.py
COPY ./backend/app/utils /app/backend/app/utils
COPY ./backend/app/core/config.py /app/backend/app/core/config.py
# Ensure parent directories and __init__.py files exist
RUN mkdir -p /app/backend/app && \
    touch /app/backend/__init__.py /app/backend/app/__init__.py

# --- Environment Setup ---
ENV PYTHONPATH=/app
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

# Change ownership
RUN chown -R appuser:appuser /app /home/appuser

# Switch to the non-root user
USER appuser

# Define the command to run the RQ worker, connecting to 'redis' host
CMD ["rq", "worker", "pipeline_tasks", "--url", "redis://redis:6379/0"]

--- END FILE: ./docker/Dockerfile.worker ---

--- START FILE: ./docker/Dockerfile.filebrowser (Size: 1025 bytes) ---
FROM debian:bookworm-slim AS builder

WORKDIR /build

RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    bash \
    ca-certificates \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

RUN echo "Installing File Browser..." && \
    curl -fsSL https://raw.githubusercontent.com/filebrowser/get/master/get.sh | bash

FROM debian:bookworm-slim

RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

WORKDIR /app

RUN mkdir -p /config /srv

COPY --from=builder /usr/local/bin/filebrowser /usr/local/bin/filebrowser

ARG UID=1000
ARG GID=1000
RUN groupadd --gid ${GID} appuser && \
    useradd --uid ${UID} --gid ${GID} --shell /bin/bash --create-home appuser

RUN chown -R appuser:appuser /home/appuser /config /srv

USER appuser

EXPOSE 8080

CMD ["/usr/local/bin/filebrowser", \
     "--port=8080", \
     "--root=/srv", \
     "--database=/config/filebrowser.db", \
     "--config=/config/settings.json"]

--- END FILE: ./docker/Dockerfile.filebrowser ---

--- START FILE: ./docker/Dockerfile.frontend (Size: 1350 bytes) ---
# ./docker/Dockerfile.frontend

# Stage 1: Base Stage
FROM node:20-bookworm-slim AS base
ENV NODE_ENV=production
WORKDIR /app
ENV CYPRESS_INSTALL_BINARY=0

# Stage 2: Dependencies Stage
FROM base AS deps
WORKDIR /app
COPY package.json package-lock.json* ./
RUN npm ci

# Stage 3: Builder Stage
FROM base AS builder
WORKDIR /app
COPY --from=deps /app/node_modules ./node_modules
COPY package.json package-lock.json* tsconfig.json next.config.ts postcss.config.mjs tailwind.config.ts ./
COPY . .
RUN rm -rf .next
ENV NEXT_TELEMETRY_DISABLED=1
RUN npm run build

# Stage 4: Production Runner Stage
FROM base AS runner
WORKDIR /app
ENV NODE_ENV=production
ENV CYPRESS_INSTALL_BINARY=0
ENV PORT=3000
ENV NEXT_TELEMETRY_DISABLED=1

RUN groupadd --system --gid 1001 nodejs && \
    useradd --system --uid 1001 --gid nodejs nextjs

# --- COPY package-lock.json as well ---
COPY --from=builder /app/package.json /app/package-lock.json* ./
# --------------------------------------

# Install ONLY production dependencies
RUN npm ci --omit=dev

# Copy necessary standalone/static files from builder stage
COPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./
COPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static
COPY --from=builder --chown=nextjs:nodejs /app/public ./public

USER nextjs
EXPOSE 3000
CMD ["node", "server.js"]

--- END FILE: ./docker/Dockerfile.frontend ---

--- START FILE: ./docker/Dockerfile.webapp (Size: 1716 bytes) ---
# ./docker/Dockerfile.webapp
# Use a specific Python version slim image
ARG PYTHON_VERSION=3.11
FROM python:${PYTHON_VERSION}-slim-bookworm AS python-base

# Set environment variables for Python
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

WORKDIR /app

# --- OS Dependencies (Install only if necessary for webapp runtime) ---
# RUN apt-get update && apt-get install -y --no-install-recommends some-package && rm -rf /var/lib/apt/lists/*

# --- Create a non-root user for security ---
ARG UID=1000
ARG GID=1000
RUN groupadd --gid ${GID} appuser && \
    useradd --uid ${UID} --gid ${GID} --shell /bin/bash --create-home appuser

# --- Install Python Dependencies ---
# Copy requirements file first for layer caching
COPY requirements_webapp.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements_webapp.txt

# --- Copy Application Code ---
# Copy backend structure needed by the webapp (routers, models, core, app.py etc)
COPY ./backend /app/backend
# REMOVED: Copy frontend files - No longer needed as frontend is separate Next.js app
# COPY ./frontend /app/frontend
# Copy the main entrypoint script
COPY ./main.py /app/main.py
# Ensure necessary __init__.py files exist if copying selectively

# Change ownership to the non-root user
RUN chown -R appuser:appuser /app

# Switch to the non-root user
USER appuser

# Expose the port FastAPI will run on
EXPOSE 8000

# Define the command to start the FastAPI application
# Adjust if your main.py or uvicorn command needs specific args or paths
# Assumes main.py handles Uvicorn startup and TLS paths (if needed) correctly,
# especially considering TLS certs are mounted via compose.
CMD ["python", "main.py"]

--- END FILE: ./docker/Dockerfile.webapp ---

--- START FILE: ./docker-compose-localbuild.yml (Size: 4931 bytes) ---
# ./docker-compose.yml
version: '3.8'

services:
  ##################
  # Frontend Application (Next.js)
  ##################
  frontend:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.frontend
    container_name: bio_frontend
    ports:
      - "${FRONTEND_PORT:-3000}:3000" # Map host port 3000 (or env var) to container 3000
    environment:
      # Tell Next.js runtime it's production
      - NODE_ENV=production
      # Point to the backend API service (internal Docker network)
      - NEXT_PUBLIC_API_BASE_URL=http://webapp:8000
      # Pass FileBrowser URL (can be external or internal depending on setup)
      # Using host.docker.internal might work for local dev if FileBrowser is exposed on host
      # Or use the external URL directly if that's how frontend accesses it
      - NEXT_PUBLIC_FILEBROWSER_URL=${NEXT_PUBLIC_FILEBROWSER_URL:-http://localhost:8081} # Default to localhost:8081 if host env not set
      # Explicitly set port for Next.js server inside container
      - PORT=3000
    # No code volumes needed for standalone build generally
    # volumes:
      # - ./frontend_app:/app # Only for development with hot-reloading (requires different Dockerfile setup)
    depends_on:
      - webapp # Optional: Frontend might start before backend is fully ready, but should handle retries
    restart: unless-stopped
    networks:
      - app-network
    # user: "nextjs" # Matches user in Dockerfile runner stage (optional, Docker handles it)

  ##################
  # Web Application (FastAPI Backend)
  ##################
  webapp:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.webapp
    container_name: bio_webapp
    ports:
      - "${WEBAPP_PORT:-8000}:8000"
    volumes:
      - ./backend:/app/backend # Keep for dev convenience
      - ./main.py:/app/main.py # Keep for dev convenience
      - ./tls:/app/tls
      # --- UPDATED Paths ---
      - results_vol:/app/bioinformatics/results # Container path remains the same
      - data_vol:/app/bioinformatics/data     # Container path remains the same
      # -------------------
      - ./docker/settings.json:/app/docker/settings.json
    environment:
      - REDIS_HOST=redis
      - PYTHONUNBUFFERED=1
    user: "${UID:-1000}:${GID:-1000}"
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - app-network

  ##################
  # RQ Worker
  ##################
  worker:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.worker
    container_name: bio_worker
    depends_on:
      - redis
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      # --- UPDATED Paths (Container paths remain the same) ---
      - data_vol:/data
      - results_vol:/results
      # -----------------------------------------------------
      - references_vol:/references
      - nextflow_work_vol:/nf_work
    environment:
      - REDIS_HOST=redis
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - NXF_WORK=/nf_work
    user: "${UID:-1000}:${GID:-1000}"
    restart: unless-stopped
    networks:
      - app-network

  ##################
  # Redis
  ##################
  redis:
    image: redis:7-alpine
    container_name: bio_redis
    volumes:
      - redis_data_vol:/data
    restart: unless-stopped
    networks:
      - app-network

  ##################
  # File Browser
  ##################
  filebrowser:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.filebrowser
    container_name: bio_filebrowser
    ports:
      - "${FILEBROWSER_PORT:-8081}:8080"
    volumes:
      # --- UPDATED Paths (Container paths remain the same) ---
      - data_vol:/srv/data
      - results_vol:/srv/results
      # -----------------------------------------------------
      - ./docker/filebrowser.db:/config/filebrowser.db
      - ./docker/settings.json:/config/settings.json
      - ./tls/server.crt:/config/server.crt
      - ./tls/server.key:/config/server.key
    user: "${UID:-1000}:${GID:-1000}"
    restart: unless-stopped
    networks:
      - app-network

##################
# Networks
##################
networks:
  app-network:
    driver: bridge

##################
# Volumes
##################
volumes:
  data_vol:
    driver: local
    driver_opts:
      type: 'none'
      o: 'bind'
      # --- UPDATED Host Path ---
      device: '/home/admin01/work/mnt/nas/mikha_temp/data'
      # -----------------------
  results_vol:
    driver: local
    driver_opts:
      type: 'none'
      o: 'bind'
      # --- UPDATED Host Path ---
      device: '/home/admin01/work/mnt/nas/mikha_temp/results'
      # -----------------------
  references_vol: # Assuming this path remains the same, adjust if needed
    driver: local
    driver_opts:
      type: 'none'
      o: 'bind'
      device: '/home/mikha/labs/bioinformatics-webapp/bioinformatics/references'
  nextflow_work_vol:
    # Define if needed
  redis_data_vol:
    # Docker managed volume

--- END FILE: ./docker-compose-localbuild.yml ---

--- START FILE: ./logs_dev/tail.pid (Size: 7 bytes) ---
245796

--- END FILE: ./logs_dev/tail.pid ---

--- START FILE: ./logs_dev/frontend.pid (Size: 7 bytes) ---
245684

--- END FILE: ./logs_dev/frontend.pid ---

--- START FILE: ./logs_dev/backend.pid (Size: 7 bytes) ---
245627

--- END FILE: ./logs_dev/backend.pid ---

--- START FILE: ./logs_dev/worker.pid (Size: 7 bytes) ---
245655

--- END FILE: ./logs_dev/worker.pid ---

--- START FILE: ./nextflow (Size: 17641 bytes) ---
#!/usr/bin/env bash
#
#  Copyright 2013-2024, Seqera Labs
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.

[[ "$NXF_DEBUG" == 'x' ]] && set -x
NXF_VER=${NXF_VER:-'24.10.5'}
NXF_ORG=${NXF_ORG:-'nextflow-io'}
NXF_HOME=${NXF_HOME:-$HOME/.nextflow}
NXF_PROT=${NXF_PROT:-'https'}
NXF_BASE=${NXF_BASE:-$NXF_PROT://www.nextflow.io/releases}
NXF_TEMP=${NXF_TEMP:-$TMPDIR}
NXF_DIST=${NXF_DIST:-$NXF_HOME/framework}
NXF_CLI="$0 $@"
NXF_CLI_OPTS=${NXF_CLI_OPTS:-}
NXF_REMOTE_DEBUG_PORT=${NXF_REMOTE_DEBUG_PORT:-5005}

function cmp_ver() {
    local IFS=.
    local i ver1 ver2
    read -r -a ver1 <<< "${1//./ }"
    read -r -a ver2 <<< "${2//./ }"

    # Compare major, minor, and patch numbers
    for ((i=0; i<3; i++)); do
        ver1[i]=${ver1[i]//[^0-9]}
        ver2[i]=${ver2[i]//[^0-9]}
        [[ ${ver1[i]:-0} -lt ${ver2[i]:-0} ]] && echo -1 && return
        [[ ${ver1[i]:-0} -gt ${ver2[i]:-0} ]] && echo 1 && return
    done

    # Extract suffixes for comparison
    local suffix1="${1##*.}"
    local suffix2="${2##*.}"
    suffix1="${suffix1//[0-9]}"
    suffix2="${suffix2//[0-9]}"

    # Compare suffixes
    [[ -z $suffix1 && -n $suffix2 ]] && echo 1 && return
    [[ -n $suffix1 && -z $suffix2 ]] && echo -1 && return
    [[ $suffix1 < $suffix2 ]] && echo -1 && return
    [[ $suffix1 > $suffix2 ]] && echo 1 && return

    # Versions are equal
    echo 0
}

# if the nextflow version is greater or equals to "24.07.0-edge" the new shadow jar launcher
# should be used, otherwise fallback on the legacy behavior setting the variable NXF_LEGACY_LAUNCHER
NXF_LEGACY_LAUNCHER=1
if [[ $(cmp_ver "$NXF_VER" "24.07.0-edge") -ge 0 ]]; then
  unset NXF_LEGACY_LAUNCHER
fi

export NXF_CLI
export NXF_ORG
export NXF_HOME

if [[ $TERM && $TERM != 'dumb' ]]; then
if command -v tput &>/dev/null; then
GREEN=$(tput setaf 2; tput bold)
YELLOW=$(tput setaf 3)
RED=$(tput setaf 1)
NORMAL=$(tput sgr0)
fi
fi

function echo_red() {
    >&2 echo -e "$RED$*$NORMAL"
}

function echo_green() {
    echo -e "$GREEN$*$NORMAL"
}

function echo_yellow() {
    >&2 echo -e "$YELLOW$*$NORMAL"
}

function die() {
  echo_red "$*"
  exit 1
}

function get_abs_filename() {
  echo "$(cd "$(dirname "$1")" && pwd)/$(basename "$1")"
}

function get() {
    if command -v curl &>/dev/null; then
        GET="curl -fsSL '$1' -o '$2'"
    elif command -v wget &>/dev/null; then
        GET="wget '$1' -O '$2' >/dev/null 2>&1"
    else
        echo_red "ERROR: Cannot find 'curl' nor 'wget' utility --  please install one of them"
        exit 1
    fi

    printf "Downloading nextflow dependencies. It may require a few seconds, please wait .. "
    eval $GET; status=$?
    printf "\r\033[K"
    if [ $status -ne 0 ]; then
        echo_red "ERROR: Cannot download nextflow required file -- make sure you can connect to the internet"
        echo ""
        echo "Alternatively you can try to download this file:"
        echo "    $1"
        echo ""
        echo "and save it as:"
        echo "    ${3:-$2}"
        echo ""
        exit 1
    fi
}

function get_ver() {
    if command -v curl &>/dev/null; then
        curl -fsSL "$1"
    elif command -v wget &>/dev/null; then
        wget "$1" >/dev/null 2>&1
    else
        echo_red "ERROR: Cannot find 'curl' nor 'wget' utility -- please install one of them"
        exit 1
    fi
}

function make_temp() {
    local base=${NXF_TEMP:=$PWD}
    if [ "$(uname)" = 'Darwin' ]; then mktemp "${base}/nxf-tmp.XXXXXX" || exit $?
    else mktemp -t nxf-tmp.XXXXXX -p "${base}" || exit $?
    fi
}

function resolve_link() {
    [[ ! -f $1 ]] && exit 1
    if command -v realpath &>/dev/null; then
      realpath "$1"
    elif command -v readlink &>/dev/null; then
      local target="$1"
      cd "$(dirname "$target")"; target="$(basename "$target")"
      while [ -L "$target" ]; do
        target="$(readlink "$target")"
        cd "$(dirname "$target")"; target="$(basename "$target")"
      done
      echo "$(cd "$(dirname "$target")"; pwd -P)/$target"
    else
      echo_yellow "WARN: Neither \`realpath\` nor \`readlink\` command can be found"
      exit 1
    fi
}

function current_ver() {
  [[ $NXF_EDGE == 1 || $NXF_VER == *"-edge" ]] && printf 'edge' || printf 'latest'
}

function install() {
    local tmpfile=$(make_temp)
    local version=$(set +u; [[ $NXF_VER ]] && printf "v$NXF_VER" || current_ver)
    local action="a=${2:-default}"
    get "$NXF_BASE/$version/nextflow?$action" "$tmpfile" "$1" || exit $?
    mv "$tmpfile" "$1" || exit $?
    chmod +x "$1" || exit $?
    bash "$1" -download || exit $?
    echo ''
    echo -e $'Nextflow installation completed. Please note:'
    echo -e $'- the executable file `nextflow` has been created in the folder:' $(dirname $1)
    if [[ ! "$PATH" =~ (^|:)"$(dirname $1)"(:|$) ]]; then
    echo -e $'- you may complete the installation by moving it to a directory in your $PATH'
    fi
    echo ''
}

function check_latest() {
    [[ $cmd != run ]] && return 0
    [[ $NXF_OFFLINE == true || $NXF_DISABLE_CHECK_LATEST == true ]] && return 0
    local latest=$(get_ver "$NXF_BASE/$(current_ver)/version?current=$NXF_VER")
    if [[ -n "$latest" && $(cmp_ver "$latest" "$NXF_VER") -gt 0 ]]; then
      echo_yellow "Nextflow $latest is available - Please consider updating your version to it"
    fi
}

function launch_nextflow() {
    # the launch command line
    local cmdline=()
    # remove leading and trailing double-quotes
    for x in "${launcher[@]}"; do
        x="${x%\"}"
        x="${x#\"}"
        cmdline+=("$x") 
    done 

    if [[ "$bg" ]]; then
      local pid_file="${NXF_PID_FILE:-.nextflow.pid}"
      cmdline+=("${args[@]}")
      exec "${cmdline[@]}" &
      disown
      echo $! > "$pid_file"
      exit 0
    fi

    cmdline+=("${args[@]}")
    exec "${cmdline[@]}"
    exit 1
}

# check self-install
if [ "$0" = "bash" ] || [[ "$0" =~ .*/bash ]]; then
    if [ -d nextflow ]; then
        echo 'Please note:'
        echo "- The install procedure needs to create a file named 'nextflow' in this folder, but a directory with this name already exists."
        echo "- Please renamed/delete that directory, or execute the Nextflow install procedure in another folder."
        echo ''
        exit 1
    fi
    install "$PWD/nextflow" install
    exit 0
fi

# clean up env
# see https://github.com/nextflow-io/nextflow/issues/1716
unset JAVA_TOOL_OPTIONS

# parse the command line
bg=''
declare -a jvmopts=()
declare -a args=("$@")
declare -a commands=(clone config drop help history info ls pull run view node console kuberun)
# $NXF_CLI_OPTS allow to pass arbitrary cli opts via the environment
# note: do not wrap with quotes because the variable can be used to specify more than on option separate by blanks
[ "$NXF_CLI_OPTS" ] && args+=($NXF_CLI_OPTS)

cmd=''
while [[ $# != 0 ]]; do
    case $1 in
    -D*)
      if [[ ! "$cmd" ]]; then
      jvmopts+=("$1")
      fi
      ;;
    -bg)
      bg=1
      ;;
    -remote-debug)
      echo_yellow "Enabling script debugging - continue the execution launching the remote VM debugger in your favourite IDE using port $NXF_REMOTE_DEBUG_PORT"
      remote_debug=1
      ;;
    -download)
      if [[ ! "$cmd" ]]; then
      rm -rf "$NXF_DIST/$NXF_VER" || exit $?
      bash "$0" -version || exit $?
      exit 0
      fi
      ;;
    -self-update|self-update)
      if [[ ! "$cmd" ]]; then
      [[ -z $NXF_EDGE && $NXF_VER = *-edge ]] && NXF_EDGE=1
      unset NXF_VER
      install "$0" update
      exit 0
      fi
      ;;
    *)
      [[ $1 && $1 != -* && ! "$cmd" && ${commands[*]} =~ $1 ]] && cmd=$1
      ;;
    esac
    shift
done

CAPSULE_LOG=${CAPSULE_LOG:=''}
CAPSULE_RESET=${CAPSULE_RESET:=''}
CAPSULE_CACHE_DIR=${CAPSULE_CACHE_DIR:="$NXF_HOME/capsule"}

NXF_PACK=one
NXF_MODE=${NXF_MODE:-''}
NXF_JAR=${NXF_JAR:-nextflow-$NXF_VER-$NXF_PACK.jar}
NXF_BIN=${NXF_BIN:-$NXF_DIST/$NXF_VER/$NXF_JAR}
NXF_PATH=$(dirname "$NXF_BIN")
NXF_URL=${NXF_URL:-$NXF_BASE/v$NXF_VER/$NXF_JAR}
NXF_HOST=${HOSTNAME:-localhost}
[[ $NXF_LAUNCHER ]] || NXF_LAUNCHER=${NXF_HOME}/tmp/launcher/nextflow-${NXF_PACK}_${NXF_VER}/${NXF_HOST}
# both NXF_GRAB and NXF_CLASSPATH are not supported any more as of version 24.04.7-edge
NXF_GRAB=${NXF_GRAB:-''}
NXF_CLASSPATH=${NXF_CLASSPATH:-''}

# Determine the path to this file
if [[ $NXF_PACK = dist ]]; then
    NXF_BIN=$(which "$0" 2>/dev/null)
    [ $? -gt 0 -a -f "$0" ] && NXF_BIN="./$0"
fi

# use nextflow custom java home path
if [[ "$NXF_JAVA_HOME" ]]; then
  JAVA_HOME="$NXF_JAVA_HOME"
  unset JAVA_CMD
fi
# Determine the Java command to use to start the JVM.
if [ ! -x "$JAVA_CMD" ] ; then
    if [ -d "$JAVA_HOME" ] ; then
        if [ -x "$JAVA_HOME/jre/sh/java" ] ; then
            # IBM's JDK on AIX uses strange locations for the executables
            JAVA_CMD="$JAVA_HOME/jre/sh/java"
        else
            JAVA_CMD="$JAVA_HOME/bin/java"
        fi
    elif [ -x /usr/libexec/java_home ]; then
        JAVA_CMD="$(/usr/libexec/java_home -v 11+ 2>/dev/null)/bin/java" || JAVA_CMD=java
    else
        JAVA_CMD="$(which java)" || JAVA_CMD=java
    fi
fi

# Retrieve the java version from a NF local file
JAVA_KEY="$NXF_HOME/tmp/ver/$(resolve_link "$JAVA_CMD" | sed 's@/@.@g')"
if [ -f "$JAVA_KEY" ]; then
  JAVA_VER="$(cat "$JAVA_KEY")"
else
  JAVA_VER="$("$JAVA_CMD" $NXF_OPTS -version 2>&1)"
  if [ $? -ne 0 ]; then
      getstarted_web="https://www.nextflow.io/docs/latest/getstarted.html"
      echo_red "${JAVA_VER:-Failed to launch the Java virtual machine}"
      echo_red "NOTE: Nextflow needs a Java virtual machine to run. To this end:
 - make sure a \`java\` command can be found; or
 - manually define the variables JAVA_HOME to point to an existing installation; or
 - install a Java virtual machine, for instance through https://sdkman.io (read the docs);
 - for more details please refer to the Nextflow Get Started page at http://docs.nextflow.io."
      echo_yellow "NOTE: Nextflow is trying to use the Java VM defined by the following environment variables:\n JAVA_CMD: $JAVA_CMD\n NXF_OPTS: $NXF_OPTS\n"
      exit 1
  fi
  JAVA_VER=$(echo "$JAVA_VER" | awk '/version/ {gsub(/"/, "", $3); print $3}')
  # check NF version
  if [[ ! $NXF_VER =~ ([0-9]+)\.([0-9]+)\.([0-9].*) ]]; then
    echo_red "Not a valid Nextflow version: $NXF_VER"
    exit 1
  fi
  major=${BASH_REMATCH[1]}
  minor=${BASH_REMATCH[2]}
  # legacy version - Java 7/8 only
  if [ $major -eq 0 ] && [ $minor -lt 26 ]; then
    version_check="^(1.7|1.8)"
    version_message="Java 7 or 8"
  else
    version_check="^(1.8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23)"
    version_message="Java 8 or later (up to 22)"
  fi
  if [[ ! $JAVA_VER =~ $version_check ]]; then
      echo_red "ERROR: Cannot find Java or it's a wrong version -- please make sure that $version_message is installed"
      if [[ "$NXF_JAVA_HOME" ]]; then
      echo_yellow "NOTE: Nextflow is trying to use the Java VM defined by the following environment variables:\n JAVA_CMD: $JAVA_CMD\n NXF_JAVA_HOME: $NXF_JAVA_HOME\n"
      else
      echo_yellow "NOTE: Nextflow is trying to use the Java VM defined by the following environment variables:\n JAVA_CMD: $JAVA_CMD\n JAVA_HOME: $JAVA_HOME\n"
      fi
      exit 1
  fi
  if [[ ! $JAVA_VER =~ ^(11|12|13|14|15|16|17|18|19|20|21|22|23) ]]; then
      echo_yellow "NOTE: Nextflow is not tested with Java $JAVA_VER -- It's recommended the use of version 11 up to 23\n"
  fi
  mkdir -p "$(dirname "$JAVA_KEY")"
  [[ -f $JAVA_VER ]] && echo $JAVA_VER > "$JAVA_KEY"
fi

# Verify nextflow jar is available
if [ ! -f "$NXF_BIN" ]; then
    [ -f "$NXF_PATH" ] && rm "$NXF_PATH"
    mkdir -p "$NXF_PATH" || exit $?
    tmpfile=$(make_temp)
    get "$NXF_URL" "$tmpfile" "$NXF_BIN"
    mv "$tmpfile" "$NXF_BIN"
fi

COLUMNS=${COLUMNS:-`tty -s && tput cols 2>/dev/null || true`}
declare -a JAVA_OPTS=()
JAVA_OPTS+=(-Dfile.encoding=UTF-8 -Dcapsule.trampoline -Dcapsule.java.cmd="$JAVA_CMD" -Dcom.sun.security.enableAIAcaIssuers=true)
if [[ $cmd == console ]]; then bg=1;
else JAVA_OPTS+=(-Djava.awt.headless=true)
fi

if [[ $NXF_LEGACY_LAUNCHER ]]; then
  [[ "$JAVA_HOME" ]] && JAVA_OPTS+=(-Dcapsule.java.home="$JAVA_HOME")
  [[ "$CAPSULE_LOG" ]] && JAVA_OPTS+=(-Dcapsule.log=$CAPSULE_LOG)
  [[ "$CAPSULE_RESET" ]] && JAVA_OPTS+=(-Dcapsule.reset=true)
fi
[[ "$cmd" != "run" && "$cmd" != "node" ]] && JAVA_OPTS+=(-XX:+TieredCompilation -XX:TieredStopAtLevel=1)
[[ "$NXF_OPTS" ]] && JAVA_OPTS+=($NXF_OPTS)
[[ "$NXF_CLASSPATH" ]] && export NXF_CLASSPATH
[[ "$NXF_GRAB" ]] && export NXF_GRAB
[[ "$COLUMNS" ]] && export COLUMNS
[[ "$NXF_TEMP" ]] && JAVA_OPTS+=(-Djava.io.tmpdir="$NXF_TEMP")
[[ "${jvmopts[@]}" ]] && JAVA_OPTS+=("${jvmopts[@]}")
export JAVA_CMD
export CAPSULE_CACHE_DIR
export NXF_PLUGINS_DIR
export NXF_PLUGINS_MODE
export NXF_PLUGINS_DEFAULT
export NXF_PACK
export NXF_ENABLE_VIRTUAL_THREADS

# lookup the a `md5` command
if hash md5sum 2>/dev/null; then MD5=md5sum;
elif hash gmd5sum 2>/dev/null; then MD5=gmd5sum;
elif hash md5 2>/dev/null; then MD5=md5;
else MD5=''
fi

# when no md5 command is available fallback on default execution
if [ ! "$MD5" ] || [ "$CAPSULE_RESET" ]; then
    launcher=($("$JAVA_CMD" "${JAVA_OPTS[@]}" -jar "$NXF_BIN"))
    launch_nextflow
    exit 1
fi

# creates a md5 unique for the given variables
env_md5() {
cat <<EOF | $MD5 | cut -f1 -d' '
$JAVA_CMD
$JAVA_VER
${JAVA_OPTS[@]}
$NXF_HOME
$NXF_VER
$NXF_OPTS
$NXF_GRAB
$NXF_CLASSPATH
$NXF_JVM_ARGS
$NXF_ENABLE_VIRTUAL_THREADS
EOF
}

# checked if a cached classpath file exists and it newer that the nextflow boot jar file
LAUNCH_FILE="${NXF_LAUNCHER}/classpath-$(env_md5)"

if [ -s "$LAUNCH_FILE" ] && [ "$LAUNCH_FILE" -nt "$NXF_BIN" ] && [[ "$remote_debug" -ne 1 ]]; then
    declare -a launcher="($(cat "$LAUNCH_FILE"))"
else
    if [[ $NXF_LEGACY_LAUNCHER ]]; then
      # otherwise run the capsule and get the result classpath in the 'launcher' and save it to a file
      cli=($("$JAVA_CMD" "${JAVA_OPTS[@]}" -jar "$NXF_BIN"))
      [[ $? -ne 0 ]] && echo_red 'Unable to initialize nextflow environment' && exit 1
    else
      # otherwise parse the command and get the result classpath in the 'launcher' and save it to a file
      cli=("\"$JAVA_CMD\"" "${JAVA_OPTS[@]}" -jar "$NXF_BIN")
    fi

    # first string between double quotes is the full path to java, also blank spaces are included
    # remainder string are arguments
    # we extract first part into `cmd_base`` and remainder into `cmd_tail`` and convert them to array as previous version
    cmd_pattern='"([^"]*)"(.*)'
    [[ "${cli[@]}" =~ $cmd_pattern ]]
    declare -a cmd_base="(${BASH_REMATCH[1]})"
    declare -a cmd_tail="(${BASH_REMATCH[2]})"

    launcher="${cmd_base[@]}"
    [[ "$NXF_JVM_ARGS" ]] && launcher+=($NXF_JVM_ARGS)
    [[ "$remote_debug" ]] && launcher+=(-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=$NXF_REMOTE_DEBUG_PORT)

    if [[ "$JAVA_VER" =~ ^(9|10|11|12|13|14|15|16|17|18|19|20|21|22|23) ]]; then
      launcher+=(--add-opens=java.base/java.lang=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/java.io=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/java.nio=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/java.net=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/java.util=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/java.nio.file.spi=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/sun.nio.ch=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/sun.nio.fs=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/sun.net.www.protocol.http=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/sun.net.www.protocol.https=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/sun.net.www.protocol.ftp=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/sun.net.www.protocol.file=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/jdk.internal.misc=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/jdk.internal.vm=ALL-UNNAMED)
      launcher+=(--add-opens=java.base/java.util.regex=ALL-UNNAMED)
      if [[ "$NXF_ENABLE_VIRTUAL_THREADS" == 'true' ]]; then
        if [[ "$JAVA_VER" =~ ^(19|20) ]]; then launcher+=(--enable-preview)
        elif [[ ! "$JAVA_VER" =~ ^(21|22|23) ]]; then die "Virtual threads require Java 19 or later - current version $JAVA_VER"
        fi
      fi
      launcher+=("${cmd_tail[@]}")
    else
      launcher+=("${cmd_tail[@]}")
    fi

    # create the launch file only when using the legacy launcher (capsule)
    if [[ $NXF_LEGACY_LAUNCHER ]]; then
    # Don't show errors if the LAUNCH_FILE can't be created
    if mkdir -p "${NXF_LAUNCHER}" 2>/dev/null; then
        STR=''
        for x in "${launcher[@]}"; do
        [[ "$x" != "\"-Duser.dir=$PWD\"" ]] && [[ ! "$x" == *"-agentlib:jdwp"* ]] && STR+=$(printf '%q ' "$x")
        done
        printf "$STR">"$LAUNCH_FILE"
    else
        echo_yellow "Warning: Couldn't create cached classpath folder: $NXF_LAUNCHER -- Maybe NXF_HOME is not writable?"
    fi
    fi

fi

# check for latest version
check_latest
# finally run it
launch_nextflow

--- END FILE: ./nextflow ---

--- START FILE: ./contextualize.sh (Size: 11257 bytes) ---
#!/bin/bash

# === gather_context_v2.sh ===
# Gathers project context into a single file for LLMs, with improved filtering.

# --- Default Configuration ---
DEFAULT_PROJECT_DIR="." # Default to current directory
DEFAULT_OUTPUT_FILE="project_context_v2.txt"
DEFAULT_MAX_SIZE_MB=1 # Default Max Size in MiB
DEFAULT_CONTEXTIGNORE=".contextignore" # File listing patterns to ignore

# --- Internal Exclusions (Always applied unless overridden by .contextignore logic if we added that) ---
# Essential directories usually not needed for context
DEFAULT_EXCLUDE_PATHS=(
    "./.git/"
    "./__pycache__/"
    "./node_modules/"       # Excludes top-level node_modules
    "./venv/"
    "./.venv/"
    "./build/"
    "./dist/"
    "./target/"
    "./.mypy_cache/"
    "./.pytest_cache/"
    "./.ruff_cache/"
    # Add project-specific data/log/result paths here or in .contextignore
    "./bioinformatics/data/"
    "./bioinformatics/logs/"
    "./bioinformatics/results/"
    "./tls/"
    # --- ADDED FOR NEXT.JS FRONTEND ---
    "./frontend_app/.next/"         # Exclude Next.js build/cache directory
    "./frontend_app/node_modules/"  # Exclude nested node_modules
    # ---------------------------------
)

# Common binary file extensions (heuristic)
DEFAULT_BINARY_EXTENSIONS=(
    "*.png" "*.jpg" "*.jpeg" "*.gif" "*.bmp" "*.ico" "*.tif" "*.tiff"
    "*.pdf" "*.doc" "*.docx" "*.xls" "*.xlsx" "*.ppt" "*.pptx"
    "*.zip" "*.tar" "*.gz" "*.bz2" "*.rar" "*.7z" "*.xz" "*.tgz"
    "*.so" "*.dll" "*.o" "*.a" "*.lib" "*.dylib" "*.bundle"
    "*.pyc" "*.pyo" "*.pyd" # Python bytecode/extensions
    "*.class" "*.jar" # Java
    "*.exe" "*.msi" "*.deb" "*.rpm" # Executables/Installers
    "*.woff" "*.woff2" "*.ttf" "*.otf" "*.eot" # Fonts
    "*.mp3" "*.wav" "*.ogg" "*.mp4" "*.avi" "*.mov" "*.webm" # Media
    "*.db" "*.sqlite" "*.sqlite3" "*.db3" # Databases
    "*.swp" "*.swo" # Swap files
)

# Specific file names/patterns to exclude
DEFAULT_EXCLUDE_FILES=(
    # Placeholder for the actual output file name
    # "*.log" # General logs (consider adding if not covered by paths)
    "*.lock" # e.g., package-lock.json, poetry.lock, yarn.lock
    "docker/filebrowser.db" # Example specific file
)

# --- Option Flags ---
ENABLE_GREP_I_CHECK=true # Set to false to rely only on extension filtering

# --- Argument Parsing ---
PROJECT_DIR="${1:-$DEFAULT_PROJECT_DIR}"
OUTPUT_FILE_NAME="${2:-$DEFAULT_OUTPUT_FILE}"
MAX_SIZE_MB="${3:-$DEFAULT_MAX_SIZE_MB}"

# Convert MB to Bytes
MAX_SIZE_BYTES=$(( MAX_SIZE_MB * 1024 * 1024 ))

# --- Helper Functions ---
log_info() {
    echo "[INFO] $1"
}

log_warn() {
    echo "[WARN] $1" >&2
}

log_error() {
    echo "[ERROR] $1" >&2
    exit 1
}

# --- Main Execution ---

# 1. Validate and Navigate to Project Directory
if [ ! -d "$PROJECT_DIR" ]; then
    log_error "Project directory not found: $PROJECT_DIR"
fi
cd "$PROJECT_DIR" || log_error "Could not change directory to $PROJECT_DIR"
PROJECT_DIR_ABS="$(pwd)" # Get absolute path
OUTPUT_FILE="$PROJECT_DIR_ABS/$OUTPUT_FILE_NAME" # Use absolute path for output file
log_info "Scanning project: $PROJECT_DIR_ABS"

# Ensure the output file itself is excluded
DEFAULT_EXCLUDE_FILES+=("$OUTPUT_FILE_NAME") # Add relative name

# 2. Prepare Exclusion Lists (Combine Defaults and .contextignore)
EXCLUDE_PATHS=("${DEFAULT_EXCLUDE_PATHS[@]}")
EXCLUDE_FILES=("${DEFAULT_EXCLUDE_FILES[@]}")
BINARY_EXTENSIONS=("${DEFAULT_BINARY_EXTENSIONS[@]}")

CONTEXTIGNORE_PATH="$PROJECT_DIR_ABS/$DEFAULT_CONTEXTIGNORE"
if [ -f "$CONTEXTIGNORE_PATH" ]; then
    log_info "Reading exclusions from $CONTEXTIGNORE_PATH"
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Trim whitespace
        pattern=$(echo "$line" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
        # Skip empty lines and comments
        [[ -z "$pattern" || "$pattern" == \#* ]] && continue

        if [[ "$pattern" == */ ]]; then
            # Directory pattern (ends with /) -> add to path excludes
            # Ensure it starts with ./ for find -path matching
            pattern="./${pattern%/}/" # Ensure ./ prefix and trailing /
            # Avoid duplicate additions if already in defaults
            if ! printf '%s\n' "${EXCLUDE_PATHS[@]}" | grep -Fxq -- "$pattern"; then
                 EXCLUDE_PATHS+=("$pattern")
                 log_info "  Ignoring path from .contextignore: ${pattern}*"
            fi
        elif [[ "$pattern" == *"/"* ]]; then
             # Specific file path pattern -> add to file excludes
             pattern="./$pattern"
             if ! printf '%s\n' "${EXCLUDE_FILES[@]}" | grep -Fxq -- "$pattern"; then
                 EXCLUDE_FILES+=("$pattern")
                 log_info "  Ignoring file path from .contextignore: $pattern"
             fi
        else
            # General file pattern (glob) -> add to file excludes
             if ! printf '%s\n' "${EXCLUDE_FILES[@]}" | grep -Fxq -- "$pattern"; then
                EXCLUDE_FILES+=("$pattern")
                log_info "  Ignoring file pattern from .contextignore: $pattern"
            fi
        fi
    done < "$CONTEXTIGNORE_PATH"
else
    log_info "No $DEFAULT_CONTEXTIGNORE file found, using defaults."
fi

# --- Construct find command arguments ---

# Start building the command
final_find_cmd=(find . )

# A: Path exclusions (directories) - Build the prune part
path_prune_conditions=()
log_info "Applying path exclusions for pruning:"
for path in "${EXCLUDE_PATHS[@]}"; do
    # Ensure path starts with ./ and ends without / for matching
    path_pattern=$(echo "$path" | sed 's:/*$::' | sed 's:^\./::')
    if [[ -n "$path_pattern" ]]; then
        # Add conditions to match the directory itself OR its contents
        # Use -wholename which is often more robust than -path for this
        path_prune_conditions+=( -wholename "./$path_pattern" -o -wholename "./$path_pattern/*" -o )
        log_info "  - Pruning ./$path_pattern and ./$path_pattern/*"
    fi
done

# Add the pruning logic to the command IF there are conditions
if [ ${#path_prune_conditions[@]} -gt 0 ]; then
    unset 'path_prune_conditions[${#path_prune_conditions[@]}-1]' # Remove trailing -o
    # Add the parentheses, the conditions, -prune, and the crucial -o separator
    final_find_cmd+=( \( "${path_prune_conditions[@]}" \) -prune -o )
fi

# B: File name/pattern exclusions (build the -not (...) part)
file_exclude_conditions=()
log_info "Applying file exclusions:"
for pattern in "${EXCLUDE_FILES[@]}"; do
    if [[ "$pattern" == *"/"* ]]; then
        # Specific path relative to root: ./path/to/file.ext
        pattern="${pattern#./}" # Remove leading ./ if present
        if [[ -n "$pattern" ]]; then
            # Use -wholename for consistency
            file_exclude_conditions+=( -wholename "./$pattern" -o )
            log_info "  - Excluding file path: ./$pattern"
        fi
    else
        # Simple name pattern: *.log, specific_file.txt
         if [[ -n "$pattern" ]]; then
             file_exclude_conditions+=( -name "$pattern" -o )
             log_info "  - Excluding file name pattern: $pattern"
         fi
    fi
done

# Add the file exclusion logic IF there are conditions
if [ ${#file_exclude_conditions[@]} -gt 0 ]; then
    unset 'file_exclude_conditions[${#file_exclude_conditions[@]}-1]' # Remove trailing -o
    final_find_cmd+=( -not \( "${file_exclude_conditions[@]}" \) )
fi

# C: Binary extension exclusions (heuristic - build the -not (...) part)
binary_exclude_conditions=()
log_info "Applying binary extension exclusions:"
for ext_pattern in "${BINARY_EXTENSIONS[@]}"; do
    binary_exclude_conditions+=( -name "$ext_pattern" -o )
     log_info "  - Excluding binary extension: $ext_pattern"
done

# Add the binary exclusion logic IF there are conditions
if [ ${#binary_exclude_conditions[@]} -gt 0 ]; then
    unset 'binary_exclude_conditions[${#binary_exclude_conditions[@]}-1]' # Remove trailing -o
    final_find_cmd+=( -not \( "${binary_exclude_conditions[@]}" \) )
fi


# Add the main selection criteria and final action
final_find_cmd+=(
    -type f                 # Select only files
    -not -empty             # Exclude empty files
    -size "-${MAX_SIZE_BYTES}c" # Exclude files larger than MAX_SIZE_BYTES
    -print0                 # Print null-terminated names for safety
)


# --- DEBUG: Print the constructed find command ---
echo "---"
log_info "Constructed find command:"
# Use printf for safer printing of arguments, especially ones with spaces or special chars
printf "%q " "${final_find_cmd[@]}"
echo # Add a newline
echo "---"
# --- End Debug ---


log_info "Starting file scan..."
log_info "Max file size: $MAX_SIZE_MB MiB ($MAX_SIZE_BYTES bytes)"
if $ENABLE_GREP_I_CHECK; then
    log_info "Binary check: Enabled (grep -I)"
else
    log_info "Binary check: Disabled (relying on extensions)"
fi
echo "---" # Separator

# Clear the output file
> "$OUTPUT_FILE" || log_error "Could not clear/create output file: $OUTPUT_FILE"

# --- Process Files ---
file_count=0
total_bytes=0
errors=0

# Execute the constructed find command
# Use process substitution to avoid subshell issues with counters
while IFS= read -r -d $'\0' file; do
    # file path is relative like ./path/to/file.txt

    # Secondary Binary Check (if enabled)
    if $ENABLE_GREP_I_CHECK; then
        # Use LC_ALL=C for performance and to avoid locale issues
        # grep -I is generally robust. Redirect stderr to avoid clutter.
        if ! LC_ALL=C grep -Iq . "$file" 2>/dev/null; then
            # log_info "[Skipping Binary (grep -I)]: $file" # Optional debug
            continue
        fi
    fi

    # Get file size for reporting (already filtered by find)
    current_size=$(stat -c%s "$file" 2>/dev/null)
    if [ -z "$current_size" ]; then
        log_warn "Could not get size for $file. Skipping."
        ((errors++)) # Count as error? Maybe just warning.
        continue # Skip if size cannot be determined
    fi

    # Append to output file
    echo "--- START FILE: ${file} (Size: ${current_size} bytes) ---" >> "$OUTPUT_FILE"
    if LC_ALL=C cat -- "$file" >> "$OUTPUT_FILE"; then
        # Ensure a newline exists after the file content if cat was successful
        echo >> "$OUTPUT_FILE"
        ((file_count++))
        total_bytes=$(( total_bytes + current_size ))
    else
        # Add a newline even if cat fails, before the error message
        echo >> "$OUTPUT_FILE"
        echo "--- ERROR READING FILE: ${file} (Code: $?) ---" >> "$OUTPUT_FILE"
        ((errors++))
        log_warn "Error reading file: $file"
    fi
    echo "--- END FILE: ${file} ---" >> "$OUTPUT_FILE"
    # Add an extra blank line for readability between file blocks
    echo "" >> "$OUTPUT_FILE"

done < <("${final_find_cmd[@]}" 2>/dev/null) # Pass find options as separate arguments, redirect stderr

# --- Final Report ---
echo "---"
log_info "Scan complete."
log_info "Included $file_count files."
if [ $errors -gt 0 ]; then
    log_warn "$errors errors encountered while reading/stat'ing files."
fi
log_info "Output written to: $OUTPUT_FILE"
ls -lh "$OUTPUT_FILE"
final_size_bytes=$(stat -c%s "$OUTPUT_FILE" 2>/dev/null || echo 0)
log_info "Total context size: $final_size_bytes bytes"

# Optional: Go back to original directory if needed
# cd - > /dev/null

exit 0

--- END FILE: ./contextualize.sh ---

--- START FILE: ./requirements_webapp.txt (Size: 248 bytes) ---
# ./requirements_webapp.txt
fastapi
uvicorn[standard]
rq
redis
# REMOVED: Jinja2 - No longer needed for templating
python-multipart # Keep for now, might be needed if any endpoint expects form data
psutil # Keep for testing/consistency with worker

--- END FILE: ./requirements_webapp.txt ---

--- START FILE: ./docs/queue.txt (Size: 4787 bytes) ---
Background Task Processing (Pipeline Execution via RQ)

To handle potentially long-running bioinformatics pipelines without blocking the web server or timing out HTTP requests, this application uses a background task queue system based on Redis Queue (RQ).

Motivation

Previously, the pipeline script was executed directly within the FastAPI application's process using asyncio. This approach had limitations:

Scalability: Only one pipeline could run at a time per web server process.

Reliability: If the web server process crashed or restarted, the running pipeline would be terminated.

Timeouts: Long pipelines could exceed web server or load balancer timeouts.

Using RQ addresses these issues by decoupling the task submission from the task execution.

How it Works

Enqueue Job: When a user submits a pipeline run via the /run_pipeline endpoint in the web UI:

The FastAPI application (backend/app/app.py) validates the input files.

Instead of running the script directly, it enqueues a job onto a specific queue (pipeline_tasks) stored in a Redis database.

The job details include the task function to run (backend.app.tasks.run_pipeline_task) and the necessary arguments (paths to input files, script path).

FastAPI immediately returns a 202 Accepted response to the frontend, along with the unique job_id for the queued task.

Worker Execution:

One or more separate RQ worker processes constantly monitor the pipeline_tasks queue in Redis.

When a worker finds a job, it dequeues it.

The worker imports and executes the specified task function (run_pipeline_task from backend/app/tasks.py) with the provided arguments.

The run_pipeline_task function is responsible for:

Running the actual pipeline.sh script using subprocess.run.

Logging relevant information (job start, script output, errors) to the worker's console output.

Handling script success or failure (including timeouts).

Returning results (like the path to the output directory) or error information upon completion. RQ stores this result in Redis associated with the job_id.

Status Polling:

The frontend (frontend/static/run_pipeline.js), after receiving the job_id, starts polling the /job_status/{job_id} endpoint on the FastAPI server periodically (e.g., every 5 seconds).

The /job_status endpoint fetches the job's current status (queued, started, finished, failed, etc.), result, and error information directly from Redis using the job_id.

The frontend updates the UI accordingly, informing the user about the pipeline's progress and eventual outcome.

Polling stops once the job reaches a final state (finished or failed).

Key Components & Files

Redis: External dependency acting as the message broker. Must be running and accessible.

RQ Worker: A separate Python process launched via the rq worker command. You need to run at least one worker for jobs to be processed.

backend/app/tasks.py: Defines the run_pipeline_task function that contains the actual pipeline execution logic (calling pipeline.sh).

backend/app/app.py:

Connects to Redis and initializes the RQ Queue.

Modified /run_pipeline (POST) endpoint to enqueue jobs.

New /job_status/{job_id} (GET) endpoint for status polling.

frontend/static/run_pipeline.js: Modified to submit the job request and then poll the status endpoint using the returned job_id.

conda_env.yml: Includes rq and redis (the Python client library) dependencies.

backend/__init__.py & backend/app/__init__.py: Empty files necessary for Python's package import system to allow the worker to find backend.app.tasks.

Running Locally for Development

Start Redis: Ensure a Redis instance is running (e.g., using Docker: docker run -d -p 6379:6379 --name bio-redis redis:alpine).

Start RQ Worker(s): In a dedicated terminal:

# Navigate to project root
cd /path/to/bioinformatics-webapp
# Activate conda environment
mamba activate bio-webapp
# Set PYTHONPATH (crucial for worker imports)
export PYTHONPATH=$(pwd):$PYTHONPATH
# Start the worker listening on the 'pipeline_tasks' queue
rq worker pipeline_tasks --url redis://localhost:6379


Keep this terminal open.

Start FastAPI App: In another terminal:

# Navigate to project root
cd /path/to/bioinformatics-webapp
# Activate conda environment
mamba activate bio-webapp
# Run the app
python main.py
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Keep this terminal open.

Access Web UI: Open https://localhost:8000 in your browser and use the "Run Pipeline" page. Observe the UI status updates and the logs in the RQ worker terminal.

You can place this within your README.md under a new section or link to it from the README if you put it in a separate docs/ file. Remember to adjust paths or commands if your specific setup differs slightly.

--- END FILE: ./docs/queue.txt ---

--- START FILE: ./frontend_app/next-env.d.ts (Size: 211 bytes) ---
/// <reference types="next" />
/// <reference types="next/image-types/global" />

// NOTE: This file should not be edited
// see https://nextjs.org/docs/app/api-reference/config/typescript for more information.

--- END FILE: ./frontend_app/next-env.d.ts ---

--- START FILE: ./frontend_app/app/page.tsx (Size: 13148 bytes) ---
// File: frontend_app/app/page.tsx
"use client";

import React from 'react';
import { useQuery } from '@tanstack/react-query';
import Link from 'next/link';
import { FolderKanban, PlayCircle, BarChart3, HardDriveDownload, Hourglass, Activity, CheckCircle2, XCircle, LayoutDashboard, ListTree, FolderGit2 } from 'lucide-react';
import { Card, CardContent, CardDescription, CardFooter, CardHeader, CardTitle } from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import { Badge } from '@/components/ui/badge';
import * as api from '@/lib/api'; // Import API functions
import { Job, ResultRun } from '@/lib/types'; // Import types
import LoadingSpinner from '@/components/common/LoadingSpinner';
import ErrorDisplay from '@/components/common/ErrorDisplay';
import { formatDistanceToNow } from 'date-fns'; // For relative time formatting
import { Skeleton } from '@/components/ui/skeleton'; // For loading state
import { useFileBrowser } from '@/components/layout/FileBrowserContext';

// Helper function for relative time (copied from JobTable for consistency)
// TODO: Move to lib/utils.ts
function formatTimestampRelative(timestamp: number | null | undefined): string {
  if (!timestamp) return "N/A";
  try {
    // Add a check for potentially invalid timestamps (e.g., 0 or negative)
    if (timestamp <= 0) return "N/A";
    return formatDistanceToNow(new Date(timestamp * 1000), { addSuffix: true });
  } catch (e) {
    console.error("Error formatting timestamp:", timestamp, e);
    return "Invalid Date";
  }
}
// Helper for status badge variants (copied from JobTable for consistency)
// TODO: Move to lib/utils.ts
function getStatusVariant(status: string): "default" | "destructive" | "secondary" | "outline" {
     switch (status?.toLowerCase()) {
        case 'finished': return 'default'; // Greenish success -> Use primary for now
        case 'failed': return 'destructive'; // Red
        case 'running':
        case 'started': return 'default'; // Blueish -> Primary
        case 'queued':
        case 'staged': return 'secondary'; // Gray
        case 'stopped':
        case 'canceled': return 'outline'; // Muted/outline
        default: return 'secondary';
    }
}


export default function HomePage() {
  const MAX_RECENT_ITEMS = 5;
  const { openFileBrowser } = useFileBrowser();

  // Fetch Jobs List
  const { data: jobs, isLoading: isLoadingJobs, isError: isErrorJobs, error: errorJobs } = useQuery<Job[], Error>({
    queryKey: ['jobsList'],
    queryFn: api.getJobsList,
    staleTime: 60 * 1000, // Cache for 1 minute
    refetchOnWindowFocus: false,
  });

  // Fetch Results List
  const { data: results, isLoading: isLoadingResults, isError: isErrorResults, error: errorResults } = useQuery<ResultRun[], Error>({
    queryKey: ['resultsList'],
    queryFn: api.getResultsList,
    staleTime: 5 * 60 * 1000, // Cache for 5 minutes
    refetchOnWindowFocus: false,
  });

  // Get recent items (slice only after data is available)
  const recentJobs = jobs?.slice(0, MAX_RECENT_ITEMS) ?? [];
  const recentResults = results?.slice(0, MAX_RECENT_ITEMS) ?? [];

  // --- Job Stats Calculation ---
  const jobStats = React.useMemo(() => {
    if (!jobs || isLoadingJobs) return { running: 0, queued: 0, completed: 0, failed: 0, staged: 0, total: 0 }; // Include staged and total
    return jobs.reduce((acc, job) => {
        const status = job.status?.toLowerCase();
        if (status === 'running' || status === 'started') acc.running++;
        else if (status === 'queued') acc.queued++;
        else if (status === 'finished') acc.completed++;
        else if (status === 'failed' || status === 'stopped') acc.failed++; // Combine failed and stopped
        else if (status === 'staged') acc.staged++; // Count staged jobs
        acc.total++; // Increment total for every job
        return acc;
    }, { running: 0, queued: 0, completed: 0, failed: 0, staged: 0, total: 0 });
  }, [jobs, isLoadingJobs]); // Depend on isLoadingJobs too


  return (
    <div className="space-y-8">
       {/* Welcome Banner */}
      <Card className="bg-gradient-to-r from-primary/10 to-secondary/10 border-primary/20">
        <CardHeader>
          <CardTitle className="text-3xl font-bold text-primary">Sarek Pipeline Dashboard</CardTitle>
          <CardDescription className="text-lg">
            Stage, run, and manage your nf-core/sarek bioinformatics analysis pipelines.
          </CardDescription>
        </CardHeader>
        <CardContent>
          <p>Use this interface to process sequencing data, monitor job progress, and browse results efficiently. Start by staging a new run or view existing jobs and results.</p>
        </CardContent>
         {/* *** MODIFIED: Changed justify-end to justify-start *** */}
         <CardFooter className="justify-start">
            <Button asChild size="lg">
              <Link href="/input"><PlayCircle className="mr-2 h-5 w-5" /> Stage New Sarek Run</Link>
            </Button>
        </CardFooter>
      </Card>

       {/* Job Stats */}
        <div className="grid gap-4 md:grid-cols-3 lg:grid-cols-5">
             <Card>
                <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
                    <CardTitle className="text-sm font-medium">Total Jobs</CardTitle>
                    <ListTree className="h-4 w-4 text-muted-foreground" />
                </CardHeader>
                <CardContent>
                    <div className="text-2xl font-bold">{isLoadingJobs ? <Skeleton className="h-7 w-12" /> : jobStats.total}</div>
                </CardContent>
            </Card>
            <Card>
                <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
                    <CardTitle className="text-sm font-medium">Staged</CardTitle>
                    <Hourglass className="h-4 w-4 text-muted-foreground" />
                </CardHeader>
                <CardContent>
                     <div className="text-2xl font-bold">{isLoadingJobs ? <Skeleton className="h-7 w-12" /> : jobStats.staged}</div>
                </CardContent>
            </Card>
            <Card>
                <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
                    <CardTitle className="text-sm font-medium">Queued/Running</CardTitle>
                    <Activity className="h-4 w-4 text-muted-foreground" />
                </CardHeader>
                <CardContent>
                    <div className="text-2xl font-bold">{isLoadingJobs ? <Skeleton className="h-7 w-12" /> : jobStats.queued + jobStats.running}</div>
                </CardContent>
            </Card>
             <Card>
                <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
                    <CardTitle className="text-sm font-medium">Completed</CardTitle>
                    <CheckCircle2 className="h-4 w-4 text-green-500" />
                </CardHeader>
                <CardContent>
                    <div className="text-2xl font-bold">{isLoadingJobs ? <Skeleton className="h-7 w-12" /> : jobStats.completed}</div>
                </CardContent>
            </Card>
             <Card>
                <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
                    <CardTitle className="text-sm font-medium">Failed/Stopped</CardTitle>
                    <XCircle className="h-4 w-4 text-destructive" />
                </CardHeader>
                <CardContent>
                    <div className="text-2xl font-bold">{isLoadingJobs ? <Skeleton className="h-7 w-12" /> : jobStats.failed}</div>
                </CardContent>
            </Card>
        </div>


      {/* Dashboard Grid: Quick Actions / Recent Activity */}
      <div className="grid grid-cols-1 lg:grid-cols-3 gap-6">
        {/* Quick Actions */}
        <Card className="lg:col-span-1">
          <CardHeader>
            <CardTitle>Quick Actions</CardTitle>
            <CardDescription>Navigate to key sections</CardDescription>
          </CardHeader>
          <CardContent className="flex flex-col space-y-2">
            <Button asChild variant="outline">
              <Link href="/input"><PlayCircle className="mr-2 h-4 w-4" /> Stage New Run</Link>
            </Button>
            <Button asChild variant="outline">
              <Link href="/jobs"><LayoutDashboard className="mr-2 h-4 w-4" /> View Jobs Dashboard</Link>
            </Button>
            <Button asChild variant="outline">
              <Link href="/results"><FolderGit2 className="mr-2 h-4 w-4" /> Browse Results</Link>
            </Button>
             <Button variant="outline" className="cursor-pointer" onClick={() => openFileBrowser('/filebrowser/files')}>
               <FolderKanban className="mr-2 h-4 w-4"/> Manage Data Files
             </Button>
          </CardContent>
        </Card>

        {/* Recent Activity */}
        <div className="lg:col-span-2 grid grid-cols-1 md:grid-cols-2 gap-6">
          {/* Recent Jobs */}
          <Card>
            <CardHeader>
              <CardTitle>Recent Jobs</CardTitle>
              <CardDescription>Last {MAX_RECENT_ITEMS} updated jobs</CardDescription>
            </CardHeader>
            <CardContent>
              {isLoadingJobs && <LoadingSpinner label="Loading jobs..." />}
              {isErrorJobs && <ErrorDisplay error={errorJobs} title="Could not load jobs" />}
              {!isLoadingJobs && !isErrorJobs && recentJobs.length === 0 && <p className="text-sm text-muted-foreground">No recent jobs found.</p>}
              {!isLoadingJobs && !isErrorJobs && recentJobs.length > 0 && (
                <ul className="space-y-3">
                  {recentJobs.map((job) => (
                    <li key={job.id} className="flex items-center justify-between text-sm border-b pb-2 last:border-0 last:pb-0">
                      <Link href="/jobs" className="hover:underline truncate mr-2 group flex-grow min-w-0">
                        <span className="font-medium block truncate group-hover:text-primary" title={job.id}>
                             {job.id.startsWith("staged_") ? "STG_" : "RQ_"}
                             {job.id.substring(job.id.indexOf('_') + 1, job.id.indexOf('_') + 9)}...
                             </span>
                        <span className="text-xs text-muted-foreground block truncate">{job.description || "No description"}</span>
                      </Link>
                       <div className="text-right flex-shrink-0 ml-2">
                            <Badge variant={getStatusVariant(job.status)} className="capitalize mb-1 text-xs px-1.5 py-0.5">{job.status}</Badge>
                            <p className="text-xs text-muted-foreground">{formatTimestampRelative(job.ended_at || job.started_at || job.enqueued_at || job.staged_at)}</p>
                       </div>
                    </li>
                  ))}
                </ul>
              )}
            </CardContent>
             <CardFooter>
                <Button variant="link" size="sm" className="mx-auto text-muted-foreground hover:text-primary" asChild>
                    <Link href="/jobs">View All Jobs</Link>
                </Button>
             </CardFooter>
          </Card>

          {/* Recent Results */}
          <Card>
            <CardHeader>
              <CardTitle>Recent Results</CardTitle>
               <CardDescription>Last {MAX_RECENT_ITEMS} completed runs</CardDescription>
            </CardHeader>
            <CardContent>
              {isLoadingResults && <LoadingSpinner label="Loading results..." />}
              {isErrorResults && <ErrorDisplay error={errorResults} title="Could not load results" />}
              {!isLoadingResults && !isErrorResults && recentResults.length === 0 && <p className="text-sm text-muted-foreground">No recent results found.</p>}
              {!isLoadingResults && !isErrorResults && recentResults.length > 0 && (
                 <ul className="space-y-3">
                  {recentResults.map((run) => (
                    <li key={run.name} className="flex items-center justify-between text-sm border-b pb-2 last:border-0 last:pb-0">
                      <Link href={`/results?highlight=${encodeURIComponent(run.name)}`} className="hover:underline truncate mr-2 group flex-grow min-w-0">
                        <span className="font-medium block truncate group-hover:text-primary" title={run.name}>{run.name}</span>
                      </Link>
                      <span className="text-xs text-muted-foreground flex-shrink-0 ml-2">{formatTimestampRelative(run.modified_time)}</span>
                    </li>
                  ))}
                </ul>
              )}
            </CardContent>
             <CardFooter>
                <Button variant="link" size="sm" className="mx-auto text-muted-foreground hover:text-primary" asChild>
                    <Link href="/results">View All Results</Link>
                </Button>
             </CardFooter>
          </Card>
        </div>
      </div>
    </div>
  );
}

--- END FILE: ./frontend_app/app/page.tsx ---

--- START FILE: ./frontend_app/app/(pages)/jobs/page.tsx (Size: 7728 bytes) ---
// File: frontend_app/app/(pages)/jobs/page.tsx
"use client";

import React, { useState, useMemo } from "react";
import { useJobsList } from "@/lib/hooks/useJobsList";
import JobTable from "@/components/jobs/JobTable";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input"; // For search/filter
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select"; // For status filter
import { RefreshCw, Search, ArrowDownUp, ListFilter, Terminal, Loader2 } from "lucide-react"; // Import icons, added Terminal, Loader2
import { Alert, AlertDescription, AlertTitle } from "@/components/ui/alert";
import ErrorDisplay from '@/components/common/ErrorDisplay'; // Import ErrorDisplay (already present)
import { Job } from "@/lib/types"; // Import Job type

const JOB_STATUSES = ['all', 'staged', 'queued', 'running', 'finished', 'failed', 'stopped'];
type SortKey = 'updated_at' | 'status'; // Add more if needed
type SortOrder = 'asc' | 'desc';

export default function JobsPage() {
  const [searchTerm, setSearchTerm] = useState("");
  const [statusFilter, setStatusFilter] = useState<string>("all");
  const [sortKey, setSortKey] = useState<SortKey>('updated_at'); // Default sort
  const [sortOrder, setSortOrder] = useState<SortOrder>("desc"); // Default sort order

  // Fetch jobs list, enable polling every 5 seconds
  const { data: jobs, isLoading, isError, error, refetch, isFetching } = useJobsList({
    refetchInterval: 5000, // Poll every 5000ms (5 seconds)
  });

  // --- ADD LOGGING HERE ---
  console.log("JobsPage - Raw data from useJobsList:", jobs);
  // ------------------------

  const handleRefresh = () => {
    refetch(); // Manually trigger a refetch
  };

   // --- Client-side Filtering and Sorting (Still calculated, but not used for rendering JobTable in this version) ---
   const filteredAndSortedJobs = useMemo(() => {
    let filtered = jobs ?? []; // Start with fetched jobs or empty array

    // --- ADD LOGGING ---
    console.log("JobsPage - Before filtering/sorting:", filtered);
    // -----------------

    // Filter by search term (case-insensitive on description or ID)
    if (searchTerm) {
        const lowerSearchTerm = searchTerm.toLowerCase();
        filtered = filtered.filter((job) =>
            job.description?.toLowerCase().includes(lowerSearchTerm) ||
            job.id.toLowerCase().includes(lowerSearchTerm)
        );
    }

    // Filter by status
    if (statusFilter !== 'all') {
        const effectiveStatus = statusFilter === 'running' ? ['running', 'started'] : [statusFilter];
        filtered = filtered.filter((job) => effectiveStatus.includes(job.status?.toLowerCase()));
    }

    // Sort
    filtered.sort((a, b) => {
        let compareA: any = null;
        let compareB: any = null;

        if (sortKey === 'updated_at') {
            compareA = a.ended_at || a.started_at || a.enqueued_at || a.staged_at || 0;
            compareB = b.ended_at || b.started_at || b.enqueued_at || b.staged_at || 0;
        } else if (sortKey === 'status') {
            compareA = a.status || '';
            compareB = b.status || '';
        }

        const comparison = (compareA < compareB) ? -1 : (compareA > compareB) ? 1 : 0;
        return sortOrder === 'asc' ? comparison : -comparison; // Apply order
    });

    // --- ADD LOGGING ---
    console.log("JobsPage - After filtering/sorting (calculated but maybe not used):", filtered);
    // -----------------

    return filtered;
  }, [jobs, searchTerm, statusFilter, sortKey, sortOrder]);

  // Toggle sort order for the current key
  const handleSort = (key: SortKey) => {
      if (key === sortKey) {
          setSortOrder(prev => prev === 'asc' ? 'desc' : 'asc');
      } else {
          setSortKey(key);
          setSortOrder('desc'); // Default to descending for new key
      }
  };


  return (
    <div className="space-y-6">
      {/* Header */}
      <div className="flex flex-col sm:flex-row justify-between items-center gap-4">
        <h1 className="text-3xl font-bold ml-2">Jobs Dashboard</h1>
        <Button
            variant="outline"
            size="sm"
            onClick={handleRefresh}
            disabled={isFetching}
            className="cursor-pointer" // Keep cursor
        >
          <RefreshCw className={`mr-2 h-4 w-4 ${isFetching ? 'animate-spin' : ''}`} />
          {isFetching ? 'Refreshing...' : 'Refresh'}
        </Button>
      </div>

      {/* Filter and Sort Controls */}
      <div className="flex flex-col sm:flex-row gap-4">
         {/* Search */}
        <div className="relative flex-grow">
          <Search className="absolute left-2.5 top-2.5 h-4 w-4 text-muted-foreground" />
          <Input
            type="search"
            placeholder="Search description or ID..."
            value={searchTerm}
            onChange={(e) => setSearchTerm(e.target.value)}
            className="pl-8 w-full"
            disabled={isLoading} // Disable controls while initial loading
          />
        </div>
         {/* Status Filter */}
        <div className="flex items-center gap-2">
            <ListFilter className="h-4 w-4 text-muted-foreground" />
            <Select value={statusFilter} onValueChange={setStatusFilter} disabled={isLoading}>
                <SelectTrigger className="w-[180px]">
                    <SelectValue placeholder="Filter by status" />
                </SelectTrigger>
                <SelectContent>
                    {JOB_STATUSES.map(status => (
                         <SelectItem key={status} value={status} className="capitalize">{status}</SelectItem>
                    ))}
                </SelectContent>
            </Select>
        </div>
         {/* Sort Button */}
        <Button
            variant="outline"
            onClick={() => handleSort('updated_at')}
            disabled={isLoading}
            className="cursor-pointer" // Keep cursor
        >
            <ArrowDownUp className="mr-2 h-4 w-4" />
            Sort by Date ({sortOrder === 'asc' ? 'Oldest' : 'Newest'} First)
        </Button>
      </div>


      {/* Loading State */}
      {isLoading && (
        <div className="flex flex-col items-center justify-center text-center py-10 gap-2">
            <Loader2 className="h-12 w-12 animate-spin text-primary" />
            <p className="text-muted-foreground">Loading jobs...</p>
        </div>
      )}


      {/* Display Error State */}
      {isError && !isLoading && (
        <Alert variant="destructive" className="mt-4">
          <Terminal className="h-4 w-4" />
          <AlertTitle>Error Loading Jobs</AlertTitle>
          <AlertDescription>
            {error instanceof Error ? error.message : "An unknown error occurred."}
             <Button variant="link" size="sm" onClick={() => refetch()} className="ml-2 p-0 h-auto">Retry</Button>
          </AlertDescription>
        </Alert>
      )}

      {/* Display Job Table when data is loaded successfully */}
      {!isLoading && !isError && jobs && (
           <JobTable jobs={filteredAndSortedJobs} />
      )}

       {/* Display message if filters result in no jobs */}
       {!isLoading && !isError && jobs && filteredAndSortedJobs.length === 0 && (searchTerm || statusFilter !== 'all') && (
           <p className="text-center text-muted-foreground py-8">No jobs match your current filters.</p>
       )}
       {/* Display message if the original list was empty */}
       {!isLoading && !isError && jobs && jobs.length === 0 && !(searchTerm || statusFilter !== 'all') && (
           <p className="text-center text-muted-foreground py-8">No jobs available yet.</p>
       )}

    </div>
  );
}

--- END FILE: ./frontend_app/app/(pages)/jobs/page.tsx ---

--- START FILE: ./frontend_app/app/(pages)/results/page.tsx (Size: 5864 bytes) ---
// File: frontend_app/app/(pages)/results/page.tsx
"use client";

// Import Suspense from React
import React, { useState, useEffect, Suspense, useMemo } from "react";
import { useQuery } from "@tanstack/react-query";
import { useSearchParams } from 'next/navigation';
import { Input } from "@/components/ui/input";
import { Button } from "@/components/ui/button";
import { Search, ArrowDownUp } from "lucide-react";

import * as api from "@/lib/api";
import { ResultRun } from "@/lib/types";
import LoadingSpinner from "@/components/common/LoadingSpinner";
import ErrorDisplay from "@/components/common/ErrorDisplay";
import RunItem from "@/components/results/RunItem";

// Keep this for now, although Suspense might make it redundant for this specific error
export const dynamic = 'force-dynamic';

// --- NEW Client Component to handle search params ---
function ResultsList({ runs, searchTerm, sortOrder }: {
    runs: ResultRun[] | undefined;
    searchTerm: string;
    sortOrder: "asc" | "desc";
}) {
    // This component uses the hook and can be suspended
    const searchParams = useSearchParams();
    const highlightedRunName = searchParams.get('highlight');

    const [expandedRun, setExpandedRun] = useState<string | null>(null);

    // Effect to expand highlighted run (runs on client)
    useEffect(() => {
        if (highlightedRunName) {
            setExpandedRun(highlightedRunName);
            setTimeout(() => {
                const element = document.getElementById(`run-${highlightedRunName}`);
                element?.scrollIntoView({ behavior: 'smooth', block: 'center' });
            }, 100);
        } else {
            setExpandedRun(null);
        }
    }, [highlightedRunName]);

    // Filter/Sort logic moved here
    const filteredAndSortedRuns = useMemo(() => {
        let filtered = runs ?? [];
        if (searchTerm) {
            filtered = filtered.filter((run) =>
                run.name.toLowerCase().includes(searchTerm.toLowerCase())
            );
        }
        filtered.sort((a, b) => {
            const timeA = a.modified_time ?? 0;
            const timeB = b.modified_time ?? 0;
            return sortOrder === "desc" ? timeB - timeA : timeA - timeB;
        });
        return filtered;
    }, [runs, searchTerm, sortOrder]);

    const handleExpandToggle = (runName: string, isOpening: boolean) => {
        setExpandedRun(isOpening ? runName : null);
    };

    // Render the actual list
    return (
        <>
            {filteredAndSortedRuns.length === 0 ? (
                <p className="text-center text-muted-foreground py-10">
                    {searchTerm ? "No matching results found." : "No pipeline results available yet."}
                </p>
            ) : (
                <div className="space-y-4">
                    {filteredAndSortedRuns.map((run) => (
                        <div key={run.name} id={`run-${run.name}`}>
                            <RunItem
                                run={run}
                                isHighlighted={run.name === highlightedRunName}
                                isExpanded={expandedRun === run.name}
                                onExpandToggle={handleExpandToggle}
                            />
                        </div>
                    ))}
                </div>
            )}
        </>
    );
}
// --- END NEW Client Component ---


// --- Main Page Component ---
export default function ResultsPage() {
    // State managed here, passed down to ResultsList
    const [searchTerm, setSearchTerm] = useState("");
    const [sortOrder, setSortOrder] = useState<"asc" | "desc">("desc");

    // Data fetching remains here
    const { data: runs, isLoading, isError, error } = useQuery<ResultRun[], Error>({
        queryKey: ["resultsList"],
        queryFn: api.getResultsList,
    });

    return (
        <div className="space-y-6">
            <h1 className="text-3xl font-bold ml-2">Pipeline Results</h1>

            {/* Controls: Search and Sort */}
            <div className="flex flex-col sm:flex-row gap-4">
                <div className="relative flex-grow">
                    <Search className="absolute left-2.5 top-2.5 h-4 w-4 text-muted-foreground" />
                    <Input
                        type="search"
                        placeholder="Search run names..."
                        value={searchTerm}
                        onChange={(e) => setSearchTerm(e.target.value)}
                        className="pl-8 w-full"
                        disabled={isLoading}
                    />
                </div>
                <div className="flex gap-2">
                    <Button
                        variant="outline"
                        onClick={() => setSortOrder(sortOrder === "asc" ? "desc" : "asc")}
                        className="cursor-pointer"
                        disabled={isLoading}
                    >
                        <ArrowDownUp className="mr-2 h-4 w-4" />
                        Sort by Date ({sortOrder === "asc" ? "Oldest" : "Newest"} First)
                    </Button>
                </div>
            </div>

            {/* Loading State */}
            {isLoading && <div className="text-center py-10"><LoadingSpinner label="Loading results..." size="lg" /></div>}

            {/* Error State */}
            {isError && <ErrorDisplay error={error} title="Failed to load results" />}

            {/* Results List Area - Wrap the component using useSearchParams in Suspense */}
            {!isLoading && !isError && (
                <Suspense fallback={<div className="text-center py-10"><LoadingSpinner label="Loading results view..." /></div>}>
                    <ResultsList runs={runs} searchTerm={searchTerm} sortOrder={sortOrder} />
                </Suspense>
            )}
        </div>
    );
}

--- END FILE: ./frontend_app/app/(pages)/results/page.tsx ---

--- START FILE: ./frontend_app/app/(pages)/input/page.tsx (Size: 25531 bytes) ---
// File: frontend_app/app/(pages)/input/page.tsx
"use client";

import React from "react";
import { useForm, useFieldArray, FormProvider } from "react-hook-form";
import { zodResolver } from "@hookform/resolvers/zod";
import { z, ZodType } from "zod";
import { useMutation, useQueryClient } from "@tanstack/react-query";
import { useRouter } from "next/navigation";
import { PlusCircle, Loader2, Play } from "lucide-react";
import { toast } from "sonner";

import { Button } from "@/components/ui/button";
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@/components/ui/card";
import { Form, FormControl, FormDescription, FormField, FormItem, FormLabel, FormMessage } from "@/components/ui/form";
import { Input } from "@/components/ui/input";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select";
import { Checkbox } from "@/components/ui/checkbox";
import SampleInputGroup from "@/components/forms/SampleInputGroup"; // Keep this component as is (it has lane)
import FileSelector from "@/components/forms/FileSelector";
import * as api from "@/lib/api";
// Import SampleInfo for the API payload, PipelineInput is now the correct type for the payload
import { PipelineInput, SampleInfo as ApiSampleInfo } from "@/lib/types";
import { cn } from "@/lib/utils";
import { Control } from "react-hook-form";

// --- Define Zod Schema for Validation ---

const noSpacesRegex = /^[^\s]+$/;

// *** RE-ADDED lane field ***
const sampleSchema = z.object({
  patient: z.string()
            .min(1, "Patient ID is required")
            .regex(noSpacesRegex, "Patient ID cannot contain spaces"),
  sample: z.string()
           .min(1, "Sample ID is required")
           .regex(noSpacesRegex, "Sample ID cannot contain spaces"),
  sex: z.enum(["XX", "XY", "X", "Y", "other"], { required_error: "Sex is required" }),
  status: z.union([z.literal(0), z.literal(1)], { required_error: "Status is required" }),
  // RE-ADD: lane field validation
  lane: z.string()
         .min(1, "Lane is required")
         .regex(/^L\d{3}$/, "Lane must be in format 'L001', 'L002', etc."),
  // ***************************
  fastq_1: z.string().min(1, "FASTQ R1 is required"),
  fastq_2: z.string().min(1, "FASTQ R2 is required"),
});
// *************************

// Tools that typically require a tumor sample
const SOMATIC_TOOLS = ["mutect2", "strelka"]; // Add others if needed

const SAREK_TOOLS = ["strelka", "mutect2", "freebayes", "mpileup", "vardict", "manta", "cnvkit"];
const SAREK_STEPS = ["mapping", "markduplicates", "prepare_recalibration", "recalibrate", "variant_calling", "annotation"];
const SAREK_PROFILES = ["docker", "singularity", "conda", "podman"];
const SAREK_GENOMES = [
    { value: "GATK.GRCh38", label: "GRCh38 (GATK Bundle)" },
    { value: "GATK.GRCh37", label: "GRCh37 (GATK Bundle)" },
    { value: "hg38", label: "hg38 (UCSC)" },
    { value: "hg19", label: "hg19 (UCSC)" },
];
const VALID_GENOME_VALUES = SAREK_GENOMES.map(g => g.value) as [string, ...string[]];
const SAREK_ALIGNERS = ["bwa-mem", "dragmap"];

const pipelineInputSchema = z.object({
  samples: z.array(sampleSchema).min(1, "At least one sample is required"),
  genome: z.enum(VALID_GENOME_VALUES, {
     required_error: "Genome build is required",
     invalid_type_error: "Invalid genome build selected",
  }),
  // Add suffix validation check for intervals file
  intervals_file: z.string().optional().refine(
      (val) => !val || val.endsWith('.bed') || val.endsWith('.list') || val.endsWith('.interval_list'),
      { message: "Intervals file must end with .bed, .list, or .interval_list" }
  ),
  dbsnp: z.string().optional(),
  known_indels: z.string().optional(),
  pon: z.string().optional(),
  tools: z.array(z.string()).default([]),
  step: z.enum(SAREK_STEPS as [string, ...string[]]).default("mapping"),
  profile: z.enum(SAREK_PROFILES as [string, ...string[]]).default("docker"),
  aligner: z.enum(SAREK_ALIGNERS as [string, ...string[]]).default("bwa-mem"),
  joint_germline: z.boolean().default(false),
  wes: z.boolean().default(false),
  trim_fastq: z.boolean().default(false),
  skip_qc: z.boolean().default(false),
  skip_annotation: z.boolean().default(false),
  skip_baserecalibrator: z.boolean().default(false),
  description: z.string().optional(),
})
.refine(data => {
    const selectedSomaticTools = data.tools.filter(tool => SOMATIC_TOOLS.includes(tool));
    if (selectedSomaticTools.length === 0) {
        return true; // No somatic tools selected, no need for tumor sample
    }
    // Check if at least one sample has status 1 (Tumor)
    const hasTumorSample = data.samples.some(sample => sample.status === 1);
    return hasTumorSample;
}, {
    message: `Tools like ${SOMATIC_TOOLS.join(', ')} require at least one sample with Status = 1 (Tumor).`,
    path: ["tools"], // Attach error message to the 'tools' field group
});
// ---------------------------------------------------------------

type PipelineFormValues = z.infer<typeof pipelineInputSchema>;


export default function InputPage() {
  const router = useRouter();
  const queryClient = useQueryClient();

  const form = useForm<PipelineFormValues>({
    resolver: zodResolver(pipelineInputSchema as ZodType<PipelineFormValues>),
    mode: "onBlur",
    reValidateMode: "onChange",
    defaultValues: {
      // *** RE-ADDED lane to default sample ***
      samples: [{ patient: "", sample: "", sex: undefined, status: undefined, lane: "", fastq_1: "", fastq_2: "" }],
      // ***************************************
      genome: "GATK.GRCh38",
      intervals_file: undefined,
      dbsnp: undefined,
      known_indels: undefined,
      pon: undefined,
      tools: [],
      step: "mapping",
      profile: "docker",
      aligner: "bwa-mem",
      joint_germline: false,
      wes: false,
      trim_fastq: false,
      skip_qc: false,
      skip_annotation: false,
      skip_baserecalibrator: false,
      description: "",
    },
  });

  const { fields, append, remove } = useFieldArray({
    control: form.control,
    name: "samples",
  });

  const stageMutation = useMutation({
     mutationFn: (values: PipelineInput) => api.stagePipelineJob(values),
     onSuccess: (data) => {
        toast.success(`Job staged successfully: ${data.staged_job_id}`);
        queryClient.invalidateQueries({ queryKey: ['jobsList'] });
        form.reset();
        router.push('/jobs');
     },
     onError: (error) => {
        let message = `Failed to stage job: ${error.message}`;
        // @ts-ignore - Check if originalError exists and has response data detail
        const detail = error.originalError?.response?.data?.detail;
        if (detail) {
          // Use detailed message from backend if available
          message = `Failed to stage job: ${typeof detail === 'string' ? detail : JSON.stringify(detail)}`;
        } else {
          // Fallback to generic error message
          message = `Failed to stage job: ${error.message}`;
        }
        toast.error(message, { duration: 10000 });
     }
  });

  function onSubmit(values: PipelineFormValues) {
     console.log("Form Values Submitted:", values);
      // Map form values to the API payload type (PipelineInput)
      // Use ApiSampleInfo for the inner sample mapping to match backend expectations
      const apiPayload: PipelineInput = {
          samples: values.samples.map((s): ApiSampleInfo => ({ // Use ApiSampleInfo type here
                patient: s.patient,
                sample: s.sample,
                sex: s.sex,
                status: s.status,
                // *** RE-ADDED lane to payload ***
                lane: s.lane,
                // *******************************
                fastq_1: s.fastq_1,
                fastq_2: s.fastq_2,
          })),
          genome: values.genome,
          intervals_file: values.intervals_file || undefined,
          dbsnp: values.dbsnp || undefined,
          known_indels: values.known_indels || undefined,
          pon: values.pon || undefined,
          // Send tools as string[], API endpoint expects string[]
          tools: values.tools && values.tools.length > 0 ? values.tools : undefined,
          step: values.step,
          profile: values.profile,
          aligner: values.aligner,
          joint_germline: values.joint_germline,
          wes: values.wes,
          trim_fastq: values.trim_fastq,
          skip_qc: values.skip_qc,
          skip_annotation: values.skip_annotation,
          skip_baserecalibrator: values.skip_baserecalibrator,
          description: values.description || undefined,
      };
     console.log("API Payload to be sent:", apiPayload); // Log the final payload
     stageMutation.mutate(apiPayload);
  }

   const toggleCheckboxValue = (fieldName: keyof PipelineFormValues | 'tools', tool?: string) => {
        if (fieldName === 'tools' && tool) {
            const currentVal = form.getValues("tools") ?? [];
            const newVal = currentVal.includes(tool)
                ? currentVal.filter((t) => t !== tool)
                : [...currentVal, tool];
            form.setValue("tools", newVal, { shouldValidate: true, shouldDirty: true });
        } else if (fieldName !== 'tools') {
            if (fieldName in form.getValues()) {
                 const currentVal = form.getValues(fieldName as keyof PipelineFormValues);
                 form.setValue(fieldName as keyof PipelineFormValues, !currentVal, { shouldValidate: true, shouldDirty: true });
            } else {
                console.warn(`Attempted to toggle non-existent field: ${fieldName}`);
            }
        }
    };

  return (
    <FormProvider {...form}>
      <Form {...form}>
        <form onSubmit={form.handleSubmit(onSubmit)} className="space-y-8">
          <h1 className="text-3xl font-bold mb-6 ml-2">Stage New Sarek Run</h1>
          {/* Samples Section */}
          <Card>
            <CardHeader>
              <CardTitle className="text-primary">Sample Information</CardTitle>
              <CardDescription>Define the samples to be processed in this run. Status 0 = Normal, 1 = Tumor. IDs cannot contain spaces.</CardDescription>
            </CardHeader>
            <CardContent className="space-y-4">
              {fields.map((field, index) => (
                // SampleInputGroup component should already have the lane field UI
                <SampleInputGroup key={field.id} index={index} remove={remove} control={form.control} />
              ))}
               <Button
                type="button"
                variant="outline"
                size="sm"
                onClick={() => append({
                    // *** RE-ADDED lane to append ***
                    patient: "",
                    sample: "",
                    sex: "XX", // Provide a default valid enum value
                    status: 0, // Provide a default valid enum value
                    lane: "", // Re-added
                    fastq_1: "",
                    fastq_2: ""
                    // ******************************
                })}
                className="mt-2 cursor-pointer"
              >
                <PlusCircle className="mr-2 h-4 w-4" />
                Add Sample
              </Button>
               <FormMessage>{form.formState.errors.samples?.message || form.formState.errors.samples?.root?.message}</FormMessage>
            </CardContent>
          </Card>

          {/* Reference & Annotation Files Section */}
          <Card>
            <CardHeader>
              <CardTitle className="text-primary">Reference & Annotation Files</CardTitle>
              <CardDescription>Select the reference genome build and optional annotation files.</CardDescription>
            </CardHeader>
            <CardContent className="space-y-4">
              <FormField control={form.control} name="genome" render={({ field }) => (
                <FormItem>
                    <FormLabel className="cursor-default">Reference Genome Build <span className="text-destructive">*</span></FormLabel>
                    <Select onValueChange={field.onChange} defaultValue={field.value} value={field.value}>
                        <FormControl>
                            <SelectTrigger>
                                <SelectValue placeholder="Select genome build" />
                            </SelectTrigger>
                        </FormControl>
                        <SelectContent>
                            {SAREK_GENOMES.map(g => (
                                <SelectItem key={g.value} value={g.value}>
                                    {g.label}
                                </SelectItem>
                            ))}
                        </SelectContent>
                    </Select>
                    <FormDescription className="italic"> Select the genome assembly key (e.g., GATK.GRCh38). Determines reference files used by Sarek. </FormDescription>
                    <FormMessage />
                </FormItem>
               )} />
              <FormField control={form.control} name="intervals_file" render={({ field }) => (
                <FormItem>
                    <FormLabel className="cursor-default"> Intervals File <span className="text-muted-foreground text-xs"> (Optional)</span> </FormLabel>
                    <FormControl>
                        <FileSelector
                            fileTypeLabel="Intervals"
                            fileType="intervals"
                            extensions={[".bed", ".list", ".interval_list"]}
                            value={field.value}
                            onChange={field.onChange}
                            placeholder="Select intervals file..."
                            allowNone
                            required={false} />
                    </FormControl>
                    <FormDescription className="italic"> Target regions (must end with .bed, .list, or .interval_list). Optional for WES analysis. </FormDescription>
                    <FormMessage />
                </FormItem>
               )} />
              <FormField control={form.control} name="dbsnp" render={({ field }) => ( <FormItem> <FormLabel className="cursor-default">dbSNP (VCF/VCF.GZ) <span className="text-muted-foreground text-xs">(Optional)</span></FormLabel> <FormControl> <FileSelector fileTypeLabel="dbSNP" fileType="vcf" extensions={[".vcf", ".vcf.gz", ".vcf.bgz"]} value={field.value} onChange={field.onChange} placeholder="Select dbSNP file..." allowNone /> </FormControl> <FormDescription className="italic"> Known variants VCF for base recalibration (e.g., dbSNP). </FormDescription> <FormMessage /> </FormItem> )} />
              <FormField control={form.control} name="known_indels" render={({ field }) => ( <FormItem> <FormLabel className="cursor-default">Known Indels (VCF/VCF.GZ) <span className="text-muted-foreground text-xs">(Optional)</span></FormLabel> <FormControl> <FileSelector fileTypeLabel="Known Indels" fileType="vcf" extensions={[".vcf", ".vcf.gz", ".vcf.bgz"]} value={field.value} onChange={field.onChange} placeholder="Select known indels file..." allowNone /> </FormControl> <FormDescription className="italic"> Known indels VCF for base recalibration (e.g., Mills, 1000G). </FormDescription> <FormMessage /> </FormItem> )} />
              <FormField control={form.control} name="pon" render={({ field }) => ( <FormItem> <FormLabel className="cursor-default">Panel of Normals (VCF/VCF.GZ) <span className="text-muted-foreground text-xs">(Optional)</span></FormLabel> <FormControl> <FileSelector fileTypeLabel="Panel of Normals" fileType="vcf" extensions={[".vcf", ".vcf.gz", ".vcf.bgz"]} value={field.value} onChange={field.onChange} placeholder="Select Panel of Normals file..." allowNone /> </FormControl> <FormDescription className="italic"> Panel of Normals VCF for somatic variant calling. </FormDescription> <FormMessage /> </FormItem> )} />
            </CardContent>
          </Card>

          {/* Parameters Section */}
           <Card>
             <CardHeader>
                 <CardTitle className="text-primary">Pipeline Parameters</CardTitle>
                 <CardDescription>Configure Sarek workflow options.</CardDescription>
             </CardHeader>
             <CardContent className="grid grid-cols-1 md:grid-cols-2 gap-6">
                 <FormField control={form.control} name="aligner" render={({ field }) => ( <FormItem> <FormLabel className="cursor-default">Aligner</FormLabel> <Select onValueChange={field.onChange} defaultValue={field.value} value={field.value}> <FormControl> <SelectTrigger> <SelectValue placeholder="Select aligner" /> </SelectTrigger> </FormControl> <SelectContent> {SAREK_ALIGNERS.map(a => <SelectItem key={a} value={a}>{a}</SelectItem>)} </SelectContent> </Select> <FormDescription className="italic"> Alignment algorithm (default: bwa-mem). </FormDescription> <FormMessage /> </FormItem> )} />
                 <FormField control={form.control} name="profile" render={({ field }) => ( <FormItem> <FormLabel className="cursor-default">Execution Profile</FormLabel> <Select onValueChange={field.onChange} defaultValue={field.value} value={field.value}> <FormControl> <SelectTrigger> <SelectValue placeholder="Select execution profile" /> </SelectTrigger> </FormControl> <SelectContent> {SAREK_PROFILES.map(p => <SelectItem key={p} value={p}>{p}</SelectItem>)} </SelectContent> </Select> <FormDescription className="italic"> Container or environment system (e.g., Docker). </FormDescription> <FormMessage /> </FormItem> )} />
                 <FormField control={form.control} name="step" render={({ field }) => ( <FormItem> <FormLabel className="cursor-default">Starting Step</FormLabel> <Select onValueChange={field.onChange} defaultValue={field.value} value={field.value}> <FormControl> <SelectTrigger> <SelectValue placeholder="Select starting step" /> </SelectTrigger> </FormControl> <SelectContent> {SAREK_STEPS.map(s => <SelectItem key={s} value={s}>{s}</SelectItem>)} </SelectContent> </Select> <FormDescription className="italic"> Start pipeline from this step (default: mapping). </FormDescription> <FormMessage /> </FormItem> )} />

                 {/* Tools Checkboxes */}
                 <div className="md:col-span-2">
                     <div className="mb-4">
                         <div className="text-base font-medium">Variant Calling Tools</div>
                         <p className="text-sm text-muted-foreground">Select tools to run (e.g., Strelka, Mutect2).</p>
                     </div>
                     <div className="grid grid-cols-2 sm:grid-cols-3 md:grid-cols-4 gap-2">
                         {SAREK_TOOLS.map((tool) => {
                             const uniqueId = `tool-${tool}`;
                             const currentTools: string[] = form.watch("tools") || [];
                             const isChecked = currentTools.includes(tool);
                             return (
                                 <FormItem
                                     key={uniqueId}
                                     className="flex flex-row items-start space-x-3 space-y-0 rounded-md border p-3 hover:bg-accent/50 transition-colors select-none"
                                 >
                                     <FormLabel
                                         htmlFor={uniqueId}
                                         className="flex flex-row items-start space-x-3 space-y-0 font-normal cursor-pointer w-full h-full"
                                     >
                                         <FormControl className="flex h-6 items-start">
                                             <Checkbox
                                                 id={uniqueId}
                                                 checked={isChecked}
                                                 onCheckedChange={() => {
                                                     toggleCheckboxValue('tools', tool);
                                                 }}
                                             />
                                         </FormControl>
                                         <span className="pt-px">{tool}</span>
                                     </FormLabel>
                                 </FormItem>
                             );
                         })}
                     </div>
                     {/* Display the refine error message for tools here */}
                     <FormField control={form.control} name="tools" render={() => <FormMessage className="pt-2" />} />
                 </div>


                 {/* Boolean Flags Group */}
                  <div className="md:col-span-2 space-y-4">
                      {[
                          { name: 'joint_germline', label: 'Joint Germline Calling', description: 'Enable joint calling across samples (requires all samples Status=0).' },
                          { name: 'wes', label: 'Whole Exome Sequencing (WES)', description: 'Check if data is WES/targeted. Requires an Intervals file.' },
                          { name: 'trim_fastq', label: 'Trim FASTQ', description: 'Enable adapter trimming using Trim Galore!.' },
                          { name: 'skip_qc', label: 'Skip QC', description: 'Skip quality control steps (FastQC, Samtools, etc.).' },
                          { name: 'skip_annotation', label: 'Skip Annotation', description: 'Skip variant annotation steps (VEP, snpEff).' },
                          { name: 'skip_baserecalibrator', label: 'Skip Base Recalibration', description: 'Skip the base quality score recalibration step. This can speed up the pipeline but may affect variant calling quality.' },
                      ].map((flag) => {
                            const fieldName = flag.name as keyof PipelineFormValues;
                            const uniqueId = `flag-${flag.name}`;
                            const isChecked = form.watch(fieldName) as boolean;
                            return (
                                <FormItem
                                    key={uniqueId}
                                    className="flex flex-row items-start space-x-3 space-y-0 rounded-md border p-4 hover:bg-accent/50 transition-colors select-none"
                                >
                                    <FormLabel
                                        htmlFor={uniqueId}
                                        className="flex flex-row items-start space-x-3 space-y-0 font-normal cursor-pointer w-full h-full"
                                    >
                                        <FormControl className="flex h-6 items-start">
                                            <Checkbox
                                                id={uniqueId}
                                                checked={isChecked}
                                                onCheckedChange={() => {
                                                    toggleCheckboxValue(fieldName);
                                                }}
                                            />
                                        </FormControl>
                                        <div className="space-y-1 leading-none pt-px">
                                            <span>{flag.label}</span>
                                            <FormDescription className="italic mt-1">
                                                {flag.description}
                                            </FormDescription>
                                        </div>
                                    </FormLabel>
                                     <FormField
                                         control={form.control}
                                         name={fieldName}
                                         render={() => <FormMessage className="pt-1 pl-[calc(1rem+0.75rem)]" />}
                                     />
                                </FormItem>
                            );
                      })}
                  </div>
             </CardContent>
           </Card>

           {/* Metadata Section */}
            <Card>
                <CardHeader>
                    <CardTitle className="text-primary">Metadata</CardTitle>
                </CardHeader>
                <CardContent>
                    <FormField control={form.control} name="description" render={({ field }) => ( <FormItem> <FormLabel className="cursor-default">Run Description <span className="text-muted-foreground text-xs">(Optional)</span></FormLabel> <FormControl> <Input placeholder="e.g., Initial somatic analysis for Cohort X" {...field} value={field.value ?? ''}/> </FormControl> <FormMessage /> </FormItem> )} />
                </CardContent>
            </Card>

          {/* Submit Button */}
          <div className="flex justify-start ml-[1%]">
            <Button
               type="submit"
               disabled={stageMutation.isPending}
               className="border border-primary hover:underline cursor-pointer bg-primary text-primary-foreground hover:bg-primary/90"
            >
              {stageMutation.isPending
                ? <Loader2 className="mr-2 h-4 w-4 animate-spin" />
                : <Play className="mr-2 h-4 w-4" />
              }
              Stage Pipeline Run
            </Button>
          </div>
        </form>
      </Form>
    </FormProvider>
  );
}

--- END FILE: ./frontend_app/app/(pages)/input/page.tsx ---

--- START FILE: ./frontend_app/app/layout.tsx (Size: 2353 bytes) ---
// File: frontend_app/app/layout.tsx
import type { Metadata } from "next";
import { GeistSans } from "geist/font/sans";
import "./globals.css"; // Keep this import

// Layout Components
import Navbar from "@/components/layout/Navbar";
import Footer from "@/components/layout/Footer";
import FileBrowserIntegration from "@/components/layout/FileBrowserIntegration";

// Providers and UI Elements
import QueryProvider from "@/components/providers/QueryProvider";
import { Toaster } from "@/components/ui/sonner";
import { ThemeProvider } from "@/components/providers/ThemeProvider";
import { cn } from "@/lib/utils"; // Import cn
import { FileBrowserProvider } from "@/components/layout/FileBrowserContext";

export const metadata: Metadata = {
  title: "Bioinformatics Pipeline UI",
  description: "Stage, run, and manage Sarek bioinformatics pipelines",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en" className={cn("h-full", GeistSans.variable)} suppressHydrationWarning>
       {/* Body is the main flex container taking full height */}
      <body className={cn(
          "bg-background text-foreground font-sans antialiased"
        )}>
        <ThemeProvider
          attribute="class"
          defaultTheme="system"
          enableSystem
          disableTransitionOnChange
        >
          <FileBrowserProvider>
            <QueryProvider>
               {/* Navbar is sticky relative to the body */}
              <Navbar />
              {/* This div becomes the scrollable container */}
              <div className="flex flex-col flex-grow overflow-y-auto"> {/* Scrollable container */}
                {/* Main content area with padding and centering */}
                <main className="flex-grow container mx-auto px-4 py-8 relative"> {/* flex-grow pushes footer down */}
                  {children}
                </main>
                {/* Footer is inside scrollable area, but outside main's container/padding */}
                <Footer />
              </div>
              {/* These are outside the scrollable area */}
              <Toaster richColors position="top-right" />
              <FileBrowserIntegration />
            </QueryProvider>
          </FileBrowserProvider>
        </ThemeProvider>
      </body>
    </html>
  );
}

--- END FILE: ./frontend_app/app/layout.tsx ---

--- START FILE: ./frontend_app/app/layout.css (Size: 185 bytes) ---
@layer utilities {
  .\[\&_svg\:not\(\[class\*\=\'size-\'\]\)\]\:size-4 svg:not([class*="size-"]) {
    width: calc(var(--spacing) * 4.5);
    height: calc(var(--spacing) * 4.5);
  }
} 
--- END FILE: ./frontend_app/app/layout.css ---

--- START FILE: ./frontend_app/app/globals.css (Size: 3089 bytes) ---
/* File: frontend_app/app/globals.css */
@config "../tailwind.config.ts"; /* Tell Tailwind v4 to load the JS config */
@import "tailwindcss";

/* Define the CSS variables */
@layer base {
  :root { /* ... Keep all your color variables ... */
    --radius: 0.625rem;
    --background: oklch(0.99 0.003 240);
    --foreground: oklch(0.1 0.03 250);
    --card: oklch(1 0 0);
    --card-foreground: oklch(0.1 0.03 250);
    --popover: oklch(1 0 0);
    --popover-foreground: oklch(0.1 0.03 250);
    --primary: oklch(0.208 0.042 265.755);
    --primary-foreground: oklch(0.984 0.003 247.858);
    --secondary: oklch(0.90 0.02 255);
    --secondary-foreground: oklch(0.15 0.03 255);
    --muted: oklch(0.93 0.01 240);
    --muted-foreground: oklch(0.40 0.03 250);
    --accent: oklch(0.94 0.015 260);
    --accent-foreground: oklch(0.15 0.03 255);
    --destructive: oklch(0.577 0.245 27.325);
    --destructive-foreground: oklch(0.984 0.003 247.858);
    --border: oklch(0.88 0.005 240);
    --input: oklch(0.88 0.005 240);
    --ring: oklch(0.65 0.04 260);
    --chart-1: oklch(0.646 0.222 41.116);
    --chart-2: oklch(0.6 0.118 184.704);
    --chart-3: oklch(0.398 0.07 227.392);
    --chart-4: oklch(0.828 0.189 84.429);
    --chart-5: oklch(0.769 0.188 70.08);
    --sidebar: oklch(0.984 0.003 247.858);
    --sidebar-foreground: oklch(0.129 0.042 264.695);
    --sidebar-primary: oklch(0.208 0.042 265.755);
    --sidebar-primary-foreground: oklch(0.984 0.003 247.858);
    --sidebar-accent: oklch(0.968 0.007 247.896);
    --sidebar-accent-foreground: oklch(0.208 0.042 265.755);
    --sidebar-border: oklch(0.929 0.013 255.508);
    --sidebar-ring: oklch(0.704 0.04 256.788);
    --font-geist-sans: ;
  }

  .dark { /* ... Keep dark mode variables ... */
    --background: oklch(0.1 0.01 240);
    --foreground: oklch(0.96 0.005 240);
    --card: oklch(0.15 0.015 240);
    --card-foreground: oklch(0.96 0.005 240);
    --popover: oklch(0.15 0.015 240);
    --popover-foreground: oklch(0.96 0.005 240);
    --primary: oklch(0.3 0.05 265.755);
    --primary-foreground: oklch(0.99 0.005 250);
    --secondary: oklch(0.25 0.03 255);
    --secondary-foreground: oklch(0.96 0.005 240);
    --muted: oklch(0.2 0.02 240);
    --muted-foreground: oklch(0.60 0.02 240);
    --accent: oklch(0.3 0.03 260);
    --accent-foreground: oklch(0.96 0.005 240);
    --destructive: oklch(0.65 0.19 22);
    --destructive-foreground: oklch(0.96 0.005 240);
    --border: oklch(0.25 0.01 240);
    --input: oklch(0.25 0.01 240);
    --ring: oklch(0.5 0.03 260);
   }

  /* *** Prevent body/html scroll and set height *** */
  html, body {
      height: 100%;
      overflow: hidden;
      overscroll-behavior: contain;
  }

  /* Body uses main background, is flex container */
  body {
    @apply bg-background text-foreground;
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
    display: flex;
    flex-direction: column;
    min-height: 100vh;
    height: 100vh;
  }

  input::placeholder,
  textarea::placeholder {
    color: var(--foreground);
    opacity: 0.5;
  }
}

--- END FILE: ./frontend_app/app/globals.css ---

--- START FILE: ./frontend_app/public/window.svg (Size: 385 bytes) ---
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.5 2.5h13v10a1 1 0 0 1-1 1h-11a1 1 0 0 1-1-1zM0 1h16v11.5a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 0 12.5zm3.75 4.5a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5M7 4.75a.75.75 0 1 1-1.5 0 .75.75 0 0 1 1.5 0m1.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5" fill="#666"/></svg>
--- END FILE: ./frontend_app/public/window.svg ---

--- START FILE: ./frontend_app/public/globe.svg (Size: 1035 bytes) ---
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><g clip-path="url(#a)"><path fill-rule="evenodd" clip-rule="evenodd" d="M10.27 14.1a6.5 6.5 0 0 0 3.67-3.45q-1.24.21-2.7.34-.31 1.83-.97 3.1M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16m.48-1.52a7 7 0 0 1-.96 0H7.5a4 4 0 0 1-.84-1.32q-.38-.89-.63-2.08a40 40 0 0 0 3.92 0q-.25 1.2-.63 2.08a4 4 0 0 1-.84 1.31zm2.94-4.76q1.66-.15 2.95-.43a7 7 0 0 0 0-2.58q-1.3-.27-2.95-.43a18 18 0 0 1 0 3.44m-1.27-3.54a17 17 0 0 1 0 3.64 39 39 0 0 1-4.3 0 17 17 0 0 1 0-3.64 39 39 0 0 1 4.3 0m1.1-1.17q1.45.13 2.69.34a6.5 6.5 0 0 0-3.67-3.44q.65 1.26.98 3.1M8.48 1.5l.01.02q.41.37.84 1.31.38.89.63 2.08a40 40 0 0 0-3.92 0q.25-1.2.63-2.08a4 4 0 0 1 .85-1.32 7 7 0 0 1 .96 0m-2.75.4a6.5 6.5 0 0 0-3.67 3.44 29 29 0 0 1 2.7-.34q.31-1.83.97-3.1M4.58 6.28q-1.66.16-2.95.43a7 7 0 0 0 0 2.58q1.3.27 2.95.43a18 18 0 0 1 0-3.44m.17 4.71q-1.45-.12-2.69-.34a6.5 6.5 0 0 0 3.67 3.44q-.65-1.27-.98-3.1" fill="#666"/></g><defs><clipPath id="a"><path fill="#fff" d="M0 0h16v16H0z"/></clipPath></defs></svg>
--- END FILE: ./frontend_app/public/globe.svg ---

--- START FILE: ./frontend_app/public/file.svg (Size: 391 bytes) ---
<svg fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><path d="M14.5 13.5V5.41a1 1 0 0 0-.3-.7L9.8.29A1 1 0 0 0 9.08 0H1.5v13.5A2.5 2.5 0 0 0 4 16h8a2.5 2.5 0 0 0 2.5-2.5m-1.5 0v-7H8v-5H3v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1M9.5 5V2.12L12.38 5zM5.13 5h-.62v1.25h2.12V5zm-.62 3h7.12v1.25H4.5zm.62 3h-.62v1.25h7.12V11z" clip-rule="evenodd" fill="#666" fill-rule="evenodd"/></svg>
--- END FILE: ./frontend_app/public/file.svg ---

--- START FILE: ./frontend_app/tsconfig.json (Size: 797 bytes) ---
// frontend_app/tsconfig.json
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    // --- CRITICAL ---
    "baseUrl": ".", // Set the base URL relative to tsconfig.json location
    "paths": {
      "@/*": ["./*"] // Ensure paths map correctly from the base URL
    }
    // --------------
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}

--- END FILE: ./frontend_app/tsconfig.json ---

--- START FILE: ./frontend_app/components/jobs/JobActions.tsx (Size: 23902 bytes) ---
// File: frontend_app/components/jobs/JobActions.tsx
"use client";

import React, { useState } from "react";
import { useRouter } from "next/navigation";
import Link from "next/link";
import { useMutation, useQueryClient } from "@tanstack/react-query";
import {
    MoreHorizontal,
    Play,
    Square,
    StopCircle,
    Trash2,
    RotateCcw,
    Info,
    FolderGit2,
    ExternalLink,
    Loader2,
} from "lucide-react";
import { toast } from "sonner";

import { Button, buttonVariants } from "@/components/ui/button";
import {
    DropdownMenu,
    DropdownMenuContent,
    DropdownMenuItem,
    DropdownMenuSeparator,
    DropdownMenuTrigger,
} from "@/components/ui/dropdown-menu";
import {
    AlertDialog,
    AlertDialogAction,
    AlertDialogCancel,
    AlertDialogContent,
    AlertDialogDescription,
    AlertDialogFooter,
    AlertDialogHeader,
    AlertDialogTitle,
} from "@/components/ui/alert-dialog";
import {
    Dialog,
    DialogContent,
    DialogHeader,
    DialogTitle,
    DialogDescription,
    DialogFooter,
    DialogClose
} from "@/components/ui/dialog";
import { Badge } from "@/components/ui/badge";
import * as DropdownMenuPrimitive from "@radix-ui/react-dropdown-menu";

// Import specific types needed
import { Job, SampleInfo, JobMeta, InputFilenames, SarekParams } from "@/lib/types";
import * as api from "@/lib/api";
import { formatDuration } from "@/lib/utils";
import { formatDistanceToNow } from 'date-fns';

// --- Helper functions (remain the same) ---
function getStatusVariant(status: string | null | undefined): "default" | "destructive" | "secondary" | "outline" {
    switch (status?.toLowerCase()) {
        case 'finished': return 'default';
        case 'failed': return 'destructive';
        case 'running': case 'started': return 'default';
        case 'queued': case 'staged': return 'secondary';
        case 'stopped': case 'canceled': return 'outline';
        default: return 'secondary';
    }
}
function formatTimestamp(timestamp: number | null | undefined): string {
  if (!timestamp) return "N/A";
  try {
    const date = new Date(timestamp * 1000);
    if (isNaN(date.getTime())) return "Invalid Date";
    return date.toLocaleString();
  } catch (e) { return "Invalid Date"; }
}
function formatTimestampRelative(timestamp: number | null | undefined): string {
  if (!timestamp) return "N/A";
  try {
    if (timestamp <= 0) return "N/A";
    const date = new Date(timestamp * 1000);
    if (isNaN(date.getTime())) return "Invalid Date";
    return formatDistanceToNow(date, { addSuffix: true });
  } catch (e) { console.error("Error formatting relative timestamp:", timestamp, e); return "Invalid Date"; }
}
const formatParamKey = (key: string): string => { return key.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase()); }
const formatParamValue = (value: string | number | boolean | null | undefined | any[] | Record<string, any>): string => {
    if (value === true) return 'Yes'; if (value === false) return 'No';
    if (value === null || value === undefined) return 'N/A';
    if (typeof value === 'string' && value.trim() === '') return 'N/A';
    if (Array.isArray(value)) { return value.length > 0 ? value.join(', ') : 'N/A'; }
    if (typeof value === 'object') { return JSON.stringify(value); }
    return String(value);
}
// --- End Helper Functions ---

interface JobActionsProps {
  job: Job | undefined;
}

export default function JobActions({ job }: JobActionsProps) {
    if (!job) {
        console.warn("JobActions rendered with undefined job prop");
        return null;
    }

    const router = useRouter();
    const queryClient = useQueryClient();
    const [isDetailsOpen, setIsDetailsOpen] = useState(false);
    const [isStopConfirmOpen, setIsStopConfirmOpen] = useState(false);
    const [isRemoveConfirmOpen, setIsRemoveConfirmOpen] = useState(false);
    const [isRerunConfirmOpen, setIsRerunConfirmOpen] = useState(false);

    const invalidateJobsList = () => { queryClient.invalidateQueries({ queryKey: ['jobsList'] }); };

    // Define mutations
    const startMutation = useMutation({ mutationFn: api.startJob, onSuccess: (data) => { toast.success(`Job ${data.job_id} started successfully.`); invalidateJobsList(); }, onError: (error: Error) => { toast.error(`Failed to start job: ${error.message}`); } });
    const stopMutation = useMutation({ mutationFn: api.stopJob, onSuccess: (data) => { toast.info(`Stop signal sent to job ${data.job_id}.`); invalidateJobsList(); }, onError: (error: Error) => { toast.error(`Failed to stop job: ${error.message}`); }, onSettled: () => setIsStopConfirmOpen(false), });
    const removeMutation = useMutation({ 
        mutationFn: api.removeJob, 
        onSuccess: () => { 
            toast.success(`Job ${job.id} removed.`); 
            invalidateJobsList(); 
        }, 
        onError: (error: Error) => { 
            toast.error(`Failed to remove job: ${error.message}`); 
        }, 
        onSettled: () => setIsRemoveConfirmOpen(false), 
    });
    const rerunMutation = useMutation({ mutationFn: api.rerunJob, onSuccess: (data) => { toast.success(`Job ${job.id} re-staged as ${data.staged_job_id}.`); invalidateJobsList(); router.push('/jobs'); }, onError: (error: Error) => { toast.error(`Failed to re-stage job: ${error.message}`); }, onSettled: () => setIsRerunConfirmOpen(false), });

    // Modify handlers to check job before mutating
    const handleStart = () => { if (!job) return; startMutation.mutate(job.id); };
    const handleStop = () => { if (!job) return; stopMutation.mutate(job.id); };
    const handleRemove = () => { if (!job) return; removeMutation.mutate(job.id); };
    const handleRerun = () => { if (!job) return; rerunMutation.mutate(job.id); };

    // Checks remain the same
    const canStart = job.status === 'staged';
    const canStop = job.status === 'running' || job.status === 'started' || job.status === 'queued';
    const canRerun = job.status === 'finished' || job.status === 'failed' || job.status === 'stopped' || job.status === 'canceled';
    const canRemove = job.status === 'staged' || job.status === 'finished' || job.status === 'failed' || job.status === 'stopped' || job.status === 'canceled';
    const meta = job.meta as JobMeta | null | undefined;
    const inputParams = meta?.input_params;
    const sarekParams = meta?.sarek_params;
    const hasParameters = !!(inputParams && Object.keys(inputParams).length > 0) || !!(sarekParams && Object.keys(sarekParams).length > 0);

    return (
        <>
            <div className="flex items-center gap-1">
                {canStart && (
                    <Button
                        variant="ghost"
                        size="icon"
                        onClick={handleStart}
                        disabled={startMutation.isPending}
                        className="h-9 w-9 p-2 flex items-center justify-center hover:bg-accent hover:text-accent-foreground cursor-pointer transition-colors"
                        title="Start Job"
                    >
                        {startMutation.isPending ? (
                            <Loader2 className="h-5 w-5 animate-spin" />
                        ) : (
                            <Play className="h-[24px] w-[24px]" />
                        )}
                        <span className="sr-only">Start Job</span>
                    </Button>
                )}
                {canStop && (
                    <Button
                        variant="ghost"
                        size="icon"
                        onClick={() => setIsStopConfirmOpen(true)}
                        disabled={stopMutation.isPending}
                        className="h-9 w-9 p-2 flex items-center justify-center hover:bg-accent hover:text-accent-foreground cursor-pointer transition-colors"
                        title="Stop Job"
                    >
                        {stopMutation.isPending ? (
                            <Loader2 className="h-5 w-5 animate-spin" />
                        ) : (
                            <Square className="h-5 w-5" />
                        )}
                        <span className="sr-only">Stop Job</span>
                    </Button>
                )}
                {canRerun && (
                    <Button
                        variant="ghost"
                        size="icon"
                        onClick={() => setIsRerunConfirmOpen(true)}
                        disabled={rerunMutation.isPending}
                        className="h-9 w-9 p-2 flex items-center justify-center hover:bg-accent hover:text-accent-foreground cursor-pointer transition-colors"
                        title="Re-stage Job"
                    >
                        {rerunMutation.isPending ? (
                            <Loader2 className="h-5 w-5 animate-spin" />
                        ) : (
                            <RotateCcw className="h-5 w-5" />
                        )}
                        <span className="sr-only">Re-stage Job</span>
                    </Button>
                )}
                <DropdownMenuPrimitive.Root>
                    <DropdownMenuPrimitive.Trigger asChild>
                        <Button 
                            variant="ghost" 
                            size="icon" 
                            className="h-9 w-9 p-2 flex items-center justify-center hover:bg-accent hover:text-accent-foreground cursor-pointer transition-colors"
                        >
                            <MoreHorizontal className="h-5 w-5" />
                            <span className="sr-only">Job Actions</span>
                        </Button>
                    </DropdownMenuPrimitive.Trigger>
                    <DropdownMenuPrimitive.Portal>
                        <DropdownMenuPrimitive.Content 
                            align="end" 
                            sideOffset={4} 
                            className="min-w-[12rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-md animate-in data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2"
                        >
                            <DropdownMenuPrimitive.Item 
                                onSelect={() => setIsDetailsOpen(true)} 
                                className="relative flex cursor-pointer select-none items-center rounded-sm px-3 py-2 text-sm outline-none transition-colors hover:bg-accent hover:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50"
                            >
                                <span className="flex items-center gap-2">
                                    <Info className="h-4 w-4" />
                                    View Details
                                </span>
                            </DropdownMenuPrimitive.Item>

                            {job.status === 'finished' && job.result?.results_path && (
                                <DropdownMenuPrimitive.Item 
                                    asChild 
                                    className="relative flex cursor-pointer select-none items-center rounded-sm px-3 py-2 text-sm outline-none transition-colors hover:bg-accent hover:text-accent-foreground"
                                >
                                    <Link href={`/results?highlight=${job.result.results_path.split('/').pop()}`} className="flex items-center gap-2">
                                        <FolderGit2 className="h-4 w-4" />
                                        View Results
                                    </Link>
                                </DropdownMenuPrimitive.Item>
                            )}

                            {canRemove && (
                                <>
                                    <DropdownMenuPrimitive.Separator className="my-1 h-px bg-muted" />
                                    <DropdownMenuPrimitive.Item 
                                        onSelect={() => setIsRemoveConfirmOpen(true)} 
                                        disabled={removeMutation.isPending} 
                                        className="relative flex cursor-pointer select-none items-center rounded-sm px-3 py-2 text-sm outline-none transition-colors hover:bg-destructive/10 hover:text-destructive text-destructive data-[disabled]:pointer-events-none data-[disabled]:opacity-50 dark:hover:bg-destructive/20"
                                    >
                                        <span className="flex items-center gap-2">
                                            {removeMutation.isPending ? <Loader2 className="h-4 w-4 animate-spin" /> : <Trash2 className="h-4 w-4" />}
                                            Remove Job
                                        </span>
                                    </DropdownMenuPrimitive.Item>
                                </>
                            )}
                        </DropdownMenuPrimitive.Content>
                    </DropdownMenuPrimitive.Portal>
                </DropdownMenuPrimitive.Root>
            </div>

             {/* Dialogs remain the same */}
             <Dialog open={isDetailsOpen} onOpenChange={setIsDetailsOpen}>
                 <DialogContent className="sm:max-w-2xl">
                     <DialogHeader>
                         <DialogTitle>Job Details: {job.id}</DialogTitle>
                         <DialogDescription>{job.description || "No description provided."}</DialogDescription>
                     </DialogHeader>
                     <div className="mt-4 max-h-[65vh] overflow-y-auto space-y-4 pr-2">
                         {/* Status & Timestamps */}
                         <div className="grid grid-cols-2 gap-x-4 gap-y-2 text-sm">
                             <div className="font-medium text-muted-foreground">Status:</div>
                             <div><Badge variant={getStatusVariant(job.status)} className="capitalize">{job.status || 'Unknown'}</Badge></div>
                             <div className="font-medium text-muted-foreground">Staged:</div>
                             <div>{formatTimestamp(job.staged_at)} ({formatTimestampRelative(job.staged_at)})</div>
                             <div className="font-medium text-muted-foreground">Enqueued:</div>
                             <div>{formatTimestamp(job.enqueued_at)} ({formatTimestampRelative(job.enqueued_at)})</div>
                             <div className="font-medium text-muted-foreground">Started:</div>
                             <div>{formatTimestamp(job.started_at)} ({formatTimestampRelative(job.started_at)})</div>
                              <div className="font-medium text-muted-foreground">Ended:</div>
                             <div>{formatTimestamp(job.ended_at)} ({formatTimestampRelative(job.ended_at)})</div>
                             <div className="font-medium text-muted-foreground">Duration:</div>
                             <div>{formatDuration(job.resources?.duration_seconds)}</div>
                         </div>
                         {/* Resources */}
                         {job.resources && (job.resources.peak_memory_mb || job.resources.average_cpu_percent) && (
                              <div>
                                  <h4 className="font-semibold mb-1">Resources Used:</h4>
                                  <div className="grid grid-cols-2 gap-x-4 gap-y-1 text-sm pl-4">
                                      <div className="font-medium text-muted-foreground">Peak Memory:</div>
                                      <div>{job.resources.peak_memory_mb ? `${job.resources.peak_memory_mb.toFixed(1)} MB` : 'N/A'}</div>
                                      <div className="font-medium text-muted-foreground">Average CPU:</div>
                                      <div>{job.resources.average_cpu_percent ? `${job.resources.average_cpu_percent.toFixed(1)} %` : 'N/A'}</div>
                                  </div>
                             </div>
                         )}
                         {/* Parameters - Use the explicit check */}
                         {hasParameters && (
                             <div>
                                 <h4 className="font-semibold mb-1">Parameters:</h4>
                                 <div className="grid grid-cols-2 gap-x-4 gap-y-1 text-sm pl-4">
                                     {inputParams && Object.entries(inputParams).map(([key, value]) => (
                                         <React.Fragment key={`input-${key}`}>
                                             <div className="font-medium text-muted-foreground truncate" title={key}>{formatParamKey(key)}:</div>
                                             <div className="font-mono text-xs break-words" title={String(value ?? '')}>{formatParamValue(value)}</div>
                                         </React.Fragment>
                                     ))}
                                      {sarekParams && Object.entries(sarekParams).map(([key, value]) => (
                                         <React.Fragment key={`sarek-${key}`}>
                                             <div className="font-medium text-muted-foreground truncate" title={key}>{formatParamKey(key)}:</div>
                                             <div className="font-mono text-xs break-words" title={String(value ?? '')}>{formatParamValue(value)}</div>
                                         </React.Fragment>
                                     ))}
                                 </div>
                             </div>
                         )}
                         {/* Sample Info */}
                         {meta?.sample_info && meta.sample_info.length > 0 && (
                              <div>
                                 <h4 className="font-semibold mb-1">Samples Processed:</h4>
                                 <ul className="text-sm pl-4 space-y-1 list-disc list-inside">
                                    {meta.sample_info.map((sample: SampleInfo, index: number) => (
                                        <li key={index}>
                                            {sample.patient} / {sample.sample} (Sex: {sample.sex}, Status: {sample.status === 1 ? 'Tumor' : 'Normal'})
                                            <span className="block text-xs text-muted-foreground pl-4 truncate" title={`${sample.fastq_1}, ${sample.fastq_2}`}>
                                                FASTQs: {sample.fastq_1}, {sample.fastq_2}
                                            </span>
                                        </li>
                                    ))}
                                 </ul>
                             </div>
                         )}
                         {/* Error Info */}
                          {job.status === 'failed' && (
                              <div>
                                 <h4 className="font-semibold mb-1 text-destructive">Error Details:</h4>
                                 <div className="space-y-1 text-sm pl-4">
                                     <p className="font-medium">{job.error || "Job failed"}</p>
                                     {meta?.stderr_snippet && (
                                         <pre className="mt-2 text-xs bg-destructive/10 p-2 rounded font-mono whitespace-pre-wrap max-h-40 overflow-y-auto">
                                             <code>{meta.stderr_snippet}</code>
                                         </pre>
                                     )}
                                 </div>
                             </div>
                          )}
                         {/* Result Path */}
                          {job.status === 'finished' && job.result?.results_path && (
                             <div>
                                 <h4 className="font-semibold mb-1">Results Path:</h4>
                                  <p className="text-sm pl-4 font-mono break-all">{job.result.results_path}</p>
                             </div>
                          )}
                     </div>
                     <DialogFooter className="mt-4">
                         <DialogClose asChild>
                             <Button type="button" variant="outline">Close</Button>
                         </DialogClose>
                     </DialogFooter>
                 </DialogContent>
             </Dialog>

             {/* Confirmation Dialogs */}
             <AlertDialog open={isStopConfirmOpen} onOpenChange={setIsStopConfirmOpen}>
                  <AlertDialogContent>
                     <AlertDialogHeader>
                         <AlertDialogTitle>Confirm Stop Job</AlertDialogTitle>
                         <AlertDialogDescription> Are you sure you want to stop job <span className="font-mono font-semibold">{job.id}</span>? This will attempt to cancel it if queued or signal it to terminate if running. </AlertDialogDescription>
                     </AlertDialogHeader>
                     <AlertDialogFooter>
                         <AlertDialogCancel disabled={stopMutation.isPending}>Cancel</AlertDialogCancel>
                         <AlertDialogAction onClick={handleStop} disabled={stopMutation.isPending} className="bg-yellow-500 hover:bg-yellow-600 text-white dark:bg-yellow-600 dark:hover:bg-yellow-700"> {stopMutation.isPending && <Loader2 className="mr-2 h-4 w-4 animate-spin" />} Stop Job </AlertDialogAction>
                     </AlertDialogFooter>
                 </AlertDialogContent>
             </AlertDialog>
              <AlertDialog open={isRemoveConfirmOpen} onOpenChange={setIsRemoveConfirmOpen}>
                  <AlertDialogContent>
                     <AlertDialogHeader>
                         <AlertDialogTitle>Confirm Remove Job</AlertDialogTitle>
                         <AlertDialogDescription> Are you sure you want to remove job <span className="font-mono font-semibold">{job.id}</span>? This will remove its entry from the list. Results files (if any) will not be deleted. This action cannot be undone. </AlertDialogDescription>
                     </AlertDialogHeader>
                     <AlertDialogFooter>
                         <AlertDialogCancel disabled={removeMutation.isPending}>Cancel</AlertDialogCancel>
                         <AlertDialogAction onClick={handleRemove} disabled={removeMutation.isPending} className={buttonVariants({ variant: "destructive" })}> {removeMutation.isPending && <Loader2 className="mr-2 h-4 w-4 animate-spin" />} Remove Job </AlertDialogAction>
                     </AlertDialogFooter>
                 </AlertDialogContent>
             </AlertDialog>
              <AlertDialog open={isRerunConfirmOpen} onOpenChange={setIsRerunConfirmOpen}>
                  <AlertDialogContent>
                     <AlertDialogHeader>
                         <AlertDialogTitle>Confirm Re-stage Job</AlertDialogTitle>
                         <AlertDialogDescription> Are you sure you want to re-stage job <span className="font-mono font-semibold">{job.id}</span>? This will create a new 'staged' job entry using the same parameters. </AlertDialogDescription>
                     </AlertDialogHeader>
                     <AlertDialogFooter>
                         <AlertDialogCancel disabled={rerunMutation.isPending}>Cancel</AlertDialogCancel>
                         <AlertDialogAction onClick={handleRerun} disabled={rerunMutation.isPending}> {rerunMutation.isPending && <Loader2 className="mr-2 h-4 w-4 animate-spin" />} Re-stage Job </AlertDialogAction>
                     </AlertDialogFooter>
                 </AlertDialogContent>
             </AlertDialog>
        </>
    );
}

--- END FILE: ./frontend_app/components/jobs/JobActions.tsx ---

--- START FILE: ./frontend_app/components/jobs/JobTable.tsx (Size: 5178 bytes) ---
// File: frontend_app/components/jobs/JobTable.tsx
import {
  Table,
  TableBody,
  TableCaption,
  TableCell,
  TableHead,
  TableHeader,
  TableRow,
} from "@/components/ui/table";
import { Badge } from "@/components/ui/badge";
import { Job } from "@/lib/types";
import { formatDistanceToNow } from 'date-fns';
import { formatDuration } from "@/lib/utils";
import JobActions from "./JobActions"; // Using the version with <span> wrappers
import React from 'react';

interface JobTableProps {
  jobs: Job[];
}

// Helper functions (keep as they are)
function formatTimestamp(timestamp: number | null | undefined): React.ReactElement | string {
  if (!timestamp) return "N/A";
  try {
    const date = new Date(timestamp * 1000);
    if (isNaN(date.getTime())) return "Invalid Date";
    const relative = formatDistanceToNow(date, { addSuffix: true });
    const absolute = date.toLocaleString();
    return <span title={absolute}>{relative}</span>;
  } catch (e) {
    console.error("Error formatting timestamp:", timestamp, e);
    return "Invalid Date";
  }
}

function getStatusVariant(status: string | null | undefined): "default" | "destructive" | "secondary" | "outline" {
    switch (status?.toLowerCase()) {
        case 'finished': return 'default';
        case 'failed': return 'destructive';
        case 'running': case 'started': return 'default';
        case 'queued': case 'staged': return 'secondary';
        case 'stopped': case 'canceled': return 'outline';
        default: return 'secondary';
    }
}

export default function JobTable({ jobs }: JobTableProps) {
  // console.log("JobTable component received jobs prop:", jobs); // Keep logs if needed

  if (!jobs || jobs.length === 0) {
    // console.log("JobTable rendering 'No jobs found.' message");
    return <p className="text-center text-muted-foreground py-8">No jobs found.</p>;
  }

  // console.log("JobTable rendering the actual table with jobs:", jobs);

  return (
    <div className="border rounded-lg overflow-hidden overflow-x-auto">
      <Table>
        <TableCaption className="mt-4">A list of your pipeline jobs.</TableCaption>
        <TableHeader>
          <TableRow>
            <TableHead className="w-[120px] hidden lg:table-cell">Job ID</TableHead>
            <TableHead>Description / ID</TableHead>
            <TableHead className="w-[100px] hidden xl:table-cell">Step</TableHead>
            <TableHead className="w-[100px] hidden xl:table-cell">Genome</TableHead>
            <TableHead className="w-[120px] hidden md:table-cell">Duration</TableHead>
            <TableHead className="w-[150px] hidden sm:table-cell">Last Updated</TableHead>
            <TableHead className="w-[100px]">Status</TableHead>
            <TableHead className="text-right w-[60px]">Actions</TableHead>
          </TableRow>
        </TableHeader>
        <TableBody>
          {jobs.map((job) => (
            <TableRow key={job.id} data-state={job.status === 'finished' ? 'completed' : job.status === 'failed' ? 'error' : undefined}>
              <TableCell className="font-mono text-xs hidden lg:table-cell" title={job.id}>
                {job.id.startsWith("staged_") ? "STAGED" : "RQ"}
                <span className="text-muted-foreground">_</span>
                {job.id.substring(job.id.indexOf('_') + 1, job.id.indexOf('_') + 9)}...
              </TableCell>
              <TableCell className="max-w-xs truncate" title={job.description ?? job.id}>
                <span className="lg:hidden font-mono text-xs mr-1">
                  {job.id.startsWith("staged_") ? "STG" : "RQ"}
                  <span className="text-muted-foreground">_</span>
                  {job.id.substring(job.id.indexOf('_') + 1, job.id.indexOf('_') + 9)}...
                </span>
                <span className="lg:hidden mr-1">|</span>
                {job.description || <span className="italic text-muted-foreground">{job.id.startsWith("staged_") ? 'Staged Job' : 'Pipeline Job'}</span>}
              </TableCell>
              <TableCell className="text-sm text-muted-foreground hidden xl:table-cell">
                {job.meta?.sarek_params?.step || 'all'}
              </TableCell>
              <TableCell className="text-sm text-muted-foreground hidden xl:table-cell">
                {job.meta?.sarek_params?.genome || 'N/A'}
              </TableCell>
              <TableCell className="text-sm text-muted-foreground hidden md:table-cell">
                {formatDuration(job.resources?.duration_seconds)}
              </TableCell>
              <TableCell className="text-sm text-muted-foreground hidden sm:table-cell">
                {formatTimestamp(job.ended_at || job.started_at || job.enqueued_at || job.staged_at)}
              </TableCell>
              <TableCell>
                <Badge variant={getStatusVariant(job.status)} className="capitalize text-xs px-1.5 py-0.5">
                  {job.status}
                </Badge>
              </TableCell>
              <TableCell className="text-right p-1">
                <JobActions job={job} />
              </TableCell>
            </TableRow>
          ))}
        </TableBody>
      </Table>
    </div>
  );
}

--- END FILE: ./frontend_app/components/jobs/JobTable.tsx ---

--- START FILE: ./frontend_app/components/results/RunItem.tsx (Size: 12999 bytes) ---
// File: frontend_app/components/results/RunItem.tsx
"use client";

import React, { useState } from "react";
import Link from "next/link";
import { FolderGit2, Cog, Download, ExternalLink, Loader2, AlertCircle, Settings2, ChevronDown } from "lucide-react";
import { Button } from "@/components/ui/button";
import {
  Accordion,
  AccordionContent,
  AccordionItem,
  AccordionTrigger,
} from "@/components/ui/accordion";
import { Dialog, DialogContent, DialogHeader, DialogTitle, DialogDescription, DialogFooter, DialogClose } from "@/components/ui/dialog";
import { useQuery, useMutation } from "@tanstack/react-query";
import { toast } from "sonner";
import { useFileBrowser } from "@/components/layout/FileBrowserContext";

// Import RunParameters type, remove unused JobMetaInputParams
import { ResultRun, ResultItem, RunParameters } from "@/lib/types";
import * as api from "@/lib/api";
import LoadingSpinner from "@/components/common/LoadingSpinner";
import ErrorDisplay from "@/components/common/ErrorDisplay";
import { cn } from "@/lib/utils";
import { formatDistanceToNow } from 'date-fns';
import FileList from "./FileList";

interface RunItemProps {
  run: ResultRun;
  isHighlighted: boolean;
  onExpandToggle: (runName: string, isOpening: boolean) => void;
  isExpanded: boolean;
}

// Helper function to format parameters for display
const formatParamValue = (value: string | number | boolean | null | undefined | any[] | Record<string, any>): string => {
    if (value === true) return 'Yes'; 
    if (value === false) return 'No';
    if (value === null || value === undefined) return 'N/A';
    if (typeof value === 'string' && value.trim() === '') return 'N/A';
    if (Array.isArray(value)) { return value.length > 0 ? value.join(', ') : 'N/A'; }
    if (typeof value === 'object') { return JSON.stringify(value); }
    return String(value);
}

// Helper function to format keys (make more readable)
const formatParamKey = (key: string): string => {
    const keyMap: Record<string, string> = {
        'skip_baserecalibrator': 'Skip Base Recalibration',
        'skip_qc': 'Skip QC',
        'skip_annotation': 'Skip Annotation',
        'wes': 'WES Mode',
        'joint_germline': 'Joint Germline',
        'trim_fastq': 'Trim FASTQ',
    };
    return keyMap[key] || key
        .replace(/_/g, ' ')
        .replace(/\b\w/g, l => l.toUpperCase());
}


export default function RunItem({ run, isHighlighted, onExpandToggle, isExpanded }: RunItemProps) {
  const [isParamsOpen, setIsParamsOpen] = useState(false);
  const { openFileBrowser } = useFileBrowser();

  // Query for run files (only enabled when expanded)
  const {
    data: runFiles,
    isLoading: isLoadingFiles,
    isError: isErrorFiles,
    error: errorFiles,
  } = useQuery<ResultItem[], Error>({
    queryKey: ["resultRun", run.name],
    queryFn: () => api.getResultRunFiles(run.name),
    enabled: isExpanded,
    staleTime: 5 * 60 * 1000,
    refetchOnWindowFocus: false,
  });

  // Query for parameters (only enabled when param dialog is opened)
   const {
    data: parameters,
    isLoading: isLoadingParams,
    isError: isErrorParams,
    error: errorParams,
  } = useQuery<RunParameters, Error>({ // Use RunParameters type
    queryKey: ["resultParams", run.name],
    queryFn: () => api.getResultRunParameters(run.name),
    enabled: isParamsOpen,
    staleTime: 5 * 60 * 1000,
    retry: 1,
    refetchOnWindowFocus: false,
  });

  // Mutation for downloading the run zip
  const downloadRunMutation = useMutation({
     mutationFn: (runName: string) => api.downloadResultRun(runName),
     onSuccess: (blob, runName) => {
        try {
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `${runName}.zip`;
            document.body.appendChild(a);
            a.click();
            window.URL.revokeObjectURL(url);
            document.body.removeChild(a);
            toast.success(`Started download for ${runName}.zip`);
        } catch (e) {
             console.error("Error creating download link:", e);
             toast.error("Failed to initiate download.");
        }
     },
     onError: (error: Error, runName) => {
        console.error(`Error downloading ${runName}:`, error);
        toast.error(`Download failed: ${error.message}`);
     }
  });

  const handleDownloadRun = () => {
    downloadRunMutation.mutate(run.name);
  };

  const handleOpenParams = () => {
      setIsParamsOpen(true);
  };

  const handleOpenFileBrowser = (e: React.MouseEvent) => {
    e.stopPropagation();
    if (run.filebrowser_link) {
      openFileBrowser(run.filebrowser_link);
    }
  };

  const formatTimestamp = (timestamp: number | null | undefined): string => {
    if (!timestamp) return "N/A";
    try {
        if (timestamp <= 0) return "Invalid Date";
        const date = new Date(timestamp * 1000);
        if (isNaN(date.getTime())) return "Invalid Date";
        return `${date.toLocaleDateString()} ${date.toLocaleTimeString()} (${formatDistanceToNow(date, { addSuffix: true })})`;
    } catch (e) {
        console.error("Error formatting timestamp:", timestamp, e);
        return "Invalid Date";
    }
  };

  return (
    <>
      <Accordion type="single" collapsible value={isExpanded ? run.name : ""} onValueChange={(value) => onExpandToggle(run.name, !!value)} className="[&>*]:border-b">
        <AccordionItem value={run.name} className={cn(
          "rounded-lg mb-4 overflow-hidden group/item",
          "border border-border bg-card",
          "shadow-sm last:border-b-1",
          "transition-all duration-200 hover:bg-muted/50",
          isHighlighted && "ring-4 ring-blue-500/30 ring-offset-2"
        )}>
          <div 
            className="flex items-center justify-between px-4 cursor-pointer" 
            onClick={() => onExpandToggle(run.name, !isExpanded)}
          >
            <div className="flex items-center gap-3 min-w-0 py-3">
              <FolderGit2 className="h-5 w-5 text-primary flex-shrink-0" />
              <div className="text-left min-w-0">
                <span className="font-semibold text-foreground block truncate" title={run.name}>{run.name}</span>
                <p className="text-xs text-muted-foreground block truncate">
                  Modified: {formatTimestamp(run.modified_time)}
                </p>
              </div>
            </div>

            <div className="flex items-center gap-1 py-3" onClick={e => e.stopPropagation()}>
              <Button 
                variant="ghost" 
                size="icon" 
                onClick={handleOpenParams}
                title="View Parameters" 
                className="h-7 w-7 cursor-pointer hover:bg-muted/80"
              >
                {isLoadingParams ? <Loader2 className="h-4 w-4 animate-spin"/> : <Settings2 className="h-4 w-4" />}
                <span className="sr-only">View Parameters</span>
              </Button>
              {run.filebrowser_link && (
                <Button 
                  variant="ghost" 
                  size="icon" 
                  title="Open in File Browser" 
                  className="h-7 w-7 cursor-pointer hover:bg-muted/80" 
                  onClick={handleOpenFileBrowser}
                >
                  <ExternalLink className="h-4 w-4" />
                  <span className="sr-only">Open in File Browser</span>
                </Button>
              )}
              <Button 
                variant="ghost" 
                size="icon" 
                onClick={handleDownloadRun}
                title="Download Run (.zip)" 
                disabled={downloadRunMutation.isPending} 
                className="h-7 w-7 cursor-pointer hover:bg-muted/80"
              >
                {downloadRunMutation.isPending ? <Loader2 className="h-4 w-4 animate-spin"/> : <Download className="h-4 w-4" />}
                <span className="sr-only">Download Run</span>
              </Button>
              <Button
                variant="ghost"
                size="icon"
                onClick={(e) => {
                  e.stopPropagation();
                  onExpandToggle(run.name, !isExpanded);
                }}
                className="h-7 w-7 cursor-pointer hover:bg-muted/80"
                title={isExpanded ? "Collapse" : "Expand"}
              >
                <ChevronDown 
                  className={cn(
                    "h-4 w-4 shrink-0 text-muted-foreground transition-transform duration-200",
                    isExpanded && "rotate-180"
                  )} 
                />
                <span className="sr-only">{isExpanded ? "Collapse" : "Expand"}</span>
              </Button>
            </div>
          </div>
          <AccordionContent className={cn(
            "bg-card",
            "border-t border-border",
            "overflow-hidden",
            "!border-b !border-b-border"
          )}>
             <div className="p-4">
               {isLoadingFiles && <div className="text-center p-4"><LoadingSpinner label="Loading files..." /></div>}
               {isErrorFiles && <ErrorDisplay error={errorFiles} title="Error Loading Files" />}
               {!isLoadingFiles && !isErrorFiles && runFiles && (
                 <FileList files={runFiles} runName={run.name} />
               )}
               {!isLoadingFiles && !isErrorFiles && (!runFiles || runFiles.length === 0) && (
                   <p className="text-center text-muted-foreground p-4">No files found in this run.</p>
               )}
             </div>
          </AccordionContent>
        </AccordionItem>
      </Accordion>

       {/* Parameters Dialog */}
      <Dialog open={isParamsOpen} onOpenChange={setIsParamsOpen}>
          <DialogContent className="sm:max-w-xl">
              <DialogHeader>
                  <DialogTitle>Parameters for Run: {run.name}</DialogTitle>
                  <DialogDescription>
                     Configuration used for this pipeline run.
                  </DialogDescription>
              </DialogHeader>
               {isLoadingParams && <div className="py-8 flex justify-center"><LoadingSpinner label="Loading parameters..." /></div>}
               {isErrorParams && (
                  <div className="flex items-center gap-2 text-destructive bg-destructive/10 p-3 rounded-md border border-destructive/30">
                    <AlertCircle className="h-5 w-5 mt-1 self-start flex-shrink-0"/>
                    <div>
                       <p className="font-medium">Error Loading Parameters</p>
                       <p className="text-sm">{errorParams instanceof Error ? errorParams.message : String(errorParams)}</p>
                    </div>
                 </div>
               )}
               {!isLoadingParams && !isErrorParams && parameters && (Object.keys(parameters.input_filenames || {}).length > 0 || Object.keys(parameters.sarek_params || {}).length > 0) && (
                 <div className="mt-4 max-h-[60vh] overflow-y-auto rounded-md border bg-muted/30 p-4 text-sm space-y-2">
                   {parameters.input_filenames && Object.entries(parameters.input_filenames).map(([key, value]) => (
                       <div key={`input-${key}`} className="grid grid-cols-3 gap-x-2 items-center">
                           <div className="font-medium text-muted-foreground capitalize truncate" title={key}>{formatParamKey(key)}:</div>
                           <div className="col-span-2 font-mono text-xs break-words" title={String(value ?? '')}>
                               {formatParamValue(value)}
                           </div>
                       </div>
                   ))}
                   {parameters.sarek_params && Object.entries(parameters.sarek_params).map(([key, value]) => (
                       <div key={`sarek-${key}`} className="grid grid-cols-3 gap-x-2 items-center">
                           <div className="font-medium text-muted-foreground capitalize truncate" title={key}>{formatParamKey(key)}:</div>
                           <div className="col-span-2 font-mono text-xs break-words" title={String(value ?? '')}>
                               {formatParamValue(value)}
                           </div>
                       </div>
                   ))}
                </div>
               )}
               {!isLoadingParams && !isErrorParams && (!parameters || (Object.keys(parameters.input_filenames || {}).length === 0 && Object.keys(parameters.sarek_params || {}).length === 0)) && (
                   <p className="text-muted-foreground text-sm text-center py-4">No parameter information found for this run.</p>
               )}
              <DialogFooter className="mt-4">
                  <DialogClose asChild>
                      <Button type="button" variant="outline">Close</Button>
                  </DialogClose>
              </DialogFooter>
          </DialogContent>
      </Dialog>
    </>
  );
}

--- END FILE: ./frontend_app/components/results/RunItem.tsx ---

--- START FILE: ./frontend_app/components/results/FileList.tsx (Size: 10627 bytes) ---
// File: frontend_app/components/results/FileList.tsx
"use client";

import React from "react";
import Link from 'next/link'; // Import Link
import { useMutation } from "@tanstack/react-query";
import { toast } from "sonner";
import { Folder, FileText, FileCode2, BarChart3, AlignLeft, Download, ExternalLink, Loader2 } from "lucide-react"; // Added Loader2

import { ResultItem } from "@/lib/types";
import * as api from "@/lib/api"; // Import API functions
import { Button } from "@/components/ui/button";
import { formatBytes } from "@/lib/utils"; // Assuming a formatBytes utility exists

interface FileListProps {
  files: ResultItem[];
  runName: string; // Receive runName to construct download links
}

// Sarek-specific file recognition
const getFileIcon = (file: ResultItem) => {
    if (file.is_dir) return <Folder className="h-4 w-4 text-yellow-600 flex-shrink-0" />;

    const ext = file.extension?.toLowerCase();
    const name = file.name.toLowerCase();

    // Prioritize specific filenames/patterns
    if (name === 'multiqc_report.html') return <BarChart3 className="h-4 w-4 text-teal-500 flex-shrink-0" />;
    if (name.includes('fastqc.html')) return <BarChart3 className="h-4 w-4 text-orange-500 flex-shrink-0" />;
    if (name === 'job_metadata.json') return <FileCode2 className="h-4 w-4 text-indigo-500 flex-shrink-0" />;

    // General extensions
    switch (ext) {
        case ".bam":
        case ".cram":
            return <AlignLeft className="h-4 w-4 text-blue-600 flex-shrink-0" />;
        case ".bai":
        case ".crai":
            return <AlignLeft className="h-4 w-4 text-blue-400 flex-shrink-0" />; // Lighter for index
        case ".vcf":
        case ".bcf":
        case ".gz": // Often VCFs are gzipped
        case ".bgz":
        case ".csi":
        case ".tbi": // Index for VCF
            return <FileCode2 className="h-4 w-4 text-purple-600 flex-shrink-0" />;
        case ".html":
            return <BarChart3 className="h-4 w-4 text-green-600 flex-shrink-0" />;
        case ".log":
        case ".txt":
        case ".out":
        case ".err":
        case ".report":
        case ".metrics":
            return <FileText className="h-4 w-4 text-gray-600 flex-shrink-0" />;
        case ".csv":
        case ".tsv":
            return <FileText className="h-4 w-4 text-lime-600 flex-shrink-0" />; // Different color for tables
        case ".json":
        case ".yaml":
        case ".yml":
            return <FileCode2 className="h-4 w-4 text-pink-600 flex-shrink-0" />;
        case ".bed":
            return <FileCode2 className="h-4 w-4 text-rose-600 flex-shrink-0" />;
        case ".fa":
        case ".fasta":
            return <FileCode2 className="h-4 w-4 text-sky-600 flex-shrink-0" />;
        default:
            return <FileText className="h-4 w-4 text-gray-500 flex-shrink-0" />;
    }
};

const formatTimestamp = (timestamp: number | null | undefined): string => {
    if (!timestamp) return "N/A";
    try {
        return new Date(timestamp * 1000).toLocaleString();
    } catch (e) {
        return "Invalid Date";
    }
};

// Component for individual file actions (download/view)
interface FileActionsProps {
  file: ResultItem;
  runName: string;
}

function FileActions({ file, runName }: FileActionsProps) {
    const downloadFileMutation = useMutation({
        mutationFn: () => api.downloadResultFile(runName, file.relative_path || file.name), // Use relative path if available
        onSuccess: (blob) => {
            try {
                const url = window.URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = file.name; // Use the actual filename
                document.body.appendChild(a);
                a.click();
                window.URL.revokeObjectURL(url);
                document.body.removeChild(a);
                toast.success(`Started download for ${file.name}`);
            } catch (e) {
                 console.error("Error creating file download link:", e);
                 toast.error("Failed to initiate file download.");
            }
        },
        onError: (error: Error) => {
             console.error(`Error downloading file ${file.name}:`, error);
            toast.error(`Download failed for ${file.name}: ${error.message}`);
        }
    });

    const handleDownload = (e: React.MouseEvent) => {
        e.stopPropagation(); // Prevent triggering parent elements if nested
        downloadFileMutation.mutate();
    };

    return (
        <div className="flex items-center gap-1">
            {/* Download Button */}
            {!file.is_dir && (
                 <Button
                    variant="ghost"
                    size="icon"
                    className="h-6 w-6"
                    title={`Download ${file.name}`}
                    onClick={handleDownload}
                    disabled={downloadFileMutation.isPending}
                >
                    {downloadFileMutation.isPending ? <Loader2 className="h-3 w-3 animate-spin"/> : <Download className="h-3 w-3" />}
                    <span className="sr-only">Download</span>
                 </Button>
            )}
             {/* View in FileBrowser Button */}
            {file.filebrowser_link && (
                <Button variant="ghost" size="icon" className="h-6 w-6" title="Open in File Browser" asChild>
                    <Link href={file.filebrowser_link} target="_blank" rel="noopener noreferrer" onClick={(e) => e.stopPropagation()}>
                        <ExternalLink className="h-3 w-3" />
                         <span className="sr-only">Open in File Browser</span>
                    </Link>
                </Button>
            )}
        </div>
    );
}


export default function FileList({ files, runName }: FileListProps) {

  // Sarek-specific categorization logic
  const categorizedFiles = React.useMemo(() => {
      const categories = {
          qcReports: [] as ResultItem[],
          variants: [] as ResultItem[],
          alignment: [] as ResultItem[],
          annotation: [] as ResultItem[],
          logsInfo: [] as ResultItem[],
          otherFiles: [] as ResultItem[],
          directories: [] as ResultItem[],
      };

      files.forEach(file => {
          if (file.is_dir) {
              // Group common Sarek output directories
              if (['VariantCalling', 'Annotation', 'Preprocessing', 'QC', 'pipeline_info', 'Reports'].includes(file.name)) {
                   categories.directories.push(file);
              } else {
                   categories.otherFiles.push(file); // Treat unknown dirs as 'other' for now
              }
              return;
          }

          const name = file.name.toLowerCase();
          const ext = file.extension?.toLowerCase();
          const path = file.relative_path?.toLowerCase() || name; // Use path if available

          // Prioritize specific files
          if (name === 'multiqc_report.html' || path.includes('multiqc/')) {
              categories.qcReports.push(file);
          } else if (name.endsWith('fastqc.html') || path.includes('fastqc/')) {
              categories.qcReports.push(file);
          } else if (ext === '.vcf' || name.endsWith('.vcf.gz') || ext === '.bcf' || ext === '.tbi' || ext === '.csi' || path.includes('variantcalling/') || path.includes('variants/')) {
              categories.variants.push(file);
          } else if (ext === '.bam' || ext === '.cram' || ext === '.bai' || ext === '.crai' || path.includes('preprocessing/')) {
              categories.alignment.push(file);
          } else if (path.includes('annotation/')) {
               categories.annotation.push(file);
          } else if (ext === '.log' || name.endsWith('.out') || name.endsWith('.err') || path.includes('pipeline_info/')) {
              categories.logsInfo.push(file);
          }
          // Removed QC file extensions from logsInfo check to avoid duplication with qcReports
          else if (!path.includes('multiqc/') && !path.includes('fastqc/') && (ext === '.html' || ext === '.qc' || ext === '.metrics' || ext === '.report' || ext === '.json')) {
              categories.qcReports.push(file);
          }
           else {
              // Only add if not already categorized
               const alreadyCategorized = Object.values(categories).flat().some(f => f.name === file.name && f.relative_path === file.relative_path);
               if (!alreadyCategorized) {
                    categories.otherFiles.push(file);
               }
          }
      });

      // Sort directories alphabetically
      categories.directories.sort((a, b) => a.name.localeCompare(b.name));

      return categories;
  }, [files]);


  const renderFileItem = (file: ResultItem) => (
     <div key={file.relative_path || file.name} className="flex items-center justify-between p-2 hover:bg-accent/50 rounded-md text-sm group">
        <div className="flex items-center gap-2 truncate mr-2 flex-grow min-w-0">
            {getFileIcon(file)}
            <span className="truncate" title={file.name}>{file.name}</span>
        </div>
        <div className="flex items-center gap-2 flex-shrink-0 text-xs text-muted-foreground ml-2">
             {!file.is_dir && <span>{formatBytes(file.size ?? 0)}</span>}
             <span className="hidden sm:inline">{formatTimestamp(file.modified_time)}</span>
             {/* Add Download/View actions here */}
             <div className="opacity-0 group-hover:opacity-100 transition-opacity">
                 <FileActions file={file} runName={runName} />
             </div>
        </div>
    </div>
  );

  const renderCategory = (title: string, items: ResultItem[]) => {
      if (items.length === 0) return null;
      return (
          <div>
              <h4 className="font-semibold mb-1 text-muted-foreground px-2">{title}</h4>
              <div className="space-y-1 border rounded-md p-1 bg-muted/20">
                  {items.map(renderFileItem)}
              </div>
          </div>
      );
  };

  return (
    <div className="space-y-4">
        {renderCategory("QC Reports", categorizedFiles.qcReports)}
        {renderCategory("Variant Calling", categorizedFiles.variants)}
        {renderCategory("Alignment", categorizedFiles.alignment)}
        {renderCategory("Annotation", categorizedFiles.annotation)}
        {renderCategory("Directories", categorizedFiles.directories)}
        {renderCategory("Logs & Info", categorizedFiles.logsInfo)}
        {renderCategory("Other Files", categorizedFiles.otherFiles)}
    </div>
  );
}

--- END FILE: ./frontend_app/components/results/FileList.tsx ---

--- START FILE: ./frontend_app/components/layout/Footer.tsx (Size: 625 bytes) ---
// File: frontend_app/components/layout/Footer.tsx
import { cn } from "@/lib/utils";

export default function Footer() {
  const currentYear = new Date().getFullYear();

  return (
    // Removed mt-auto, added mt-16 for spacing inside scrollable area
    <footer className={cn(
        "bg-muted text-muted-foreground text-center py-4 border-t border-border",
        "mt-16" // Space above footer within the scrollable div
    )}>
      <div className="container mx-auto">
        <p className="text-sm">
           {currentYear} Bioinformatics Webapp. All rights reserved.
        </p>
      </div>
    </footer>
  );
}

--- END FILE: ./frontend_app/components/layout/Footer.tsx ---

--- START FILE: ./frontend_app/components/layout/FileBrowserContext.tsx (Size: 1119 bytes) ---
"use client";

import React, { createContext, useContext, useState } from 'react';

interface FileBrowserContextType {
  isOpen: boolean;
  currentPath: string | null;
  openFileBrowser: (path: string) => void;
  closeFileBrowser: () => void;
}

const FileBrowserContext = createContext<FileBrowserContextType | undefined>(undefined);

export function FileBrowserProvider({ children }: { children: React.ReactNode }) {
  const [isOpen, setIsOpen] = useState(false);
  const [currentPath, setCurrentPath] = useState<string | null>(null);

  const openFileBrowser = (path: string) => {
    setCurrentPath(path);
    setIsOpen(true);
  };

  const closeFileBrowser = () => {
    setIsOpen(false);
    setCurrentPath(null);
  };

  return (
    <FileBrowserContext.Provider value={{ isOpen, currentPath, openFileBrowser, closeFileBrowser }}>
      {children}
    </FileBrowserContext.Provider>
  );
}

export function useFileBrowser() {
  const context = useContext(FileBrowserContext);
  if (context === undefined) {
    throw new Error('useFileBrowser must be used within a FileBrowserProvider');
  }
  return context;
} 
--- END FILE: ./frontend_app/components/layout/FileBrowserContext.tsx ---

--- START FILE: ./frontend_app/components/layout/FileBrowserIntegration.tsx (Size: 4762 bytes) ---
// File: frontend_app/components/layout/FileBrowserIntegration.tsx
"use client";

import React, { useState, useEffect } from "react";
// Import both icons
import { FolderClosed, FolderOpen } from "lucide-react";
import { Button } from "@/components/ui/button";
import {
  Dialog,
  DialogContent,
  DialogHeader,
  DialogTitle,
} from "@/components/ui/dialog";
import { cn } from "@/lib/utils";
import { useFileBrowser } from "./FileBrowserContext";

export default function FileBrowserIntegration() {
  const [fileBrowserUrl, setFileBrowserUrl] = useState<string | null>(null);
  const [iframeKey, setIframeKey] = useState<number>(0);
  const { isOpen, currentPath, closeFileBrowser, openFileBrowser } = useFileBrowser();

  useEffect(() => {
    // Fetch URL from env var only on the client-side
    const url = process.env.NEXT_PUBLIC_FILEBROWSER_URL;
    if (url) {
      // Basic validation/cleanup (remove trailing slash)
      setFileBrowserUrl(url.replace(/\/$/, ""));
    } else {
      console.warn("NEXT_PUBLIC_FILEBROWSER_URL is not set.");
    }
  }, []);

  const handleToggle = () => {
    // Increment key to force reload when opening
    setIframeKey(prev => prev + 1);
    
    if (isOpen) {
      closeFileBrowser();
    } else {
      // Open with root path when clicking the floating button
      openFileBrowser('/filebrowser/files');
    }
  };

  // Construct the iframe URL based on the current path
  const getIframeUrl = () => {
    if (!fileBrowserUrl) return null;
    
    // If we have a currentPath, it's already in the format /filebrowser/files/results/...
    // We just need to append it to the base URL
    if (currentPath) {
      // Remove any leading slash from currentPath to avoid double slashes
      const cleanPath = currentPath.startsWith('/') ? currentPath.slice(1) : currentPath;
      return `${fileBrowserUrl}/${cleanPath}`;
    }
    
    return fileBrowserUrl;
  };

  // Use Dialog component for overlay and content management
  return (
    <>
      {/* Floating Button */}
      <Button
        variant="secondary" // Keep the base variant for initial styling
        size="icon"
        className={cn(
            "fixed bottom-5 left-5 z-40 h-14 w-14 rounded-full shadow-lg",
            // Add group utility for targeting child icons on hover
            "group",
            // Ensure cursor is pointer unless disabled
            "cursor-pointer",
            // Add border and set its color using the theme variable
            "border border-border",
            // Override the default hover background change for secondary variant
            // Apply the non-hover secondary background color even on hover
            "hover:bg-secondary"
        )}
        onClick={handleToggle}
        aria-label="Toggle File Browser"
        disabled={!fileBrowserUrl} // Disable if URL is not set
      >
        {/* Default Icon: Visible normally, hidden on group hover */}
        <FolderClosed className={cn(
            "h-8 w-8", // Increased icon size
            "block group-hover:hidden transition-opacity duration-150" // Show by default, hide on hover
            )}
        />
        {/* Hover Icon: Hidden normally, visible on group hover */}
        <FolderOpen className={cn(
            "h-8 w-8", // Increased icon size
            "hidden group-hover:block transition-opacity duration-150" // Hide by default, show on hover
            )}
        />
      </Button>

      {/* Dialog for Modal */}
      <Dialog open={isOpen} onOpenChange={closeFileBrowser}>
        <DialogContent
          className={cn(
            "p-0 gap-0 sm:max-w-[90vw] h-[85vh] flex flex-col", // Adjust size as needed
            // Remove default padding and make flex column
          )}
          onInteractOutside={(e) => {
            // Prevent closing when clicking inside the iframe itself
            if ((e.target as HTMLElement)?.closest('iframe')) {
              e.preventDefault();
            }
          }}
        >
          <DialogHeader className="p-4 border-b">
            <DialogTitle>File Browser</DialogTitle>
             {/* DialogClose is automatically handled by Dialog component's X */}
          </DialogHeader>
          <div className="flex-grow overflow-hidden p-1"> {/* Container for iframe */}
            {fileBrowserUrl && (
              <iframe
                key={iframeKey} // Use key to force reload
                src={getIframeUrl() || fileBrowserUrl}
                title="File Browser"
                className="w-full h-full border-0"
                // sandbox="allow-scripts allow-same-origin allow-forms allow-popups" // Consider security implications
              />
            )}
          </div>
        </DialogContent>
      </Dialog>
    </>
  );
}

--- END FILE: ./frontend_app/components/layout/FileBrowserIntegration.tsx ---

--- START FILE: ./frontend_app/components/layout/Navbar.tsx (Size: 1527 bytes) ---
// File: frontend_app/components/layout/Navbar.tsx
"use client";

import Link from "next/link";
import { usePathname } from "next/navigation";
import { cn } from "@/lib/utils";

const navItems = [
  { href: "/", label: "Home" },
  { href: "/input", label: "Input" },
  { href: "/jobs", label: "Jobs" },
  { href: "/results", label: "Results" },
];

export default function Navbar() {
  const pathname = usePathname();

  return (
    // *** Ensure sticky styles are present ***
    <nav className="sticky top-0 z-50 bg-muted text-muted-foreground p-4 shadow-md border-b border-border">
      <div className="container mx-auto flex justify-start items-center gap-8">
        <Link href="/" className="text-xl font-bold text-foreground hover:text-primary transition-colors">
          BioPipeline UI
        </Link>
        <ul className="flex space-x-6 items-center">
          {navItems.map((item) => (
            <li key={item.href}>
              <Link
                href={item.href}
                className={cn(
                  "text-sm hover:text-primary transition-colors pb-1",
                  pathname === item.href
                    ? "border-b-2 border-primary font-semibold text-primary"
                    : "border-b-2 border-transparent text-muted-foreground"
                )}
              >
                {item.label}
              </Link>
            </li>
          ))}
        </ul>
        <div className="ml-auto">
          {/* Placeholder */}
        </div>
      </div>
    </nav>
  );
}

--- END FILE: ./frontend_app/components/layout/Navbar.tsx ---

--- START FILE: ./frontend_app/components/forms/FileSelector.tsx (Size: 4269 bytes) ---
// frontend_app/components/forms/FileSelector.tsx
"use client";

import React, { useState } from "react";
import { useQuery } from "@tanstack/react-query";
import { Check, ChevronsUpDown } from "lucide-react";
import { cn } from "@/lib/utils";
import { Button } from "@/components/ui/button";
import {
  Command,
  CommandEmpty,
  CommandGroup,
  CommandInput,
  CommandItem,
  CommandList,
} from "@/components/ui/command";
import {
  Popover,
  PopoverContent,
  PopoverTrigger,
} from "@/components/ui/popover";
import { Skeleton } from "@/components/ui/skeleton";
import * as api from "@/lib/api";
import { DataFile } from "@/lib/types";

interface FileSelectorProps {
  fileTypeLabel: string;
  fileType: string;
  extensions?: string[];
  value: string | undefined;
  onChange: (value: string | undefined) => void;
  placeholder?: string;
  allowNone?: boolean;
  required?: boolean;
  disabled?: boolean;
}

export default function FileSelector({
  fileTypeLabel,
  fileType,
  extensions,
  value,
  onChange,
  placeholder = "Select a file...",
  allowNone = false,
  required = false,
  disabled = false,
}: FileSelectorProps) {

  const [open, setOpen] = useState(false);

  const { data: files, isLoading, isError } = useQuery<DataFile[], Error>({
    queryKey: ["dataFiles", fileType, extensions?.join(',')],
    queryFn: () => api.getDataFiles(fileType, extensions),
    enabled: !disabled,
    staleTime: 5 * 60 * 1000,
  });

  const handleSelect = (selectedValue: string) => {
    const newValue = selectedValue === value ? undefined : selectedValue === "##NONE##" ? undefined : selectedValue;
    onChange(newValue);
    setOpen(false);
  };

  const currentFile = files?.find((file) => file.name === value);
  const displayValue = currentFile?.name ?? (allowNone && !value ? "None" : (value || placeholder));


  if (isLoading) {
     return <Skeleton className="h-9 w-full" />;
  }

  if (isError) {
      return (
          <Button variant="outline" disabled className="w-full justify-start font-normal text-destructive">
              Error loading files for {fileTypeLabel}
          </Button>
      );
  }

  return (
    <Popover open={open} onOpenChange={setOpen}>
      <PopoverTrigger asChild>
        <Button
          variant="outline"
          role="combobox"
          aria-expanded={open}
          className="w-full justify-between font-normal cursor-pointer"
          disabled={disabled || isLoading}
        >
          <span className="truncate">{displayValue}</span>
          <ChevronsUpDown className="ml-2 h-4 w-4 shrink-0 opacity-50" />
        </Button>
      </PopoverTrigger>
      <PopoverContent
         className="w-[var(--radix-popover-trigger-width)] p-0 z-50"
         align="start"
         sideOffset={5}
      >
        <Command className="w-full">
          <CommandInput placeholder={`Search ${fileTypeLabel}...`} className="w-full" />
           <CommandList className="w-full">
              <CommandEmpty>No files found.</CommandEmpty>
              <CommandGroup>
                {allowNone && (
                    <CommandItem
                        key="##NONE##"
                        value="##NONE##"
                        onSelect={() => handleSelect("##NONE##")}
                    >
                    <Check
                        className={cn(
                        "mr-2 h-4 w-4",
                        !value ? "opacity-100" : "opacity-0"
                        )}
                    />
                    None
                    </CommandItem>
                )}
                {files && files.map((file) => (
                  <CommandItem
                    key={file.name}
                    value={file.name}
                    onSelect={() => handleSelect(file.name)}
                    className="cursor-pointer"
                  >
                    <Check
                      className={cn(
                        "mr-2 h-4 w-4",
                        value === file.name ? "opacity-100" : "opacity-0"
                      )}
                    />
                    {file.name}
                  </CommandItem>
                ))}
              </CommandGroup>
           </CommandList>
        </Command>
      </PopoverContent>
    </Popover>
  );
}

--- END FILE: ./frontend_app/components/forms/FileSelector.tsx ---

--- START FILE: ./frontend_app/components/forms/SampleInputGroup.tsx (Size: 7425 bytes) ---
// File: frontend_app/components/forms/SampleInputGroup.tsx
"use client";

import React from "react";
import { Trash2 } from "lucide-react";
// *** Make absolutely sure this import is present and correct ***
import { Card, CardContent } from "@/components/ui/card";
import { Input } from "@/components/ui/input";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select";
import { Button } from "@/components/ui/button";
import FileSelector from "@/components/forms/FileSelector";
import { FormField, FormItem, FormLabel, FormControl, FormMessage, FormDescription } from "@/components/ui/form";
import { cn } from "@/lib/utils";
import { Control } from "react-hook-form"; // Import Control type

interface SampleInputGroupProps {
  index: number;
  remove: (index: number) => void;
  control: Control<any>; // Use the passed control prop
}

export default function SampleInputGroup({ index, remove, control }: SampleInputGroupProps) {
  // No useFormContext needed here

  return (
    // The Card component causing the error
    <Card className={cn(
        "relative border border-border pt-8",
        "isolate",
        "bg-muted/10 dark:bg-muted/20"
        )}>
      <Button
          variant="ghost"
          size="icon"
          className="absolute top-2 right-2 text-muted-foreground hover:text-destructive cursor-pointer"
          onClick={() => remove(index)}
          type="button"
      >
          <Trash2 className="h-4 w-4" />
          <span className="sr-only">Remove Sample</span>
      </Button>

      {/* Use CardContent */}
      <CardContent className="grid grid-cols-1 sm:grid-cols-2 gap-4 pt-4">
        {/* Add cursor-default to these non-interactive labels */}
        <FormField control={control} name={`samples.${index}.patient`} render={({ field }) => ( 
          <FormItem>
            <div className="flex justify-between items-center">
              <FormLabel className="cursor-default">Patient ID</FormLabel>
              <FormMessage className="text-xs" />
            </div>
            <FormControl>
              <Input placeholder="e.g., Patient_A" {...field} />
            </FormControl>
          </FormItem>
        )} />
        <FormField control={control} name={`samples.${index}.sample`} render={({ field }) => ( 
          <FormItem>
            <div className="flex justify-between items-center">
              <FormLabel className="cursor-default">Sample ID</FormLabel>
              <FormMessage className="text-xs" />
            </div>
            <FormControl>
              <Input placeholder="e.g., Sample_A_T" {...field} />
            </FormControl>
          </FormItem>
        )} />
        <FormField control={control} name={`samples.${index}.sex`} render={({ field }) => ( 
          <FormItem>
            <div className="flex justify-between items-center">
              <FormLabel className="cursor-default">Sex</FormLabel>
              <FormMessage className="text-xs" />
            </div>
            <Select onValueChange={field.onChange} defaultValue={field.value} value={field.value}>
              <FormControl>
                <SelectTrigger>
                  <SelectValue placeholder="Select sex" />
                </SelectTrigger>
              </FormControl>
              <SelectContent position="popper" sideOffset={5}>
                <SelectItem value="XX">XX</SelectItem>
                <SelectItem value="XY">XY</SelectItem>
                <SelectItem value="X">X</SelectItem>
                <SelectItem value="Y">Y</SelectItem>
                <SelectItem value="other">Other</SelectItem>
              </SelectContent>
            </Select>
          </FormItem>
        )} />
        <FormField control={control} name={`samples.${index}.status`} render={({ field }) => ( 
          <FormItem>
            <div className="flex justify-between items-center">
              <FormLabel className="cursor-default">Status</FormLabel>
              <FormMessage className="text-xs" />
            </div>
            <Select onValueChange={(val) => field.onChange(parseInt(val, 10))} defaultValue={String(field.value ?? '')} value={String(field.value ?? '')}>
              <FormControl>
                <SelectTrigger>
                  <SelectValue placeholder="Select status" />
                </SelectTrigger>
              </FormControl>
              <SelectContent position="popper" sideOffset={5}>
                <SelectItem value="0">Normal (0)</SelectItem>
                <SelectItem value="1">Tumor (1)</SelectItem>
              </SelectContent>
            </Select>
          </FormItem>
        )} />
        <div className="sm:col-span-1">
          <FormField control={control} name={`samples.${index}.lane`} render={({ field }) => ( 
            <FormItem>
              <div className="flex justify-between items-center">
                <FormLabel className="cursor-default">Lane</FormLabel>
                <FormMessage className="text-xs" />
              </div>
              <FormControl>
                <Input 
                  placeholder="e.g., L001" 
                  {...field} 
                  value={field.value || ''}
                  onChange={(e) => {
                    // Convert to uppercase and remove spaces
                    const value = e.target.value.toUpperCase().replace(/\s+/g, '');
                    field.onChange(value);
                  }}
                />
              </FormControl>
              <FormDescription className="text-xs italic">
                Lane identifier (e.g., L001, L002)
              </FormDescription>
            </FormItem>
          )} />
        </div>
        <div className="sm:col-span-2 grid grid-cols-2 gap-4">
          <FormField control={control} name={`samples.${index}.fastq_1`} render={({ field }) => ( 
            <FormItem>
              <div className="flex justify-between items-center">
                <FormLabel className="cursor-default">FASTQ Read 1</FormLabel>
                <FormMessage className="text-xs" />
              </div>
              <FormControl>
                <FileSelector 
                  fileTypeLabel="FASTQ R1" 
                  fileType="fastq" 
                  extensions={[".fastq.gz", ".fq.gz", ".fastq", ".fq"]} 
                  value={field.value} 
                  onChange={field.onChange} 
                  placeholder="Select R1 FASTQ..." 
                  required 
                />
              </FormControl>
            </FormItem>
          )} />
          <FormField control={control} name={`samples.${index}.fastq_2`} render={({ field }) => ( 
            <FormItem>
              <div className="flex justify-between items-center">
                <FormLabel className="cursor-default">FASTQ Read 2</FormLabel>
                <FormMessage className="text-xs" />
              </div>
              <FormControl>
                <FileSelector 
                  fileTypeLabel="FASTQ R2" 
                  fileType="fastq" 
                  extensions={[".fastq.gz", ".fq.gz", ".fastq", ".fq"]} 
                  value={field.value} 
                  onChange={field.onChange} 
                  placeholder="Select R2 FASTQ..." 
                  required 
                />
              </FormControl>
            </FormItem>
          )} />
        </div>
      </CardContent>
    </Card>
  );
}

--- END FILE: ./frontend_app/components/forms/SampleInputGroup.tsx ---

--- START FILE: ./frontend_app/components/common/LoadingSpinner.tsx (Size: 701 bytes) ---
// File: frontend_app/components/common/LoadingSpinner.tsx
import { Loader2 } from "lucide-react";
import { cn } from "@/lib/utils";

interface LoadingSpinnerProps {
  size?: "sm" | "md" | "lg";
  className?: string;
  label?: string;
}

export default function LoadingSpinner({ size = "md", className, label }: LoadingSpinnerProps) {
  const sizeClasses = {
    sm: "h-4 w-4",
    md: "h-8 w-8",
    lg: "h-12 w-12",
  };

  return (
    <div className={cn("flex flex-col items-center justify-center gap-2", className)}>
      <Loader2 className={cn("animate-spin text-primary", sizeClasses[size])} />
      {label && <span className="text-sm text-muted-foreground">{label}</span>}
    </div>
  );
}

--- END FILE: ./frontend_app/components/common/LoadingSpinner.tsx ---

--- START FILE: ./frontend_app/components/common/ErrorDisplay.tsx (Size: 798 bytes) ---
// File: frontend_app/components/common/ErrorDisplay.tsx
import { AlertCircle } from "lucide-react";
import { Alert, AlertDescription, AlertTitle } from "@/components/ui/alert";
import { cn } from "@/lib/utils";

interface ErrorDisplayProps {
  error: Error | string | null | undefined;
  title?: string;
  className?: string;
}

export default function ErrorDisplay({
  error,
  title = "An Error Occurred",
  className,
}: ErrorDisplayProps) {
  if (!error) return null;

  const errorMessage = typeof error === "string" ? error : error.message;

  return (
    <Alert variant="destructive" className={cn(className)}>
      <AlertCircle className="h-4 w-4" />
      <AlertTitle>{title}</AlertTitle>
      <AlertDescription>{errorMessage || "Unknown error"}</AlertDescription>
    </Alert>
  );
}

--- END FILE: ./frontend_app/components/common/ErrorDisplay.tsx ---

--- START FILE: ./frontend_app/components/providers/ThemeProvider.tsx (Size: 451 bytes) ---
// File: frontend_app/components/providers/ThemeProvider.tsx
"use client"

import * as React from "react"
import { ThemeProvider as NextThemesProvider } from "next-themes"
// Import type directly from the package root
import { type ThemeProviderProps } from "next-themes" // <-- CHANGED IMPORT PATH

export function ThemeProvider({ children, ...props }: ThemeProviderProps) {
  return <NextThemesProvider {...props}>{children}</NextThemesProvider>
}


--- END FILE: ./frontend_app/components/providers/ThemeProvider.tsx ---

--- START FILE: ./frontend_app/components/providers/QueryProvider.tsx (Size: 1324 bytes) ---
// File: frontend_app/components/providers/QueryProvider.tsx
"use client"; // This component uses useState, so it must be a Client Component

import React, { useState } from "react";
import { QueryClient, QueryClientProvider } from "@tanstack/react-query";
// Optional: Import React Query DevTools for development
import { ReactQueryDevtools } from "@tanstack/react-query-devtools";

export default function QueryProvider({
  children,
}: {
  children: React.ReactNode;
}) {
  // Create the QueryClient instance *once* using useState
  // This prevents creating a new client on every render
  const [queryClient] = useState(
    () =>
      new QueryClient({
        defaultOptions: {
          queries: {
            // Default query options can go here, e.g.:
            staleTime: 1000 * 60 * 5, // Data is considered fresh for 5 minutes
            refetchOnWindowFocus: false, // Optional: disable refetching on window focus
            retry: 1, // Optional: retry failed queries once
          },
        },
      })
  );

  return (
    // Provide the client to the rest of your app
    <QueryClientProvider client={queryClient}>
      {children}
      {/* Optional: Add React Query DevTools for easy debugging in development */}
      <ReactQueryDevtools initialIsOpen={false} />
    </QueryClientProvider>
  );
}

--- END FILE: ./frontend_app/components/providers/QueryProvider.tsx ---

--- START FILE: ./frontend_app/components/ui/input.tsx (Size: 967 bytes) ---
import * as React from "react"

import { cn } from "@/lib/utils"

function Input({ className, type, ...props }: React.ComponentProps<"input">) {
  return (
    <input
      type={type}
      data-slot="input"
      className={cn(
        "file:text-foreground placeholder:text-muted-foreground selection:bg-primary selection:text-primary-foreground dark:bg-input/30 border-input flex h-9 w-full min-w-0 rounded-md border bg-transparent px-3 py-1 text-base shadow-xs transition-[color,box-shadow] outline-none file:inline-flex file:h-7 file:border-0 file:bg-transparent file:text-sm file:font-medium disabled:pointer-events-none disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",
        "focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px]",
        "aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
        className
      )}
      {...props}
    />
  )
}

export { Input }

--- END FILE: ./frontend_app/components/ui/input.tsx ---

--- START FILE: ./frontend_app/components/ui/command.tsx (Size: 4840 bytes) ---
// File: frontend_app/components/ui/command.tsx
"use client"

import * as React from "react"
import { Command as CommandPrimitive } from "cmdk"
import { SearchIcon } from "lucide-react"

import { cn } from "@/lib/utils"
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogHeader,
  DialogTitle,
} from "@/components/ui/dialog"

function Command({
  className,
  ...props
}: React.ComponentProps<typeof CommandPrimitive>) {
  return (
    <CommandPrimitive
      data-slot="command"
      className={cn(
        "bg-popover text-popover-foreground flex h-full w-full flex-col overflow-hidden rounded-md",
        className
      )}
      {...props}
    />
  )
}

function CommandDialog({
  title = "Command Palette",
  description = "Search for a command to run...",
  children,
  ...props
}: React.ComponentProps<typeof Dialog> & {
  title?: string
  description?: string
}) {
  return (
    <Dialog {...props}>
      <DialogHeader className="sr-only">
        <DialogTitle>{title}</DialogTitle>
        <DialogDescription>{description}</DialogDescription>
      </DialogHeader>
      <DialogContent className="overflow-hidden p-0">
        <Command className="[&_[cmdk-group-heading]]:text-muted-foreground **:data-[slot=command-input-wrapper]:h-12 [&_[cmdk-group-heading]]:px-2 [&_[cmdk-group-heading]]:font-medium [&_[cmdk-group]]:px-2 [&_[cmdk-group]:not([hidden])_~[cmdk-group]]:pt-0 [&_[cmdk-input-wrapper]_svg]:h-5 [&_[cmdk-input-wrapper]_svg]:w-5 [&_[cmdk-input]]:h-12 [&_[cmdk-item]]:px-2 [&_[cmdk-item]]:py-3 [&_[cmdk-item]_svg]:h-5 [&_[cmdk-item]_svg]:w-5">
          {children}
        </Command>
      </DialogContent>
    </Dialog>
  )
}

function CommandInput({
  className,
  ...props
}: React.ComponentProps<typeof CommandPrimitive.Input>) {
  return (
    <div
      data-slot="command-input-wrapper"
      className="flex h-9 items-center gap-2 border-b px-3"
    >
      <SearchIcon className="size-4 shrink-0 opacity-50" />
      <CommandPrimitive.Input
        data-slot="command-input"
        className={cn(
          "placeholder:text-muted-foreground flex h-10 w-full rounded-md bg-transparent py-3 text-sm outline-none disabled:cursor-not-allowed disabled:opacity-50", // Corrected outline class
          className
        )}
        {...props}
      />
    </div>
  )
}

function CommandList({
  className,
  ...props
}: React.ComponentProps<typeof CommandPrimitive.List>) {
  return (
    <CommandPrimitive.List
      data-slot="command-list"
      className={cn(
        "max-h-[300px] scroll-py-1 overflow-x-hidden overflow-y-auto",
        className
      )}
      {...props}
    />
  )
}

function CommandEmpty({
  ...props
}: React.ComponentProps<typeof CommandPrimitive.Empty>) {
  return (
    <CommandPrimitive.Empty
      data-slot="command-empty"
      className="py-6 text-center text-sm"
      {...props}
    />
  )
}

function CommandGroup({
  className,
  ...props
}: React.ComponentProps<typeof CommandPrimitive.Group>) {
  return (
    <CommandPrimitive.Group
      data-slot="command-group"
      className={cn(
        "text-foreground [&_[cmdk-group-heading]]:text-muted-foreground overflow-hidden p-1 [&_[cmdk-group-heading]]:px-2 [&_[cmdk-group-heading]]:py-1.5 [&_[cmdk-group-heading]]:text-xs [&_[cmdk-group-heading]]:font-medium",
        className
      )}
      {...props}
    />
  )
}

function CommandSeparator({
  className,
  ...props
}: React.ComponentProps<typeof CommandPrimitive.Separator>) {
  return (
    <CommandPrimitive.Separator
      data-slot="command-separator"
      className={cn("bg-border -mx-1 h-px", className)}
      {...props}
    />
  )
}

function CommandItem({
  className,
  ...props
}: React.ComponentProps<typeof CommandPrimitive.Item>) {
  return (
    <CommandPrimitive.Item
      data-slot="command-item"
      className={cn(
        "data-[selected=true]:bg-accent data-[selected=true]:text-accent-foreground [&_svg:not([class*='text-'])]:text-muted-foreground relative flex items-center gap-2 rounded-sm px-2 py-1.5 text-sm outline-none select-none aria-selected:bg-accent aria-selected:text-accent-foreground",
        "cursor-pointer hover:bg-accent/50",
        "data-[disabled=true]:pointer-events-none data-[disabled=true]:opacity-50",
        "[&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
        className
      )}
      {...props}
    />
  )
}

function CommandShortcut({
  className,
  ...props
}: React.ComponentProps<"span">) {
  return (
    <span
      data-slot="command-shortcut"
      className={cn(
        "text-muted-foreground ml-auto text-xs tracking-widest",
        className
      )}
      {...props}
    />
  )
}

export {
  Command,
  CommandDialog,
  CommandInput,
  CommandList,
  CommandEmpty,
  CommandGroup,
  CommandItem,
  CommandShortcut,
  CommandSeparator,
}

--- END FILE: ./frontend_app/components/ui/command.tsx ---

--- START FILE: ./frontend_app/components/ui/accordion.tsx (Size: 2173 bytes) ---
// File: frontend_app/components/ui/accordion.tsx
"use client"

import * as React from "react"
import * as AccordionPrimitive from "@radix-ui/react-accordion"
import { ChevronDownIcon } from "lucide-react"

import { cn } from "@/lib/utils"

function Accordion({
  ...props
}: React.ComponentProps<typeof AccordionPrimitive.Root>) {
  return <AccordionPrimitive.Root data-slot="accordion" {...props} />
}

function AccordionItem({
  className,
  ...props
}: React.ComponentProps<typeof AccordionPrimitive.Item>) {
  return (
    <AccordionPrimitive.Item
      data-slot="accordion-item"
      className={cn("border-b last:border-b-0", className)}
      {...props}
    />
  )
}

function AccordionTrigger({
  className,
  children,
  ...props
}: React.ComponentProps<typeof AccordionPrimitive.Trigger>) {
  return (
    <AccordionPrimitive.Header className="flex">
      <AccordionPrimitive.Trigger
        data-slot="accordion-trigger"
        className={cn(
          "focus-visible:border-ring focus-visible:ring-ring/50 flex flex-1 items-start justify-between gap-4 rounded-md py-4 text-left text-sm font-medium transition-all outline-none hover:underline focus-visible:ring-[3px] disabled:pointer-events-none disabled:opacity-50 [&[data-state=open]>svg]:rotate-180",
          // *** ADDED cursor-pointer ***
          "cursor-pointer",
          className
        )}
        {...props}
      >
        {children}
        <ChevronDownIcon className="text-muted-foreground pointer-events-none size-4 shrink-0 translate-y-0.5 transition-transform duration-200" />
      </AccordionPrimitive.Trigger>
    </AccordionPrimitive.Header>
  )
}

function AccordionContent({
  className,
  children,
  ...props
}: React.ComponentProps<typeof AccordionPrimitive.Content>) {
  return (
    <AccordionPrimitive.Content
      data-slot="accordion-content"
      className="data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down overflow-hidden text-sm"
      {...props}
    >
      <div className={cn("pt-0 pb-4", className)}>{children}</div>
    </AccordionPrimitive.Content>
  )
}

export { Accordion, AccordionItem, AccordionTrigger, AccordionContent }

--- END FILE: ./frontend_app/components/ui/accordion.tsx ---

--- START FILE: ./frontend_app/components/ui/label.tsx (Size: 781 bytes) ---
// File: frontend_app/components/ui/label.tsx
// Ensure this file looks exactly like this (NO cursor-pointer in base styles)
"use client"

import * as React from "react"
import * as LabelPrimitive from "@radix-ui/react-label"

import { cn } from "@/lib/utils"

function Label({
  className,
  ...props
}: React.ComponentProps<typeof LabelPrimitive.Root>) {
  return (
    <LabelPrimitive.Root
      data-slot="label"
      className={cn(
        // NO cursor-pointer here by default
        "flex items-center gap-2 text-sm leading-none font-medium select-none group-data-[disabled=true]:pointer-events-none group-data-[disabled=true]:opacity-50 peer-disabled:cursor-not-allowed peer-disabled:opacity-50",
        className
      )}
      {...props}
    />
  )
}

export { Label }

--- END FILE: ./frontend_app/components/ui/label.tsx ---

--- START FILE: ./frontend_app/components/ui/skeleton.tsx (Size: 276 bytes) ---
import { cn } from "@/lib/utils"

function Skeleton({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="skeleton"
      className={cn("bg-accent animate-pulse rounded-md", className)}
      {...props}
    />
  )
}

export { Skeleton }

--- END FILE: ./frontend_app/components/ui/skeleton.tsx ---

--- START FILE: ./frontend_app/components/ui/checkbox.tsx (Size: 1342 bytes) ---
// File: frontend_app/components/ui/checkbox.tsx
"use client"

import * as React from "react"
import * as CheckboxPrimitive from "@radix-ui/react-checkbox"
import { CheckIcon } from "lucide-react"

import { cn } from "@/lib/utils"

function Checkbox({
  className,
  ...props
}: React.ComponentProps<typeof CheckboxPrimitive.Root>) {
  return (
    <CheckboxPrimitive.Root
      data-slot="checkbox"
      className={cn(
        "peer border-input dark:bg-input/30 data-[state=checked]:bg-primary data-[state=checked]:text-primary-foreground dark:data-[state=checked]:bg-primary data-[state=checked]:border-primary focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive size-4 shrink-0 rounded-[4px] border shadow-xs transition-shadow outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50",
         // *** ADDED cursor-pointer ***
        "cursor-pointer",
        className
      )}
      {...props}
    >
      <CheckboxPrimitive.Indicator
        data-slot="checkbox-indicator"
        className="flex items-center justify-center text-current transition-none"
      >
        <CheckIcon className="size-3.5" />
      </CheckboxPrimitive.Indicator>
    </CheckboxPrimitive.Root>
  )
}

export { Checkbox }

--- END FILE: ./frontend_app/components/ui/checkbox.tsx ---

--- START FILE: ./frontend_app/components/ui/badge.tsx (Size: 1631 bytes) ---
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const badgeVariants = cva(
  "inline-flex items-center justify-center rounded-md border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&>svg]:size-3 gap-1 [&>svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden",
  {
    variants: {
      variant: {
        default:
          "border-transparent bg-primary text-primary-foreground [a&]:hover:bg-primary/90",
        secondary:
          "border-transparent bg-secondary text-secondary-foreground [a&]:hover:bg-secondary/90",
        destructive:
          "border-transparent bg-destructive text-white [a&]:hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",
        outline:
          "text-foreground [a&]:hover:bg-accent [a&]:hover:text-accent-foreground",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  }
)

function Badge({
  className,
  variant,
  asChild = false,
  ...props
}: React.ComponentProps<"span"> &
  VariantProps<typeof badgeVariants> & { asChild?: boolean }) {
  const Comp = asChild ? Slot : "span"

  return (
    <Comp
      data-slot="badge"
      className={cn(badgeVariants({ variant }), className)}
      {...props}
    />
  )
}

export { Badge, badgeVariants }

--- END FILE: ./frontend_app/components/ui/badge.tsx ---

--- START FILE: ./frontend_app/components/ui/sonner.tsx (Size: 564 bytes) ---
"use client"

import { useTheme } from "next-themes"
import { Toaster as Sonner, ToasterProps } from "sonner"

const Toaster = ({ ...props }: ToasterProps) => {
  const { theme = "system" } = useTheme()

  return (
    <Sonner
      theme={theme as ToasterProps["theme"]}
      className="toaster group"
      style={
        {
          "--normal-bg": "var(--popover)",
          "--normal-text": "var(--popover-foreground)",
          "--normal-border": "var(--border)",
        } as React.CSSProperties
      }
      {...props}
    />
  )
}

export { Toaster }

--- END FILE: ./frontend_app/components/ui/sonner.tsx ---

--- START FILE: ./frontend_app/components/ui/alert-dialog.tsx (Size: 3864 bytes) ---
"use client"

import * as React from "react"
import * as AlertDialogPrimitive from "@radix-ui/react-alert-dialog"

import { cn } from "@/lib/utils"
import { buttonVariants } from "@/components/ui/button"

function AlertDialog({
  ...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Root>) {
  return <AlertDialogPrimitive.Root data-slot="alert-dialog" {...props} />
}

function AlertDialogTrigger({
  ...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Trigger>) {
  return (
    <AlertDialogPrimitive.Trigger data-slot="alert-dialog-trigger" {...props} />
  )
}

function AlertDialogPortal({
  ...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Portal>) {
  return (
    <AlertDialogPrimitive.Portal data-slot="alert-dialog-portal" {...props} />
  )
}

function AlertDialogOverlay({
  className,
  ...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Overlay>) {
  return (
    <AlertDialogPrimitive.Overlay
      data-slot="alert-dialog-overlay"
      className={cn(
        "data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 fixed inset-0 z-50 bg-black/50",
        className
      )}
      {...props}
    />
  )
}

function AlertDialogContent({
  className,
  ...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Content>) {
  return (
    <AlertDialogPortal>
      <AlertDialogOverlay />
      <AlertDialogPrimitive.Content
        data-slot="alert-dialog-content"
        className={cn(
          "bg-background data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 fixed top-[50%] left-[50%] z-50 grid w-full max-w-[calc(100%-2rem)] translate-x-[-50%] translate-y-[-50%] gap-4 rounded-lg border p-6 shadow-lg duration-200 sm:max-w-lg",
          className
        )}
        {...props}
      />
    </AlertDialogPortal>
  )
}

function AlertDialogHeader({
  className,
  ...props
}: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="alert-dialog-header"
      className={cn("flex flex-col gap-2 text-center sm:text-left", className)}
      {...props}
    />
  )
}

function AlertDialogFooter({
  className,
  ...props
}: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="alert-dialog-footer"
      className={cn(
        "flex flex-col-reverse gap-2 sm:flex-row sm:justify-end",
        className
      )}
      {...props}
    />
  )
}

function AlertDialogTitle({
  className,
  ...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Title>) {
  return (
    <AlertDialogPrimitive.Title
      data-slot="alert-dialog-title"
      className={cn("text-lg font-semibold", className)}
      {...props}
    />
  )
}

function AlertDialogDescription({
  className,
  ...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Description>) {
  return (
    <AlertDialogPrimitive.Description
      data-slot="alert-dialog-description"
      className={cn("text-muted-foreground text-sm", className)}
      {...props}
    />
  )
}

function AlertDialogAction({
  className,
  ...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Action>) {
  return (
    <AlertDialogPrimitive.Action
      className={cn(buttonVariants(), className)}
      {...props}
    />
  )
}

function AlertDialogCancel({
  className,
  ...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Cancel>) {
  return (
    <AlertDialogPrimitive.Cancel
      className={cn(buttonVariants({ variant: "outline" }), className)}
      {...props}
    />
  )
}

export {
  AlertDialog,
  AlertDialogPortal,
  AlertDialogOverlay,
  AlertDialogTrigger,
  AlertDialogContent,
  AlertDialogHeader,
  AlertDialogFooter,
  AlertDialogTitle,
  AlertDialogDescription,
  AlertDialogAction,
  AlertDialogCancel,
}

--- END FILE: ./frontend_app/components/ui/alert-dialog.tsx ---

--- START FILE: ./frontend_app/components/ui/select.tsx (Size: 6155 bytes) ---
// File: frontend_app/components/ui/select.tsx
"use client"

import * as React from "react"
import * as SelectPrimitive from "@radix-ui/react-select"
import { CheckIcon, ChevronDownIcon, ChevronUpIcon } from "lucide-react"

import { cn } from "@/lib/utils"

const Select = SelectPrimitive.Root
const SelectGroup = SelectPrimitive.Group
const SelectValue = SelectPrimitive.Value

const SelectTrigger = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Trigger>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Trigger
    ref={ref}
    className={cn(
      "flex h-10 w-full items-center justify-between rounded-md border border-input bg-background px-3 py-2 text-sm ring-offset-background placeholder:text-muted-foreground focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 [&>span]:line-clamp-1",
      "cursor-pointer", // Keep cursor change
      className
    )}
    {...props}
  >
    {children}
    <SelectPrimitive.Icon asChild>
      <ChevronDownIcon className="h-4 w-4 opacity-50" />
    </SelectPrimitive.Icon>
  </SelectPrimitive.Trigger>
))
SelectTrigger.displayName = SelectPrimitive.Trigger.displayName

const SelectScrollUpButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollUpButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollUpButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollUpButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className
    )}
    {...props}
  >
    <ChevronUpIcon className="h-4 w-4" />
  </SelectPrimitive.ScrollUpButton>
))
SelectScrollUpButton.displayName = SelectPrimitive.ScrollUpButton.displayName

const SelectScrollDownButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollDownButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollDownButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollDownButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className
    )}
    {...props}
  >
    <ChevronDownIcon className="h-4 w-4" />
  </SelectPrimitive.ScrollDownButton>
))
SelectScrollDownButton.displayName =
  SelectPrimitive.ScrollDownButton.displayName

// --- MODIFIED SelectContent (with inline style fallback) ---
const SelectContent = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Content>
>(({ className, children, position = "popper", sideOffset=4, ...props }, ref) => (
  <SelectPrimitive.Portal>
    <SelectPrimitive.Content
      ref={ref}
      // *** ADDED INLINE STYLE USING CSS VARIABLE ***
      style={{ backgroundColor: 'var(--popover)' }}
      className={cn(
        // Keep original classes, but REMOVE bg-popover
        "relative z-50 max-h-96 min-w-[8rem] overflow-hidden rounded-md border text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        // REMOVED 'bg-popover' from here
        position === "popper" &&
          "data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",
        className
      )}
      position={position}
      sideOffset={sideOffset}
      {...props}
    >
      <SelectScrollUpButton />
      <SelectPrimitive.Viewport
        className={cn(
          "p-1",
          position === "popper" &&
            "h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)]"
        )}
      >
        {children}
      </SelectPrimitive.Viewport>
      <SelectScrollDownButton />
    </SelectPrimitive.Content>
  </SelectPrimitive.Portal>
))
SelectContent.displayName = SelectPrimitive.Content.displayName
// --- END MODIFIED SelectContent ---


const SelectLabel = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Label>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Label
    ref={ref}
    className={cn("py-1.5 pl-8 pr-2 text-sm font-semibold", className)}
    {...props}
  />
))
SelectLabel.displayName = SelectPrimitive.Label.displayName

const SelectItem = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Item>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex w-full select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      "cursor-pointer data-[disabled]:cursor-not-allowed", // Keep cursor changes
      className
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <SelectPrimitive.ItemIndicator>
        <CheckIcon className="h-4 w-4" />
      </SelectPrimitive.ItemIndicator>
    </span>

    <SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
  </SelectPrimitive.Item>
))
SelectItem.displayName = SelectPrimitive.Item.displayName

const SelectSeparator = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
))
SelectSeparator.displayName = SelectPrimitive.Separator.displayName

export {
  Select,
  SelectGroup,
  SelectValue,
  SelectTrigger,
  SelectContent,
  SelectLabel,
  SelectItem,
  SelectSeparator,
  SelectScrollUpButton,
  SelectScrollDownButton,
}

--- END FILE: ./frontend_app/components/ui/select.tsx ---

--- START FILE: ./frontend_app/components/ui/card.tsx (Size: 2144 bytes) ---
// File: frontend_app/components/ui/card.tsx
import * as React from "react"

import { cn } from "@/lib/utils"

function Card({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card"
      className={cn(
        "bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm",
        className
      )}
      {...props}
    />
  )
}

function CardHeader({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-header"
      className={cn(
        "@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-1.5 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6",
        className
      )}
      {...props}
    />
  )
}

function CardTitle({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-title"
      className={cn("leading-none font-semibold", className)}
      {...props}
    />
  )
}

function CardDescription({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-description"
      className={cn("text-muted-foreground text-sm", className)}
      {...props}
    />
  )
}

function CardAction({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-action"
      className={cn(
        "col-start-2 row-span-2 row-start-1 self-start justify-self-end",
        className
      )}
      {...props}
    />
  )
}

// --- REVERTED CardContent ---
function CardContent({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-content"
      // --- REMOVED 'isolate' class ---
      className={cn("px-6", className)}
      {...props}
    />
  )
}
// --- END REVERTED CardContent ---


function CardFooter({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-footer"
      className={cn("flex items-center px-6 [.border-t]:pt-6", className)}
      {...props}
    />
  )
}

export {
  Card,
  CardHeader,
  CardFooter,
  CardTitle,
  CardAction,
  CardDescription,
  CardContent,
}

--- END FILE: ./frontend_app/components/ui/card.tsx ---

--- START FILE: ./frontend_app/components/ui/dialog.tsx (Size: 3813 bytes) ---
"use client"

import * as React from "react"
import * as DialogPrimitive from "@radix-ui/react-dialog"
import { XIcon } from "lucide-react"

import { cn } from "@/lib/utils"

function Dialog({
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Root>) {
  return <DialogPrimitive.Root data-slot="dialog" {...props} />
}

function DialogTrigger({
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Trigger>) {
  return <DialogPrimitive.Trigger data-slot="dialog-trigger" {...props} />
}

function DialogPortal({
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Portal>) {
  return <DialogPrimitive.Portal data-slot="dialog-portal" {...props} />
}

function DialogClose({
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Close>) {
  return <DialogPrimitive.Close data-slot="dialog-close" {...props} />
}

function DialogOverlay({
  className,
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Overlay>) {
  return (
    <DialogPrimitive.Overlay
      data-slot="dialog-overlay"
      className={cn(
        "data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 fixed inset-0 z-50 bg-black/50",
        className
      )}
      {...props}
    />
  )
}

function DialogContent({
  className,
  children,
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Content>) {
  return (
    <DialogPortal data-slot="dialog-portal">
      <DialogOverlay />
      <DialogPrimitive.Content
        data-slot="dialog-content"
        className={cn(
          "bg-background data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 fixed top-[50%] left-[50%] z-50 grid w-full max-w-[calc(100%-2rem)] translate-x-[-50%] translate-y-[-50%] gap-4 rounded-lg border p-6 shadow-lg duration-200 sm:max-w-lg",
          className
        )}
        {...props}
      >
        {children}
        <DialogPrimitive.Close className="ring-offset-background focus:ring-ring data-[state=open]:bg-accent data-[state=open]:text-muted-foreground absolute top-4 right-4 rounded-xs opacity-70 transition-opacity hover:opacity-100 focus:ring-2 focus:ring-offset-2 focus:outline-hidden disabled:pointer-events-none [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4">
          <XIcon />
          <span className="sr-only">Close</span>
        </DialogPrimitive.Close>
      </DialogPrimitive.Content>
    </DialogPortal>
  )
}

function DialogHeader({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="dialog-header"
      className={cn("flex flex-col gap-2 text-center sm:text-left", className)}
      {...props}
    />
  )
}

function DialogFooter({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="dialog-footer"
      className={cn(
        "flex flex-col-reverse gap-2 sm:flex-row sm:justify-end",
        className
      )}
      {...props}
    />
  )
}

function DialogTitle({
  className,
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Title>) {
  return (
    <DialogPrimitive.Title
      data-slot="dialog-title"
      className={cn("text-lg leading-none font-semibold", className)}
      {...props}
    />
  )
}

function DialogDescription({
  className,
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Description>) {
  return (
    <DialogPrimitive.Description
      data-slot="dialog-description"
      className={cn("text-muted-foreground text-sm", className)}
      {...props}
    />
  )
}

export {
  Dialog,
  DialogClose,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogOverlay,
  DialogPortal,
  DialogTitle,
  DialogTrigger,
}

--- END FILE: ./frontend_app/components/ui/dialog.tsx ---

--- START FILE: ./frontend_app/components/ui/form.tsx (Size: 4287 bytes) ---
// File: frontend_app/components/ui/form.tsx
"use client"

import * as React from "react"
import * as LabelPrimitive from "@radix-ui/react-label"
import { Slot } from "@radix-ui/react-slot" // Keep Slot import if used elsewhere, though not needed for FormControl now
import {
  Controller,
  FormProvider,
  useFormContext,
  useFormState,
  type ControllerProps,
  type FieldPath,
  type FieldValues,
} from "react-hook-form"

import { cn } from "@/lib/utils"
import { Label } from "@/components/ui/label" // Ensure Label is imported

const Form = FormProvider

type FormFieldContextValue<
  TFieldValues extends FieldValues = FieldValues,
  TName extends FieldPath<TFieldValues> = FieldPath<TFieldValues>,
> = {
  name: TName
}

const FormFieldContext = React.createContext<FormFieldContextValue>(
  {} as FormFieldContextValue
)

const FormField = <
  TFieldValues extends FieldValues = FieldValues,
  TName extends FieldPath<TFieldValues> = FieldPath<TFieldValues>,
>({
  ...props
}: ControllerProps<TFieldValues, TName>) => {
  return (
    <FormFieldContext.Provider value={{ name: props.name }}>
      <Controller {...props} />
    </FormFieldContext.Provider>
  )
}

const useFormField = () => {
  const fieldContext = React.useContext(FormFieldContext)
  const itemContext = React.useContext(FormItemContext)
  const { getFieldState } = useFormContext()
  const formState = useFormState({ name: fieldContext.name })
  const fieldState = getFieldState(fieldContext.name, formState)

  if (!fieldContext) {
    throw new Error("useFormField should be used within <FormField>")
  }

  const { id } = itemContext

  return {
    id,
    name: fieldContext.name,
    formItemId: `${id}-form-item`,
    formDescriptionId: `${id}-form-item-description`,
    formMessageId: `${id}-form-item-message`,
    ...fieldState,
  }
}

type FormItemContextValue = {
  id: string
}

const FormItemContext = React.createContext<FormItemContextValue>(
  {} as FormItemContextValue
)

function FormItem({ className, ...props }: React.ComponentProps<"div">) {
  const id = React.useId()

  return (
    <FormItemContext.Provider value={{ id }}>
      <div
        data-slot="form-item"
        // Added 'relative' class previously, keeping it
        className={cn("grid gap-2 relative", className)}
        {...props}
      />
    </FormItemContext.Provider>
  )
}

function FormLabel({
  className,
  ...props
}: React.ComponentProps<typeof LabelPrimitive.Root>) {
  const { error, formItemId } = useFormField()

  return (
    <Label
      data-slot="form-label"
      data-error={!!error}
      className={cn(
        "data-[error=true]:text-destructive",
        "cursor-pointer", // Kept cursor change from previous step
        className
      )}
      htmlFor={formItemId}
      {...props}
    />
  )
}

// --- *** MODIFIED FormControl *** ---
function FormControl(props: React.ComponentProps<"div">) { // Changed props type from Slot to div
  const { error, formItemId, formDescriptionId, formMessageId } = useFormField();

  // Replace Slot with div
  return (
    <div // Use a div instead of Slot
      data-slot="form-control"
      id={formItemId}
      aria-describedby={
        !error
          ? `${formDescriptionId}`
          : `${formDescriptionId} ${formMessageId}`
      }
      aria-invalid={!!error}
      {...props} // Spread props onto the div, this includes children
    />
  );
}
// --- *** END MODIFIED FormControl *** ---


function FormDescription({ className, ...props }: React.ComponentProps<"p">) {
  const { formDescriptionId } = useFormField()

  return (
    <p
      data-slot="form-description"
      id={formDescriptionId}
      className={cn("text-muted-foreground text-sm", className)}
      {...props}
    />
  )
}

function FormMessage({ className, ...props }: React.ComponentProps<"p">) {
  const { error, formMessageId } = useFormField()
  const body = error ? String(error?.message ?? "") : props.children

  if (!body) {
    return null
  }

  return (
    <p
      data-slot="form-message"
      id={formMessageId}
      className={cn("text-destructive text-sm", className)}
      {...props}
    >
      {body}
    </p>
  )
}

export {
  useFormField,
  Form,
  FormItem,
  FormLabel,
  FormControl,
  FormDescription,
  FormMessage,
  FormField,
}

--- END FILE: ./frontend_app/components/ui/form.tsx ---

--- START FILE: ./frontend_app/components/ui/button.tsx (Size: 2123 bytes) ---
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const buttonVariants = cva(
  "inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
  {
    variants: {
      variant: {
        default:
          "bg-primary text-primary-foreground shadow-xs hover:bg-primary/90",
        destructive:
          "bg-destructive text-white shadow-xs hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",
        outline:
          "border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50",
        secondary:
          "bg-secondary text-secondary-foreground shadow-xs hover:bg-secondary/80",
        ghost:
          "hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-9 px-4 py-2 has-[>svg]:px-3",
        sm: "h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5",
        lg: "h-10 rounded-md px-6 has-[>svg]:px-4",
        icon: "size-9",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  }
)

function Button({
  className,
  variant,
  size,
  asChild = false,
  ...props
}: React.ComponentProps<"button"> &
  VariantProps<typeof buttonVariants> & {
    asChild?: boolean
  }) {
  const Comp = asChild ? Slot : "button"

  return (
    <Comp
      data-slot="button"
      className={cn(buttonVariants({ variant, size, className }))}
      {...props}
    />
  )
}

export { Button, buttonVariants }

--- END FILE: ./frontend_app/components/ui/button.tsx ---

--- START FILE: ./frontend_app/components/ui/alert.tsx (Size: 1614 bytes) ---
import * as React from "react"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const alertVariants = cva(
  "relative w-full rounded-lg border px-4 py-3 text-sm grid has-[>svg]:grid-cols-[calc(var(--spacing)*4)_1fr] grid-cols-[0_1fr] has-[>svg]:gap-x-3 gap-y-0.5 items-start [&>svg]:size-4 [&>svg]:translate-y-0.5 [&>svg]:text-current",
  {
    variants: {
      variant: {
        default: "bg-card text-card-foreground",
        destructive:
          "text-destructive bg-card [&>svg]:text-current *:data-[slot=alert-description]:text-destructive/90",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  }
)

function Alert({
  className,
  variant,
  ...props
}: React.ComponentProps<"div"> & VariantProps<typeof alertVariants>) {
  return (
    <div
      data-slot="alert"
      role="alert"
      className={cn(alertVariants({ variant }), className)}
      {...props}
    />
  )
}

function AlertTitle({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="alert-title"
      className={cn(
        "col-start-2 line-clamp-1 min-h-4 font-medium tracking-tight",
        className
      )}
      {...props}
    />
  )
}

function AlertDescription({
  className,
  ...props
}: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="alert-description"
      className={cn(
        "text-muted-foreground col-start-2 grid justify-items-start gap-1 text-sm [&_p]:leading-relaxed",
        className
      )}
      {...props}
    />
  )
}

export { Alert, AlertTitle, AlertDescription }

--- END FILE: ./frontend_app/components/ui/alert.tsx ---

--- START FILE: ./frontend_app/components/ui/popover.tsx (Size: 1348 bytes) ---
// frontend_app/components/ui/popover.tsx
"use client"

import * as React from "react"
import * as PopoverPrimitive from "@radix-ui/react-popover"

import { cn } from "@/lib/utils"

const Popover = PopoverPrimitive.Root

const PopoverTrigger = PopoverPrimitive.Trigger

const PopoverAnchor = PopoverPrimitive.Anchor

const PopoverContent = React.forwardRef<
  React.ElementRef<typeof PopoverPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof PopoverPrimitive.Content>
>(({ className, align = "center", sideOffset = 4, ...props }, ref) => (
  <PopoverPrimitive.Portal>
    <PopoverPrimitive.Content
      ref={ref}
      align={align}
      sideOffset={sideOffset}
      className={cn(
        "z-50 w-72 rounded-md border bg-popover p-4 text-popover-foreground shadow-md outline-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className
      )}
      {...props}
    />
  </PopoverPrimitive.Portal>
))
PopoverContent.displayName = PopoverPrimitive.Content.displayName

export { Popover, PopoverTrigger, PopoverContent, PopoverAnchor }

--- END FILE: ./frontend_app/components/ui/popover.tsx ---

--- START FILE: ./frontend_app/components/ui/dropdown-menu.tsx (Size: 3764 bytes) ---
// File: frontend_app/components/ui/dropdown-menu.tsx
"use client"

import * as React from "react"
import * as DropdownMenuPrimitive from "@radix-ui/react-dropdown-menu"
import { CheckIcon, ChevronRightIcon, CircleIcon } from "lucide-react"

import { cn } from "@/lib/utils"

// --- MODIFIED DropdownMenuItem ---
function DropdownMenuItem({
  className,
  inset,
  variant = "default",
  children, // Explicitly destructure children
  ...props // Keep rest of the props
}: React.ComponentProps<typeof DropdownMenuPrimitive.Item> & {
  inset?: boolean
  variant?: "default" | "destructive"
}) {
  return (
    <DropdownMenuPrimitive.Item
      data-slot="dropdown-menu-item"
      data-inset={inset}
      data-variant={variant}
      className={cn(
        "focus:bg-accent focus:text-accent-foreground data-[variant=destructive]:text-destructive data-[variant=destructive]:focus:bg-destructive/10 dark:data-[variant=destructive]:focus:bg-destructive/20 data-[variant=destructive]:focus:text-destructive data-[variant=destructive]*:[svg]:!text-destructive [&_svg:not([class*='text-'])]:text-muted-foreground relative flex items-center gap-2 rounded-sm px-2 py-1.5 text-sm outline-none select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 data-[inset]:pl-8 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
        "cursor-pointer data-[disabled]:cursor-not-allowed",
        className
      )}
      {...props} // Spread remaining props (like onSelect, disabled, etc.)
    >
      {/* Wrap children in a single element */}
      <span>{children}</span>
    </DropdownMenuPrimitive.Item>
  )
}
DropdownMenuItem.displayName = "DropdownMenuItem"
// --- END MODIFICATION ---

// --- Other components remain unchanged ---
function DropdownMenu({ /* ... */ }) { /* ... */ }
DropdownMenu.displayName = "DropdownMenu"
function DropdownMenuPortal({ /* ... */ }) { /* ... */ }
DropdownMenuPortal.displayName = "DropdownMenuPortal"
function DropdownMenuTrigger({ /* ... */ }) { /* ... */ }
DropdownMenuTrigger.displayName = "DropdownMenuTrigger"
function DropdownMenuContent({ /* ... */ }) { /* ... */ }
DropdownMenuContent.displayName = "DropdownMenuContent"
function DropdownMenuGroup({ /* ... */ }) { /* ... */ }
DropdownMenuGroup.displayName = "DropdownMenuGroup"
function DropdownMenuCheckboxItem({ /* ... */ }) { /* ... */ }
DropdownMenuCheckboxItem.displayName = "DropdownMenuCheckboxItem"
function DropdownMenuRadioGroup({ /* ... */ }) { /* ... */ }
DropdownMenuRadioGroup.displayName = "DropdownMenuRadioGroup"
function DropdownMenuRadioItem({ /* ... */ }) { /* ... */ }
DropdownMenuRadioItem.displayName = "DropdownMenuRadioItem"
function DropdownMenuLabel({ /* ... */ }) { /* ... */ }
DropdownMenuLabel.displayName = "DropdownMenuLabel"
function DropdownMenuSeparator({ /* ... */ }) { /* ... */ }
DropdownMenuSeparator.displayName = "DropdownMenuSeparator"
function DropdownMenuShortcut({ /* ... */ }) { /* ... */ }
DropdownMenuShortcut.displayName = "DropdownMenuShortcut"
function DropdownMenuSub({ /* ... */ }) { /* ... */ }
DropdownMenuSub.displayName = "DropdownMenuSub"
function DropdownMenuSubTrigger({ /* ... */ }) { /* ... */ }
DropdownMenuSubTrigger.displayName = "DropdownMenuSubTrigger"
function DropdownMenuSubContent({ /* ... */ }) { /* ... */ }
DropdownMenuSubContent.displayName = "DropdownMenuSubContent"

export {
  DropdownMenu,
  DropdownMenuPortal,
  DropdownMenuTrigger,
  DropdownMenuContent,
  DropdownMenuGroup,
  DropdownMenuLabel,
  DropdownMenuItem, // Export modified component
  DropdownMenuCheckboxItem,
  DropdownMenuRadioGroup,
  DropdownMenuRadioItem,
  DropdownMenuSeparator,
  DropdownMenuShortcut,
  DropdownMenuSub,
  DropdownMenuSubTrigger,
  DropdownMenuSubContent,
}

--- END FILE: ./frontend_app/components/ui/dropdown-menu.tsx ---

--- START FILE: ./frontend_app/components/ui/table.tsx (Size: 2863 bytes) ---
// File: frontend_app/components/ui/table.tsx
"use client"

import * as React from "react"
import { cn } from "@/lib/utils"

// Main table component
function Table({ className, children, ...props }: React.ComponentProps<"table">) {
  return (
    <div
      data-slot="table-container"
      className="relative w-full overflow-x-auto"
    >
      <table
        data-slot="table"
        className={cn("w-full caption-bottom text-sm", className)}
        {...props}
      >{children}</table> {/* Explicit children, no whitespace */}
    </div>
  )
}
Table.displayName = "Table"

// TableRow component
function TableRow({ className, children, ...props }: React.ComponentProps<"tr">) {
  return (
    <tr
      data-slot="table-row"
      className={cn(
        "hover:bg-muted/50 data-[state=selected]:bg-muted border-b transition-colors",
        className
      )}
      {...props}
    >{children}</tr>
  );
}
TableRow.displayName = "TableRow"

// --- Other components remain unchanged ---
function TableHeader({ className, ...props }: React.ComponentProps<"thead">) {
  return (
    <thead
      data-slot="table-header"
      className={cn("[&_tr]:border-b", className)}
      {...props}
    />
  )
}
TableHeader.displayName = "TableHeader"

function TableBody({ className, ...props }: React.ComponentProps<"tbody">) {
  return (
    <tbody
      data-slot="table-body"
      className={cn("[&_tr:last-child]:border-0", className)}
      {...props}
    />
  )
}
TableBody.displayName = "TableBody"

function TableFooter({ className, ...props }: React.ComponentProps<"tfoot">) {
  return (
    <tfoot
      data-slot="table-footer"
      className={cn(
        "bg-muted/50 border-t font-medium [&>tr]:last:border-b-0",
        className
      )}
      {...props}
    />
  )
}
TableFooter.displayName = "TableFooter"

function TableHead({ className, ...props }: React.ComponentProps<"th">) {
  return (
    <th
      data-slot="table-head"
      className={cn(
        "h-12 px-4 text-left align-middle font-medium text-muted-foreground [&:has([role=checkbox])]:pr-0",
        className
      )}
      {...props}
    />
  )
}
TableHead.displayName = "TableHead"

function TableCell({ className, ...props }: React.ComponentProps<"td">) {
  return (
    <td
      data-slot="table-cell"
      className={cn(
        "p-4 align-middle [&:has([role=checkbox])]:pr-0",
        className
      )}
      {...props}
    />
  )
}
TableCell.displayName = "TableCell"

function TableCaption({
  className,
  ...props
}: React.ComponentProps<"caption">) {
  return (
    <caption
      data-slot="table-caption"
      className={cn("text-muted-foreground mt-4 text-sm", className)}
      {...props}
    />
  )
}
TableCaption.displayName = "TableCaption"

export {
  Table,
  TableHeader,
  TableBody,
  TableFooter,
  TableHead,
  TableRow,
  TableCell,
  TableCaption,
}

--- END FILE: ./frontend_app/components/ui/table.tsx ---

--- START FILE: ./frontend_app/postcss.config.mjs (Size: 163 bytes) ---
// frontend_app/postcss.config.mjs
const config = {
  plugins: {
    "@tailwindcss/postcss": {}, // Use the v4 plugin object syntax
  },
};
export default config;

--- END FILE: ./frontend_app/postcss.config.mjs ---

--- START FILE: ./frontend_app/tailwind.config.ts (Size: 2391 bytes) ---
// File: frontend_app/tailwind.config.ts
import type { Config } from "tailwindcss";
import defaultTheme from "tailwindcss/defaultTheme";

const config = {
  // --- CHANGE THIS LINE ---
  darkMode: "class", // Use the string 'class' instead of an array
  // ------------------------
  content: [
    './pages/**/*.{ts,tsx}',
    './components/**/*.{ts,tsx}',
    './app/**/*.{ts,tsx}',
    './src/**/*.{ts,tsx}',
	],
  theme: {
    container: {
      center: true,
      padding: "2rem",
      screens: {
        "2xl": "1400px",
      },
    },
    extend: {
      colors: {
        border: "hsl(var(--border))",
        input: "hsl(var(--input))",
        ring: "hsl(var(--ring))",
        background: "var(--background)",
        foreground: "var(--foreground)",
        primary: {
          DEFAULT: "var(--primary)",
          foreground: "var(--primary-foreground)",
        },
        secondary: {
          DEFAULT: "var(--secondary)",
          foreground: "var(--secondary-foreground)",
        },
        destructive: {
          DEFAULT: "var(--destructive)",
          foreground: "var(--destructive-foreground)",
        },
        muted: {
          DEFAULT: "var(--muted)",
          foreground: "var(--muted-foreground)",
        },
        accent: {
          DEFAULT: "var(--accent)",
          foreground: "var(--accent-foreground)",
        },
        popover: {
          DEFAULT: "var(--popover)",
          foreground: "var(--popover-foreground)",
        },
        card: {
          DEFAULT: "var(--card)",
          foreground: "var(--card-foreground)",
        },
      },
      borderRadius: {
        lg: "var(--radius)",
        md: "calc(var(--radius) - 2px)",
        sm: "calc(var(--radius) - 4px)",
      },
      fontFamily: {
        sans: ["var(--font-geist-sans)", ...defaultTheme.fontFamily.sans],
      },
      keyframes: {
        "accordion-down": {
          from: { height: "0" },
          to: { height: "var(--radix-accordion-content-height)" },
        },
        "accordion-up": {
          from: { height: "var(--radix-accordion-content-height)" },
          to: { height: "0" },
        },
      },
      animation: {
        "accordion-down": "accordion-down 0.2s ease-out",
        "accordion-up": "accordion-up 0.2s ease-out",
      },
    },
  },
  plugins: [require("tailwindcss-animate")],
} satisfies Config;

export default config;

--- END FILE: ./frontend_app/tailwind.config.ts ---

--- START FILE: ./frontend_app/package.json (Size: 1505 bytes) ---
{
  "name": "frontend_app",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@tailwindcss/postcss": "^4.1.3",
    "@hookform/resolvers": "^5.0.1",
    "@radix-ui/react-accordion": "^1.2.4",
    "@radix-ui/react-alert-dialog": "^1.1.7",
    "@radix-ui/react-checkbox": "^1.1.5",
    "@radix-ui/react-dialog": "^1.1.7",
    "@radix-ui/react-dropdown-menu": "^2.1.7",
    "@radix-ui/react-label": "^2.1.3",
    "@radix-ui/react-popover": "^1.1.7",
    "@radix-ui/react-select": "^2.1.7",
    "@radix-ui/react-slot": "^1.2.0",
    "@tanstack/react-query": "^5.72.2",
    "@tanstack/react-query-devtools": "^5.72.2",
    "axios": "^1.8.4",
    "class-variance-authority": "^0.7.1",
    "clsx": "^2.1.1",
    "cmdk": "^1.1.1",
    "date-fns": "^4.1.0",
    "geist": "^1.3.1",
    "lucide-react": "^0.487.0",
    "next": "^15.3.0",
    "next-themes": "^0.4.6",
    "postcss": "^8.5.3",
    "react": "^19.1.0",
    "react-dom": "^19.1.0",
    "react-hook-form": "^7.55.0",
    "sonner": "^2.0.3",
    "tailwind-merge": "^3.2.0",
    "tailwindcss-animate": "^1.0.7",
    "tw-animate-css": "^1.2.5",
    "zod": "^3.24.2"
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "15.3.0",
    "tailwindcss": "^4.1.3",
    "typescript": "^5"
  }
}

--- END FILE: ./frontend_app/package.json ---

--- START FILE: ./frontend_app/components.json (Size: 424 bytes) ---
{
  "$schema": "https://ui.shadcn.com/schema.json",
  "style": "new-york",
  "rsc": true,
  "tsx": true,
  "tailwind": {
    "config": "",
    "css": "app/globals.css",
    "baseColor": "slate",
    "cssVariables": true,
    "prefix": ""
  },
  "aliases": {
    "components": "@/components",
    "utils": "@/lib/utils",
    "ui": "@/components/ui",
    "lib": "@/lib",
    "hooks": "@/hooks"
  },
  "iconLibrary": "lucide"
}
--- END FILE: ./frontend_app/components.json ---

--- START FILE: ./frontend_app/lib/api.ts (Size: 7518 bytes) ---
// File: frontend_app/lib/api.ts
import axios from "axios";
// Import updated types, including RunParameters
// REMOVED JobMetaInputParams from import
import { Job, PipelineInput, ResultRun, ResultItem, DataFile, JobStatusDetails, RunParameters } from "./types";
// Import zod schema for type reference in stagePipelineJob
import { z } from "zod";
// Assuming pipelineInputSchema is defined/imported elsewhere for the type definition below

const API_BASE_URL = process.env.NEXT_PUBLIC_API_BASE_URL || "";

if (!API_BASE_URL) {
  console.error("CRITICAL: NEXT_PUBLIC_API_BASE_URL is not defined. API calls will likely fail.");
}

const apiClient = axios.create({
  baseURL: API_BASE_URL,
  headers: {
    "Content-Type": "application/json",
  },
});

apiClient.interceptors.request.use(
  (config) => {
    return config;
  },
  (error) => {
    console.error("Request Error Interceptor:", error);
    return Promise.reject(error);
  }
);

apiClient.interceptors.response.use(
  (response) => {
    return response;
  },
  (error) => {
    console.error("Response Error Interceptor:", error.response?.status, error.config?.url, error.message);
    const message = error.response?.data?.detail || error.message || "An unknown error occurred";
    const enhancedError = new Error(message);
    (enhancedError as Error & { originalError?: any; status?: number }).originalError = error;
    (enhancedError as Error & { originalError?: any; status?: number }).status = error.response?.status;
    return Promise.reject(enhancedError);
  }
);

// --- API Functions ---

export const getJobsList = async (): Promise<Job[]> => {
  try {
    const response = await apiClient.get<Job[]>("/api/jobs_list");
    return response.data || [];
  } catch (error) {
    console.error("Failed to fetch jobs list:", error);
    throw error;
  }
};

export const getJobStatus = async (jobId: string): Promise<JobStatusDetails> => {
  if (!jobId) {
     throw new Error("Job ID cannot be empty for getJobStatus");
  }
  try {
    const response = await apiClient.get<JobStatusDetails>(`/api/job_status/${jobId}`);
    return response.data;
  } catch (error) {
     console.error(`Failed to fetch status for job ${jobId}:`, error);
    throw error;
  }
};

export const startJob = async (stagedJobId: string): Promise<{ message: string; job_id: string }> => {
    if (!stagedJobId || !stagedJobId.startsWith('staged_')) throw new Error("Invalid staged Job ID provided.");
    try {
        const response = await apiClient.post(`/api/start_job/${stagedJobId}`);
        return response.data;
    } catch (error) {
        console.error(`Failed to start job ${stagedJobId}:`, error);
        throw error;
    }
}

export const stopJob = async (jobId: string): Promise<{ message: string; job_id: string }> => {
    if (!jobId) throw new Error("Job ID is required to stop.");
     try {
        const response = await apiClient.post(`/api/stop_job/${jobId}`);
        return response.data;
    } catch (error) {
         console.error(`Failed to stop job ${jobId}:`, error);
        throw error;
    }
}

export const removeJob = async (jobId: string): Promise<{ message: string; removed_id: string }> => {
    if (!jobId) throw new Error("Job ID is required to remove.");
     try {
        const response = await apiClient.delete(`/api/remove_job/${jobId}`);
        return response.data;
    } catch (error) {
         console.error(`Failed to remove job ${jobId}:`, error);
        throw error;
    }
}

export const rerunJob = async (jobId: string): Promise<{ message: string; staged_job_id: string }> => {
     if (!jobId) throw new Error("Job ID is required to rerun.");
    try {
       const response = await apiClient.post(`/api/rerun_job/${jobId}`);
       return response.data;
    } catch (error) {
        console.error(`Failed to rerun job ${jobId}:`, error);
        throw error;
    }
};

export const getResultsList = async (): Promise<ResultRun[]> => {
  try {
    const response = await apiClient.get<ResultRun[]>("/api/get_results");
    return response.data || [];
  } catch (error) {
    console.error("Error fetching results list:", error);
    throw error;
  }
};

export const getResultRunFiles = async (runDirName: string): Promise<ResultItem[]> => {
  if (!runDirName) throw new Error("Run directory name is required.");
  try {
    const response = await apiClient.get<ResultItem[]>(`/api/get_results/${encodeURIComponent(runDirName)}`);
    return response.data || [];
  } catch (error) {
    console.error(`Error fetching files for run ${runDirName}:`, error);
    throw error;
  }
};

// Use the imported PipelineInput type directly
export const stagePipelineJob = async (values: PipelineInput): Promise<{ message: string; staged_job_id: string }> => {
    const apiPayload: PipelineInput = values;

  try {
    console.log("Staging Job with API Payload:", apiPayload);
    const response = await apiClient.post('/api/run_pipeline', apiPayload);
    return response.data;
  } catch (error) {
    console.error("Error staging pipeline job:", error);
    throw error;
  }
};


export const getDataFiles = async (type?: string, extensions?: string[]): Promise<DataFile[]> => {
  try {
    const response = await apiClient.get<DataFile[]>("/api/get_data");
    let files = response.data || [];
    if (extensions && extensions.length > 0) {
        files = files.filter(file => extensions.some(ext => file.name.toLowerCase().endsWith(ext.toLowerCase())));
    }
    return files;
  } catch (error) {
    console.error(`Error fetching data files (type: ${type}):`, error);
    throw error;
  }
};


export const getResultRunParameters = async (runDirName: string): Promise<RunParameters> => {
    if (!runDirName) throw new Error("Run directory name is required to fetch parameters.");
    try {
        const response = await apiClient.get<RunParameters>(`/api/results/${encodeURIComponent(runDirName)}/parameters`);
        return response.data || {};
    } catch (error) {
        console.error(`Error fetching parameters for run ${runDirName}:`, error);
        throw error;
    }
};

export const downloadResultRun = async (runDirName: string): Promise<Blob> => {
    if (!runDirName) throw new Error("Run directory name is required for download.");
    try {
        const response = await apiClient.get(`/api/download_result/${encodeURIComponent(runDirName)}`, {
            responseType: 'blob',
        });
        if (!(response.data instanceof Blob)) {
             throw new Error("Invalid response received from server during download.");
        }
        return response.data;
    } catch (error) {
        console.error(`Error downloading result run ${runDirName}:`, error);
        throw error;
    }
};

export const downloadResultFile = async (runDirName: string, filePath: string): Promise<Blob> => {
    if (!runDirName || !filePath) throw new Error("Run directory and file path are required for download.");
    try {
        const encodedFilePath = filePath.split('/').map(encodeURIComponent).join('/');
        const response = await apiClient.get(`/api/download_file/${encodeURIComponent(runDirName)}/${encodedFilePath}`, {
            responseType: 'blob',
        });
         if (!(response.data instanceof Blob)) {
             throw new Error("Invalid response received from server during file download.");
        }
        return response.data;
    } catch (error) {
        console.error(`Error downloading file ${filePath} from ${runDirName}:`, error);
        throw error;
    }
};

--- END FILE: ./frontend_app/lib/api.ts ---

--- START FILE: ./frontend_app/lib/types.ts (Size: 3710 bytes) ---
// File: frontend_app/lib/types.ts

export interface JobResourceInfo {
  peak_memory_mb?: number | null;
  average_cpu_percent?: number | null;
  duration_seconds?: number | null;
}

// Keep this specific to Sarek params we know
export interface SarekParams { // Already exported
  genome?: string;
  tools?: string; // Comma-separated string stored in meta
  step?: string;
  profile?: string;
  aligner?: string;
  joint_germline?: boolean;
  wes?: boolean;
  trim_fastq?: boolean;
  skip_qc?: boolean;
  skip_annotation?: boolean;
  skip_baserecalibrator?: boolean;
}

export interface InputFilenames { // Already exported
    intervals_file?: string | null;
    dbsnp?: string | null;
    known_indels?: string | null;
    pon?: string | null;
}

export interface SampleInfo { // Already exported
    patient: string;
    sample: string;
    sex: string;
    status: number; // 0=Normal, 1=Tumor
    // *** ADD lane field ***
    lane: string;
    // ********************
    fastq_1: string;
    fastq_2: string;
}

export interface JobMeta { // Already exported
  input_params?: InputFilenames;
  sarek_params?: SarekParams;
  sample_info?: SampleInfo[]; // Will now include lane
  staged_job_id_origin?: string;
  error_message?: string;
  stderr_snippet?: string;
  progress?: number;
  current_task?: string;
  peak_memory_mb?: number | null;
  average_cpu_percent?: number | null;
  duration_seconds?: number | null;
  results_path?: string;
  warning_message?: string;
  input_csv_path_used?: string; // Added in jobs.py for rerun debugging
  is_rerun_execution?: boolean; // Added in jobs.py
  original_job_id?: string; // Added in jobs.py for rerun reference
}

export interface JobResultSuccess { // Already exported
    status: "success";
    results_path?: string;
    message?: string;
    resources: JobMeta;
}

export interface Job { // Already exported
  id: string;
  status: "staged" | "queued" | "started" | "running" | "finished" | "failed" | "stopped" | "canceled" | string;
  description: string | null;
  enqueued_at: number | null;
  started_at: number | null;
  ended_at: number | null;
  staged_at?: number | null;
  result: JobResultSuccess | null | any;
  error: string | null;
  meta: JobMeta; // Meta now potentially includes more fields
  resources: JobResourceInfo | null;
}

export interface ResultRun { // Already exported
  name: string;
  is_dir: boolean;
  modified_time: number;
  size: number | null;
  extension: string | null;
  filebrowser_link: string | null;
  error?: string;
}

export interface ResultItem { // Already exported
    name: string;
    is_dir: boolean;
    modified_time: number;
    size: number | null;
    extension: string | null;
    filebrowser_link: string | null;
    error?: string;
    relative_path: string;
}

export interface JobStatusDetails extends Omit<Job, 'id' | 'staged_at'> { // Already exported
    job_id: string;
}

export interface DataFile { // Already exported
    name: string;
    type: 'file';
}

// Type for the API input payload
export interface PipelineInput {
  samples: SampleInfo[]; // Uses SampleInfo which now includes lane
  genome: string;
  intervals_file?: string;
  dbsnp?: string;
  known_indels?: string;
  pon?: string;
  tools?: string[]; // Frontend sends list of strings
  step?: string;
  profile?: string;
  aligner?: string;
  joint_germline?: boolean;
  wes?: boolean;
  trim_fastq?: boolean;
  skip_qc?: boolean;
  skip_annotation?: boolean;
  skip_baserecalibrator?: boolean;
  description?: string;
}

export interface RunParameters { // Already exported
  input_filenames?: InputFilenames | null;
  sarek_params?: SarekParams | null;
  sample_info?: SampleInfo[] | null; // Includes lane
}

--- END FILE: ./frontend_app/lib/types.ts ---

--- START FILE: ./frontend_app/lib/hooks/useJobsList.ts (Size: 1250 bytes) ---
// File: frontend_app/lib/hooks/useJobsList.ts
import { useQuery } from "@tanstack/react-query";
import { getJobsList } from "@/lib/api"; // Import the API function
import { Job } from "@/lib/types"; // Import the Job type

// Define the query key
const jobsQueryKey = ["jobsList"];

export function useJobsList(options?: { refetchInterval?: number | false }) {
  return useQuery<Job[], Error>({ // Specify return type and error type
    queryKey: jobsQueryKey,
    queryFn: getJobsList, // Use the API function as the query function
    refetchInterval: options?.refetchInterval, // Pass through refetch interval if provided
    // Add other options like staleTime if needed, though defaults are in QueryProvider
    // staleTime: 1000 * 30, // e.g., 30 seconds
  });
}

// Function to invalidate the query cache (used after actions like start/stop/remove)
// We might move this to a more central place later if needed by many components
// For now, components can import queryClient and call invalidateQueries directly
// import { useQueryClient } from "@tanstack/react-query";
// export function useInvalidateJobsList() {
//   const queryClient = useQueryClient();
//   return () => queryClient.invalidateQueries({ queryKey: jobsQueryKey });
// }

--- END FILE: ./frontend_app/lib/hooks/useJobsList.ts ---

--- START FILE: ./frontend_app/lib/utils.ts (Size: 1440 bytes) ---
// File: frontend_app/lib/utils.ts
import { clsx, type ClassValue } from "clsx"
import { twMerge } from "tailwind-merge"

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}

export function formatBytes(bytes: number, decimals = 2): string {
  if (!+bytes) return '0 Bytes'
  const k = 1024
  const dm = decimals < 0 ? 0 : decimals
  const sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB']
  if (bytes > 0 && bytes < 1) return `${bytes.toFixed(dm)} Bytes`;
  const i = Math.floor(Math.log(bytes) / Math.log(k))
  const index = Math.min(i, sizes.length - 1);
  return `${parseFloat((bytes / Math.pow(k, index)).toFixed(dm))} ${sizes[index]}`
}

// --- ADD THIS EXPORTED FUNCTION ---
export function formatDuration(seconds: number | null | undefined): string {
    if (seconds === null || seconds === undefined || seconds < 0 || isNaN(seconds)) return 'N/A'; // Added NaN check
    if (seconds < 1) return "< 1s";

    const h = Math.floor(seconds / 3600);
    const m = Math.floor((seconds % 3600) / 60);
    const s = Math.floor(seconds % 60);

    let str = "";
    if (h > 0) str += `${h}h `;
    if (m > 0 || h > 0) str += `${m.toString().padStart(h > 0 ? 2 : 1, '0')}m `;
    // Pad seconds only if minutes or hours are present
    str += `${s.toString().padStart(str ? 2 : 1, '0')}s`;

    return str.trim() || '0s'; // Handle case where duration is exactly 0
}
// --- END ADDED FUNCTION ---

--- END FILE: ./frontend_app/lib/utils.ts ---

--- START FILE: ./frontend_app/next.config.ts (Size: 817 bytes) ---
// frontend_app/next.config.ts
/** @type {import('next').NextConfig} */
const nextConfig = {
  output: 'standalone',
  webpack(config) {
    config.resolve.alias = {
      ...config.resolve.alias,
      '@': require('path').resolve(__dirname),
    };
    return config;
  },
  async headers() {
    return [
      {
        source: '/:path*',
        headers: [
          {
            key: 'Access-Control-Allow-Origin',
            value: 'http://100.121.160.49:3000',
          },
          {
            key: 'Access-Control-Allow-Methods',
            value: 'GET, POST, PUT, DELETE, OPTIONS',
          },
          {
            key: 'Access-Control-Allow-Headers',
            value: 'X-Requested-With, Content-Type, Authorization',
          },
        ],
      },
    ];
  },
};

export default nextConfig;

--- END FILE: ./frontend_app/next.config.ts ---

--- START FILE: ./frontend_app/eslint.config.mjs (Size: 393 bytes) ---
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
];

export default eslintConfig;

--- END FILE: ./frontend_app/eslint.config.mjs ---

--- START FILE: ./frontend_app/.gitignore (Size: 480 bytes) ---
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# env files (can opt-in for committing if needed)
.env*

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

--- END FILE: ./frontend_app/.gitignore ---

--- START FILE: ./main.py (Size: 330 bytes) ---
import uvicorn
from backend.app.app import app  # Import FastAPI app from backend/app/app.py

if __name__ == "__main__":

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        ssl_keyfile="./tls/server.key",  # Path to the key file
        ssl_certfile="./tls/server.crt", # Path to the cert file
    )

--- END FILE: ./main.py ---

--- START FILE: ./.github/workflows/docker-publish.yml (Size: 4059 bytes) ---
name: Docker Build and Push Components to GHCR

on:
  push:
    branches:
      - main
    paths-ignore:
      - 'README.md'
      - 'CHANGELOG.md'
      - '.gitignore'
      - '.dockerignore'
      - 'docs/**'
      - 'tls/**'
      - 'bioinformatics/**'
      - '*.db'
      - '*.srl'
  pull_request:
    branches:
      - main
    paths-ignore:
      - 'README.md'
      - 'CHANGELOG.md'
      - '.gitignore'
      - '.dockerignore'
      - 'docs/**'
      - 'tls/**'
      - 'bioinformatics/**'
      - '*.db'
      - '*.srl'
  workflow_dispatch:
    inputs:
      build_frontend:
        description: 'Build Frontend Image?'
        required: true
        type: boolean
        default: true
      build_webapp:
        description: 'Build Webapp Image?'
        required: true
        type: boolean
        default: true
      build_worker:
        description: 'Build Worker Image?'
        required: true
        type: boolean
        default: true
      build_filebrowser:
        description: 'Build File Browser Image?'
        required: true
        type: boolean
        default: true

jobs:
  build-and-push-components:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}

      - name: Build and push frontend image
        if: github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && github.event.inputs.build_frontend == 'true')
        uses: docker/build-push-action@v5
        with:
          context: ./frontend_app
          file: ./docker/Dockerfile.frontend
          push: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
          tags: |
            ghcr.io/mikha-22/bioinformatics-webapp/frontend:latest
            ghcr.io/mikha-22/bioinformatics-webapp/frontend:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push webapp image
        if: github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && github.event.inputs.build_webapp == 'true')
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile.webapp
          push: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
          tags: |
            ghcr.io/mikha-22/bioinformatics-webapp/webapp:latest
            ghcr.io/mikha-22/bioinformatics-webapp/webapp:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push worker image
        if: github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && github.event.inputs.build_worker == 'true')
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile.worker
          push: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
          tags: |
            ghcr.io/mikha-22/bioinformatics-webapp/worker:latest
            ghcr.io/mikha-22/bioinformatics-webapp/worker:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push filebrowser image
        if: github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && github.event.inputs.build_filebrowser == 'true')
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile.filebrowser
          push: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
          tags: |
            ghcr.io/mikha-22/bioinformatics-webapp/filebrowser:latest
            ghcr.io/mikha-22/bioinformatics-webapp/filebrowser:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

--- END FILE: ./.github/workflows/docker-publish.yml ---

--- START FILE: ./.gitignore (Size: 860 bytes) ---
# Sensitive files
tls/

# Nextflow specific
.nextflow/
.nextflow.log*
work/
nextflow
nextflow/

# CHANGED: Make results pattern specific to pipeline results
/results/
nxf-tmp.*

# Pipeline specific
bioinformatics/work/
bioinformatics/results/
bioinformatics/data/
bioinformatics/nf-temp/
backend/app/.nextflow/
backend/app/.nextflow.log*
backend/app/work/
backend/app/results/

# Environment and IDE
.env
.venv/
venv/
env/
.idea/
.vscode/
*.pyc
__pycache__/
.DS_Store

# Node.js
node_modules/
.next/
out/
build/
dist/

# Temporary files
*.swp
*.swo
*~
*.tmp
.*.swp

# Log files
*.log
logs/
log/

# Local development, ignore for now, frontend_app/.env.local sets certain variables for API URL and FileBrowser URL.

# .env.local
# .env.development.local
# .env.test.local
# .env.production.local

# System files
.DS_Store
Thumbs.db

# Misc
docker/filebrowser.db

--- END FILE: ./.gitignore ---

